<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>《Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization》阅读笔记 - dp0d's blog</title><meta name=Description content="dp0d's blog,where to share life as well as technology of nlp."><meta property="og:title" content="《Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization》阅读笔记"><meta property="og:description" content="原文链接：https://aclanthology.org/2022.emnlp-main.54.pdf 源码链接：https://githu"><meta property="og:type" content="article"><meta property="og:url" content="https://dp0d.github.io/bert_embedding_product_quantization/"><meta property="og:image" content="https://dp0d.github.io/images/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-17T15:05:19+08:00"><meta property="article:modified_time" content="2023-07-17T15:05:19+08:00"><meta property="og:site_name" content="dp0d's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dp0d.github.io/images/logo.png"><meta name=twitter:title content="《Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization》阅读笔记"><meta name=twitter:description content="原文链接：https://aclanthology.org/2022.emnlp-main.54.pdf 源码链接：https://githu"><meta name=application-name content="dp0d's blog"><meta name=apple-mobile-web-app-title content="dp0d's blog"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://dp0d.github.io/bert_embedding_product_quantization/><link rel=prev href=https://dp0d.github.io/jvm/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"《Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization》阅读笔记","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/dp0d.github.io\/bert_embedding_product_quantization\/"},"image":["https:\/\/dp0d.github.io\/images\/logo.png"],"genre":"posts","wordcount":4681,"url":"https:\/\/dp0d.github.io\/bert_embedding_product_quantization\/","datePublished":"2023-07-17T15:05:19+08:00","dateModified":"2023-07-17T15:05:19+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"dp0d","logo":"https:\/\/dp0d.github.io\/images\/logo.png"},"author":{"@type":"Person","name":"dp0d"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e)}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="dp0d's blog"><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span>Natural Language Processing</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/charge/>充电 </a><a class=menu-item href=/friends/>朋友萌 </a><a class=menu-item href=/collection/>收藏 </a><a class=menu-item href=/copyright/>版权 </a><a class=menu-item href=/about/>关于我 </a><a class=menu-item href=https://github.com/dp0d title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span>
</span><a href=# onclick=return!1 class="menu-item theme-select" title=切换主题><i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title=切换主题><option value=light>浅色</option><option value=dark>深色</option><option value=black>黑色</option><option value=auto>跟随系统</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="dp0d's blog"><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span>Natural Language Processing</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=# onclick=return!1 class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/charge/ title>充电</a><a class=menu-item href=/friends/ title>朋友萌</a><a class=menu-item href=/collection/ title>收藏</a><a class=menu-item href=/copyright/ title>版权</a><a class=menu-item href=/about/ title>关于我</a><a class=menu-item href=https://github.com/dp0d title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i></a><a href=# onclick=return!1 class="menu-item theme-select" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title=切换主题><option value=light>浅色</option><option value=dark>深色</option><option value=black>黑色</option><option value=auto>跟随系统</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#行文动机>行文动机</a></li><li><a href=#背景知识>背景知识</a><ul><li><a href=#向量量化vector-quantization>向量量化（Vector Quantization）</a></li></ul></li><li><a href=#信息检索中的乘积量化>信息检索中的乘积量化</a></li><li><a href=#端到端的联合重构和量化框架>端到端的联合重构和量化框架</a><ul><li><a href=#一种two-stage方法>一种Two-stage方法</a></li><li><a href=#基于对比乘积量化实现的端到端的细化和量化方法>基于对比乘积量化实现的端到端的细化和量化方法</a></li><li><a href=#引入互信息增强表征能力>引入互信息增强表征能力</a></li></ul></li><li><a href=#模型训练>模型训练</a></li><li><a href=#评估指标>评估指标</a><ul><li><a href=#asymmetric-distance-computation-adc>Asymmetric distance computation (ADC)</a></li><li><a href=#symmetric-distance-computation-sdc>Symmetric distance computation (SDC)</a></li></ul></li><li><a href=#总体结果>总体结果</a></li><li><a href=#-实现细节>## 实现细节</a><ul><li><a href=#码词初始化>码词初始化</a></li></ul></li><li><a href=#原始嵌入维数转化及refine网络>原始嵌入维数转化及refine网络</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">《Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization》阅读笔记</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=https://github.com/dp0d title=Author target=_blank rel="noopener noreffer author" class=author>dp0d</a>
</span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/paper_reading/><i class="far fa-folder fa-fw"></i>论文阅读</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2023-07-17>2023-07-17</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2023-07-17>2023-07-17</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 4681 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 10 分钟&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#行文动机>行文动机</a></li><li><a href=#背景知识>背景知识</a><ul><li><a href=#向量量化vector-quantization>向量量化（Vector Quantization）</a></li></ul></li><li><a href=#信息检索中的乘积量化>信息检索中的乘积量化</a></li><li><a href=#端到端的联合重构和量化框架>端到端的联合重构和量化框架</a><ul><li><a href=#一种two-stage方法>一种Two-stage方法</a></li><li><a href=#基于对比乘积量化实现的端到端的细化和量化方法>基于对比乘积量化实现的端到端的细化和量化方法</a></li><li><a href=#引入互信息增强表征能力>引入互信息增强表征能力</a></li></ul></li><li><a href=#模型训练>模型训练</a></li><li><a href=#评估指标>评估指标</a><ul><li><a href=#asymmetric-distance-computation-adc>Asymmetric distance computation (ADC)</a></li><li><a href=#symmetric-distance-computation-sdc>Symmetric distance computation (SDC)</a></li></ul></li><li><a href=#总体结果>总体结果</a></li><li><a href=#-实现细节>## 实现细节</a><ul><li><a href=#码词初始化>码词初始化</a></li></ul></li><li><a href=#原始嵌入维数转化及refine网络>原始嵌入维数转化及refine网络</a></li></ul></nav></div></div><div class=content id=content><p><img class=lazyload data-src=MD_img/image-20230717150626135.png data-srcset="/bert_embedding_product_quantization/MD_img/image-20230717150626135.png, MD_img/image-20230717150626135.png 1.5x, /bert_embedding_product_quantization/MD_img/image-20230717150626135.png 2x" data-sizes=auto alt=/bert_embedding_product_quantization/MD_img/image-20230717150626135.png title=image-20230717150626135></p><p>原文链接：<a href=https://aclanthology.org/2022.emnlp-main.54.pdf target=_blank rel="noopener noreffer">https://aclanthology.org/2022.emnlp-main.54.pdf</a></p><p>源码链接：<a href=https://github.com/zexuanqiu/MICPQ target=_blank rel="noopener noreffer">https://github.com/zexuanqiu/MICPQ</a></p><h2 id=行文动机 class=headerLink><a href=#%e8%a1%8c%e6%96%87%e5%8a%a8%e6%9c%ba class=header-mark></a>行文动机</h2><ol><li>现有文本哈希方法大都建立在统计特征上如TF-IDF，CBOW等，但是它们缺乏语序信息以及长依赖关系，较为传统和落后（某些方面还是有优势的（文章角度不同））。</li><li>尽管预训练语言模型在下游任务中的应用提升巨大，但是其中如原始的BERT嵌入已经被报道出在语义相似性上的表现不尽如人意，甚至不如Glove等静态嵌入（<a href=https://aclanthology.org/D19-1410.pdf target=_blank rel="noopener noreffer">Reimers and Gurevych, 2019</a>，6088+cite）。这是因为"anisotropy"（各向异性）现象（<a href=https://aclanthology.org/2020.emnlp-main.733.pdf target=_blank rel="noopener noreffer">Li et al., 2020</a>，383+cite），说白了就是含有的信息太多，语义相似性只占很小一部分。因此如何有效利用BERT嵌入中的相关信息十分关键。</li><li>汉明距离度量在现有方法中仅有少数一些取值存在（这与语义哈希，以及哈希码的低维特性以及构建方式有关，即各个维度独立且二值）。近期，计算机视觉领域提出了一种可替代的方法——使用乘积量化的方法来解决这个问题（<a href=https://ieeexplore.ieee.org/abstract/document/5432202 target=_blank rel="noopener noreffer">Jégou et al., 2011</a>; <a href=https://openaccess.thecvf.com/content/ICCV2021/papers/Jang_Self-Supervised_Product_Quantization_for_Deep_Unsupervised_Image_Retrieval_ICCV_2021_paper.pdf target=_blank rel="noopener noreffer">Jang and Cho, 2021</a>; <a href=https://doi.org/10.1609/aaai.v36i3.20147 target=_blank rel="noopener noreffer">Wang et al., 2021</a>）。</li></ol><h2 id=背景知识 class=headerLink><a href=#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86 class=header-mark></a>背景知识</h2><h3 id=向量量化vector-quantization class=headerLink><a href=#%e5%90%91%e9%87%8f%e9%87%8f%e5%8c%96vector-quantization class=header-mark></a>向量量化（Vector Quantization）</h3><p><a href=https://ieeexplore.ieee.org/abstract/document/5432202 target=_blank rel="noopener noreffer">Jégou et al., 2011</a></p><p>量化是一种破坏性过程，已在信息论中得到广泛研究(<a href=https://doi.org/10.1109/18.720541 target=_blank rel="noopener noreffer">R.M.GrayandD.L.Neuho, 1998</a> )</p><p>量化器quantizer是一个能将 $D$ 维向量 $\boldsymbol x \in\mathbb R^D$ 映射到向量$q(\boldsymbol x) \in \mathcal C={\boldsymbol c_i;i\in\mathcal I}$的映射函数$q$，其中$\mathcal C$是码本$codebook$。码本中的所有向量$\boldsymbol c_i$都称为质心向量$centroids$，<strong>它们和$\boldsymbol x$等维</strong>。一个由所有指向同一个质心向量$\boldsymbol c_i$的原始向量组成的集合定义为沃罗诺伊单元Voronoi cell $\mathcal V_i$：</p>$$
\mathcal V_i \triangleq\{\boldsymbol x\in\mathbb R^D|q(x)=\boldsymbol c_i\}
$$<p>通过极大似然损失来优化映射函数$q(x)$：</p>$$
\operatorname{MSE}(q)=\mathbb{E}_{X}\left[d(q(x), x)^{2}\right]=\int p(x) d(q(x), x)^{2} d x
$$<p>积分使用蒙特卡罗估计，$d(\cdot)$为欧几里得距离。</p><p>优化条件——洛伊德最优条件Lloyd optimality conditions（对应K-means算法，K-means算法仅找到量化误差方面的局部最优值）。</p><blockquote><p>条件一，量化器需要能够将特定向量$\boldsymbol x$映射到离其欧几里德距离最小的质心（即聚类中心）$\boldsymbol c_i$：</p></blockquote>$$
q(x)=\arg \min_{\boldsymbol c_i \in \mathcal C} d(\boldsymbol x, \boldsymbol c_i)
$$<blockquote><p>条件二，对于每个簇$i$，其质心$\boldsymbol c_i$（即聚类中心）必须等于该簇中所有数据点的平均值。</p></blockquote>$$
\boldsymbol c_i = \mathbb E_X[\boldsymbol x|i] = \int_{\mathcal V_i}p(\boldsymbol x) \boldsymbol x d\boldsymbol x
$$<p>使用的另一个需要满足的条件是均方失真$\xi(q,\boldsymbol c_i)$，由基于质心来重构沃罗诺伊单元内的向量的过程得到。设$p_i=\mathbb P(q(x)=\boldsymbol c_i)$，均方失真如下计算</p>$$
\xi(q,\boldsymbol c_i)=\frac{1}{p_i}\int_{\mathcal V_i} d(\boldsymbol x,q(\boldsymbol x))^2p(\boldsymbol x)d\boldsymbol x
$$<p><strong>公式含义</strong>：</p><ul><li>在各个沃罗诺伊单元空间中，向量量化后要尽量与原向量保持欧几里德距离近似</li><li>当一个向量越不确定属于本单元时，他所引入的失真度越大。</li></ul><p>此种方式的损失函数可以转为如下形式，即各单元中的量化向量和原始向量的极大似然之和</p>$$
{\rm MSE}(q) =\sum_{i\in \mathcal I}p_i\xi(q,\boldsymbol c_i)
$$<p>如果没有特殊处理，索引存储的内存消耗是$\log_2^K$。因此使用 2 的幂作为 k是很方便的，因为量化器生成的代码存储在二进制存储器中。</p><p>为什么索引内存消耗是$\log_2^K$呢？在K-means算法中，我们需要为每个数据点分配一个索引值，以指示该数据点属于哪个簇。在二进制编码中，每一位可以表示两个状态，0或1。因此，用n位二进制编码可以表示2^n 个不同的状态。在这种情况下，我们有K个簇，因此需要n位二进制编码，使得2^n >= K，两边取$\log_2$得n至少是$\log_2^K$位。</p><h2 id=信息检索中的乘积量化 class=headerLink><a href=#%e4%bf%a1%e6%81%af%e6%a3%80%e7%b4%a2%e4%b8%ad%e7%9a%84%e4%b9%98%e7%a7%af%e9%87%8f%e5%8c%96 class=header-mark></a>信息检索中的乘积量化</h2><p>在传统的语义哈希中，哈希码的相似性差异范围很小，对于一个B位的哈希码，它的汉明距离仅仅局限于从-B到B，这对于丰富的相似度信息而言过于严格。</p><p>由背景知识可知，向量量化是将原向量向码本空间映射的过程，即等维映射，对于一个64比特的二进制码而言，它的码本大小可达$2^{64}$，规模巨大难以处理。</p><p>为了解决这个问题，<a href=https://ieeexplore.ieee.org/abstract/document/5432202 target=_blank rel="noopener noreffer">Jégou et al., 2011</a>提出了另一种量化方式——乘积量化。码本$C$可以表示为$M$个小码本的笛卡尔积（Cartesian product）。</p>$$
C = C^1\times C^2\times \cdots\times C^M
$$<p>其中第$m$个小码本$C^m$有K个码词(质心)构成${\boldsymbol c_k^m|\boldsymbol c_k^m\in\mathbb R^{D/M}}^K_{k=1}$，对于每个最终码词，乘积量化会选用所有小码本的码词中的词进行组合</p>$$
\boldsymbol c =\boldsymbol c^1 \circ \boldsymbol c^2 \cdots \circ \boldsymbol c^M
$$<p>直观理解这个公式：假设有两个小码本$C^1=[[a,b],[a,c]], C^2=[[d,f][d,g]]$，那么他们的笛卡尔积就是</p>$$
C = [[a,b,d,f],[a,b,d,g],[a,c,d,f],[a,c,d,g]]
$$<p>对于每个码词（质心），内存占用为$M\log_2^K$，大小与$K$和$M$有关。得益于笛卡尔积，现在只需要存储$MK$个质心了，如果想要一个$2^{64}$大小的码本，那么仅需要设$M=32,K=4$即存储32个大小为$2^4$的小码本，即存储$2^{12}$个小码词（质心）。</p><h2 id=端到端的联合重构和量化框架 class=headerLink><a href=#%e7%ab%af%e5%88%b0%e7%ab%af%e7%9a%84%e8%81%94%e5%90%88%e9%87%8d%e6%9e%84%e5%92%8c%e9%87%8f%e5%8c%96%e6%a1%86%e6%9e%b6 class=header-mark></a>端到端的联合重构和量化框架</h2><h3 id=一种two-stage方法 class=headerLink><a href=#%e4%b8%80%e7%a7%8dtwo-stage%e6%96%b9%e6%b3%95 class=header-mark></a>一种Two-stage方法</h3><p>为了获取保留语义的量化表示，可以（1）首先提升原始BERT嵌入中的语义信息，（2）然后量化BERT嵌入。假设原始BERT嵌入为$z(x)$，突出语义后的BERT嵌入为</p>$$
\tilde{z}(x) = g(z(x))
$$<p>映射函数$g(\cdot)$，可以是flow-based mapping（<a href=https://aclanthology.org/2020.emnlp-main.733.pdf target=_blank rel="noopener noreffer">Li et al., 2020</a>，383+cite）或者是<a href=http://dx.doi.org/10.18653/v1/2021.emnlp-main.552 target=_blank rel="noopener noreffer">SimCSE</a>。然后对$\tilde{z}(x)$进行标准的乘积量化。</p><h3 id=基于对比乘积量化实现的端到端的细化和量化方法 class=headerLink><a href=#%e5%9f%ba%e4%ba%8e%e5%af%b9%e6%af%94%e4%b9%98%e7%a7%af%e9%87%8f%e5%8c%96%e5%ae%9e%e7%8e%b0%e7%9a%84%e7%ab%af%e5%88%b0%e7%ab%af%e7%9a%84%e7%bb%86%e5%8c%96%e5%92%8c%e9%87%8f%e5%8c%96%e6%96%b9%e6%b3%95 class=header-mark></a>基于对比乘积量化实现的端到端的细化和量化方法</h3><p>首先将原始BERT嵌入$z(x)$分成$M$节$z^m(x)\in \mathbb R^{D/M}, m=1,2,\cdots,M$。然后通过映射函数获得每一节的语义更为丰富的嵌入， $g_{\theta}^m(\cdot)$同时还完成了维数变换，让$\tilde{z}^m(x)$和$\boldsymbol c_{i}^m$等维。</p>$$
\tilde{z}^m(x) = g_{\theta}^m(z^m(x))
$$<p>将嵌入$\tilde{z}^m(x)$量化到从码词集合中${\boldsymbol c_{k^m}^m}^K_{k^m=1}$根据<strong>如下分布</strong>随机选择得到的一个码词</p>$$
p({k^m}|x)=\frac{\exp(-||\tilde{z}^m(x) - \boldsymbol c_{k^m}^m||^2)}{\sum^K_{i=1}\exp(-||\tilde{z}^m(x) - \boldsymbol c_{i}^m||^2)}
$$<p>表示为$k^m \sim p(k^m|x)$。子码本m提供的最终量化向量的第m个的量化表示（可以是向量）</p>$$
h^m(x) = C^m\cdot {\rm one\_hot}(k^m)
$$<p>所以对$x$完整的量化结果为</p>$$
h(x) = h^1(x) \circ h^2(x) \circ \cdots \circ h^M(x)
$$<p>这里量化表示$h(x)$依赖于随机值$k^m \sim p(k^m|x)$，所以$h(x)$本身也是随机的。</p><p>接下来对于每一个文档$x$，都进行两次输入BERT（两次Dropout 掩码不同）得到两个嵌入$h^{(1)}(x)和 h^{(2)}(x) $。</p><p>然后定义对比学习的Loss为</p>$$
\mathcal L_{cl} = -\frac{1}{|\mathcal B|}\sum_{x\in\mathcal B}(\ell ^{(1)}(x) + \ell ^{(2)}(x))
$$<p>其中$\mathcal B$为minibatch中的训练文档，且$\ell ^{(i)}(x)$定义为</p>$$
\ell ^{(i)}(x)\triangleq\log\frac{\mathcal S(h^{(1)}(x),h^{(2)}(x)}{\mathcal S(h^{(1)}(x),h^{(2)}(x))+\sum_{t\in\mathcal B\setminus x;n=1,2}\mathcal S(h^{(i)}(x),h^{(n)}(t))}
$$<p>公式解释：二次编码的两个嵌入近，离其他样本编码的嵌入远。$\mathcal S(h_1,h_2)$定义为</p>$$
\mathcal S(h_1, h_2) \triangleq\exp(sim(h_1,h_2)/\tau_{cl})
$$<p>其中$sim(h_1,h_2)$为余弦相似度$\frac{h_1^Th_2}{||h_1||||h_2||}$。</p><p>在上述量化过程中可知，量化表示$h(x)$ 依赖$x$的每一个分节对应采样的码词codeword $k^m\sim p(k^m|x)$。因此对于一个给定文档而言，量化过程不确定的。因此不直接优化随机对比损失$\mathcal L_{cl}$，而是最小化它的期望。</p>$$
\overline{{{\mathcal{L}}}}_{c l}=-\frac{1}{\left|{\mathcal{B}}\right|}\sum_{x\in{\mathcal{B}}}\left(\overline{{{\ell}}}^{(1)}(x)+\overline{{{\ell}}}^{(2)}(x)\right)
$$<p>其中${\overline{{{\ell}}}}^{(i)}(x)$代表${{{\ell}}}^{(i)}(x)$期望，涉及$k^{m},\sim,p(k^{m}|x);\mathrm{for}; m=,1,2,\cdot\cdot,.,.,,,M$。</p>$$
\bar{\ell}^{(i)}(x)=\mathbb{E}_{k^{1}, k^{2}, \cdots, k^{M}}\left[\ell^{(i)}(x)\right]
$$<p>显然这不太可能推出${\overline{{{\ell}}}}^{(i)}(x)$的解析式期望解析式，使得$\overline{{{\mathcal{L}}}}_{c l}$的优化无法进行。为了解决这个问题，使用Gumbel-Softmax重参数化技巧来估计采样过程</p>$$
k^m\sim p({k^m}|x)=\frac{\exp(-||\tilde{z}^m(x) - \boldsymbol c_{k^m}^m||^2)}{\sum^K_{i=1}\exp(-||\tilde{z}^m(x) - \boldsymbol c_{i}^m||^2)}
$$
<center>转为⬇️</center>
$$
k^m=\arg\max_i\left[-||\tilde{z}^m(x)-c_i^m||^2+\xi_i\right]
$$<p>因此，第m个量化维度为</p>$$
\widetilde h^{m}(\mathcal{x})\stackrel{\rightarrow}{\rightarrow}\subset\mathcal{C}^{m}\cdot\mathcal{v}
$$<p>其中$v\in\mathbb R^K$为概率向量（原来应该是one-hot）其中第$k$个元素为</p>$$
v_{k}=\frac{\exp \left(-\frac{\left\|\tilde{z}^{m}(x)-c_{k}^{m}\right\|^{2}+\xi_{k}}{\tau}\right)}{\sum_{i=1}^{K} \exp \left(-\frac{\left\|\tilde{z}^{m}(x)-c_{i}^{m}\right\|^{2}+\xi_{i}}{\tau}\right)}
$$<p>当$\tau$趋近于0，$\widetilde h^{m}(\mathcal{x})$会趋近于 $ h^{m}(\mathcal{x})$，基于上述估计方法，则有如下公式。</p>$$
\bar{\ell}^{(i)}(x) \approx \log \frac{\mathcal{S}\left(\widetilde{h}_{x}^{(1)}, \widetilde{h}_{x}^{(2)}\right)}{\mathcal{S}\left(\widetilde{h}_{x}^{(1)}, \widetilde{h}_{x}^{(2)}\right)+\sum_{\substack{t \in \mathcal{B} \backslash x \\ n=1,2}} \mathcal{S}\left(\widetilde{h}_{x}^{(i)}, \widetilde{h}_{t}^{(n)}\right)}
$$<h3 id=引入互信息增强表征能力 class=headerLink><a href=#%e5%bc%95%e5%85%a5%e4%ba%92%e4%bf%a1%e6%81%af%e5%a2%9e%e5%bc%ba%e8%a1%a8%e5%be%81%e8%83%bd%e5%8a%9b class=header-mark></a>引入互信息增强表征能力</h3><p>可以看到，码词的作用类似于聚类中心，而聚类算法如Kmeans容易陷入局部最优，因此局部最优也容易作用在码词上。最近有研究表明最大化数据和聚类群体之间的互信息可以带来更佳的聚类效果。于是提出最大化原始文档$x$和分配给它的码词（即索引）之间的互信息来增强码词的表征能力。</p><p>给定码词的条件分布$p(k^m|x)$，首先如下估计其边缘概率分布</p>$$
p(k^{m})\approx{\frac{1}{|\mathcal D|}}\sum_{x\in \mathcal D}p(k^{m}|x)
$$<p>其中$\mathcal D$表示训练集。根据定义，码词$k^m$的熵可以被估计为</p>$$
H(K^m) = -\sum_{k^m}^Kp(k^m)\log(p(k^m))
$$<p>类似的，给定$x$下的码词$k^m$的条件熵为</p>$$
H(K^m|X)=\frac{1}{\mathcal D}p(k^m|x)\log(p(k^m|x))
$$<p>如此可以计算码词索引$k^m$和数据$x$之间的互信息$I(X,K^m)=H(K^m)-H(K^m|X)$，实践中发现如下形式更佳</p>$$
I(X,K^m)=H(K^m)-\alpha H(K^m|X)
$$<p>$\alpha$是非负超参数，直观上，最大化互信息可以理解为对于给定文档$x$，鼓励只有一个码词高概率分配给它，并且总体上所有码词都被均匀使用。</p><p>因此总体的目标函数定义如下</p>$$
\mathcal L =\overline{{{\mathcal{L}}}}_{c l}-\lambda\sum_{m=1}^MI(X,K^m)
$$<h2 id=模型训练 class=headerLink><a href=#%e6%a8%a1%e5%9e%8b%e8%ae%ad%e7%bb%83 class=header-mark></a>模型训练</h2><ul><li>在训练中，固定BERT的模型参数，即对于给定文档 $x$，在训练中量化前嵌入是固定的。</li><li>码词维度固定为24，小码本中的码词数量固定为16，通过小码本数量$M={4,8,16,32}$来控制量化后目标值位数为${16,32,64,128}$依据$B=M\log_2 K$。注意，量化后的每一位都是一个24维的向量。</li><li><img src=MD_img/image-20230721000702755.png alt=image-20230721000702755 style=zoom:67%></li></ul><h2 id=评估指标 class=headerLink><a href=#%e8%af%84%e4%bc%b0%e6%8c%87%e6%a0%87 class=header-mark></a>评估指标</h2><p><a href=https://ieeexplore.ieee.org/abstract/document/5432202 target=_blank rel="noopener noreffer">Jégou et al., 2011</a></p><h3 id=asymmetric-distance-computation-adc class=headerLink><a href=#asymmetric-distance-computation-adc class=header-mark></a>Asymmetric distance computation (ADC)</h3><div style=text-align:center><img src=MD_img/image-20230720200441474.png alt=image-20230720200441474 style=zoom:67%></div><p>在非对称距离估计中，数据库中的向量$y$表示为$q(y)$，但是查询向量$x$不被编码。原始欧几里得距离$d(x,y)$被估计为$\tilde{d}\bigl(x,y\bigr)\ \triangleq d\bigl(x,q\bigl(y\bigr)\bigr)$</p>$$
\tilde{d}\bigl(x,y\bigr)\ = d\bigl(x,q\bigl(y\bigr)\bigr)=\sqrt{\sum_{j}d(u_{j}(x),~q_{j}(u_{j}(y)))^{2}}
$$<p>其中$u_{j}(x)$表示$x$的第$j$个分节。并且查询向量与各码本中的码词距离$d(u_j(x),c_{j,i})^2:j=1\cdots m,i=1\cdots k^<em>$需要在检索前就被计算好。时间复杂度为$mk^</em>$。</p><h3 id=symmetric-distance-computation-sdc class=headerLink><a href=#symmetric-distance-computation-sdc class=header-mark></a>Symmetric distance computation (SDC)</h3><div style=text-align:center><img src=MD_img/image-20230720230753090.png alt=image-20230720230753090 style=zoom:67%></div><p>对称距离计算中查询向量$x$和$y$都需要表示为对应的质心（码词）$q(x)$ 和$q(y)$。欧几里德距离$d(x,y)$被估计为$\tilde{d}\bigl(x,y\bigr)\ \triangleq d\bigl(x,q\bigl(y\bigr)\bigr)$。即原始向量距离衡量用量化后的距离替代</p>$$
\tilde{d}\bigl(x,y\bigr)\ = d\bigl(q\bigl(x\bigr),q\bigl(y\bigr)\bigr)=\sqrt{\sum_{j}d(q_j\bigl(x\bigr),q_j\bigl(y\bigr)))^{2}}
$$<p>码词间距离$d(c_{j,i},c_{j,i^{&rsquo;}})^2$可以通过查询表look-up table来存储，查表即可。</p><p>查询效率对比如下</p><div style=text-align:center><img src=MD_img/image-20230720210548749.png alt=image-20230720210548749 style=zoom:40%></div><p>本文中采用的是ADC方法，即非对称方法，注意如果检索到的文档具有相同的标签，则认为它们与查询相关。</p><h2 id=总体结果 class=headerLink><a href=#%e6%80%bb%e4%bd%93%e7%bb%93%e6%9e%9c class=header-mark></a>总体结果</h2><p><img class=lazyload data-src=MD_img/image-20230720224619400.png data-srcset="/bert_embedding_product_quantization/MD_img/image-20230720224619400.png, MD_img/image-20230720224619400.png 1.5x, /bert_embedding_product_quantization/MD_img/image-20230720224619400.png 2x" data-sizes=auto alt=/bert_embedding_product_quantization/MD_img/image-20230720224619400.png title=image-20230720224619400></p><p>其中AEPQ是在本文基础上，让量化后的哈希码重构BERT嵌入。CSH是遵循NASH假设，二值码通过多元伯努利分布采样得到。</p><h2 id=-实现细节 class=headerLink><a href=#-%e5%ae%9e%e7%8e%b0%e7%bb%86%e8%8a%82 class=header-mark></a>## 实现细节</h2><h3 id=码词初始化 class=headerLink><a href=#%e7%a0%81%e8%af%8d%e5%88%9d%e5%a7%8b%e5%8c%96 class=header-mark></a>码词初始化</h3><div style=text-align:center><img src=MD_img/image-20230720232139557.png alt=image-20230720232139557 style=zoom:33%></div><h2 id=原始嵌入维数转化及refine网络 class=headerLink><a href=#%e5%8e%9f%e5%a7%8b%e5%b5%8c%e5%85%a5%e7%bb%b4%e6%95%b0%e8%bd%ac%e5%8c%96%e5%8f%8arefine%e7%bd%91%e7%bb%9c class=header-mark></a>原始嵌入维数转化及refine网络</h2><div style=text-align:center><img src=MD_img/image-20230721104103212.png alt=image-20230721104103212 style=zoom:33%><p>​ and</p><img src=MD_img/image-20230721104015201.png alt=image-20230721104015201 style=zoom:33%><p>​ and</p><img src=MD_img/image-20230721104215075.png alt=image-20230721104215075 style=zoom:33%></div><div class=sponsor><div class=sponsor-avatar><img class=lazyload data-src=/images/logo.png data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x" data-sizes=auto alt=/images/logo.png title=/images/logo.png></div><p class=sponsor-bio><em>如果你觉得这篇文章对你有所帮助，欢迎赞赏~</em></p><a href=/images/buymeacoffee.jpg title=Sponsor target=_blank class=sponsor-button rel="noopener noreferrer"><i class="far fa-heart fa-fw icon" style=color:#ec6cb9></i>
<span>赞赏</span></a></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2023-07-17</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-mardown href=/bert_embedding_product_quantization/index.md target=_blank rel="noopener noreferrer">阅读原始文档</a>
</span><span>|&nbsp;<a class=link-to-report href="https://github.com/dp0d/dp0d.github.io/issues/new?title=[bug]%20%E3%80%8AEfficient+Document+Retrieval+by+End-to-End+Refining+and+Quantizing+BERT+Embedding+with+Contrastive+Product+Quantization%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0&body=|Field|Value|%0A|-|-|%0A|Title|%E3%80%8AEfficient+Document+Retrieval+by+End-to-End+Refining+and+Quantizing+BERT+Embedding+with+Contrastive+Product+Quantization%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0|%0A|Url|https://dp0d.github.io/bert_embedding_product_quantization/|%0A|Filename|https://github.com/dp0d/dp0d.github.io/posts/2023/BERT_embedding_Product_Quantization/index.md|" target=_blank rel="noopener noreferrer">报告问题</a></span></div><div class=post-info-share><span><a href=# onclick=return!1 title="分享到 Twitter" data-sharer=twitter data-url=https://dp0d.github.io/bert_embedding_product_quantization/ data-title="《Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization》阅读笔记"><i class="fab fa-twitter fa-fw"></i></a><a href=# onclick=return!1 title="分享到 Facebook" data-sharer=facebook data-url=https://dp0d.github.io/bert_embedding_product_quantization/><i class="fab fa-facebook-square fa-fw"></i></a><a href=# onclick=return!1 title="分享到 微博" data-sharer=weibo data-url=https://dp0d.github.io/bert_embedding_product_quantization/ data-title="《Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization》阅读笔记"><i class="fab fa-weibo fa-fw"></i></a><a href=# onclick=return!1 title="分享到 Telegram" data-sharer=telegram data-url=https://dp0d.github.io/bert_embedding_product_quantization/ data-title="《Efficient Document Retrieval by End-to-End Refining and Quantizing BERT Embedding with Contrastive Product Quantization》阅读笔记" data-web><i class="fab fa-telegram-plane fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/jvm/ class=prev rel=prev title=一些JVM基础知识><i class="fas fa-angle-left fa-fw"></i>一些JVM基础知识</a></div></div><div id=comments><div id=giscus></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app/>giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/dp0d target=_blank rel="noopener noreferrer">dp0d</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href=https://beian.miit.gov.cn/ target=_blank>赣ICP备2022002313号-1</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/object-fit-images/ofi.min.js></script><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/fuse/fuse.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:100},comment:{giscus:{darkTheme:"dark",dataCategory:"General",dataCategoryId:"DIC_kwDOHB3uW84COZbL",dataEmitMetadata:"0",dataInputPosition:"top",dataLang:"zh-CN",dataMapping:"title",dataReactionsEnabled:"1",dataRepo:"dp0d/dp0d.github.io",dataRepoId:"R_kgDOHB3uWw",lightTheme:"light"}},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!0,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:50,threshold:.1,type:"fuse",useExtendedSearch:!1},sharerjs:!0,table:{sort:!0},twemoji:!0}</script><script type=text/javascript src=/js/giscus.min.js defer></script><script type=text/javascript src=/lib/twemoji/twemoji.min.js defer></script><script type=text/javascript src=/js/twemoji.min.js defer></script><script type=text/javascript src=/lib/lightgallery/lightgallery.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-thumbnail.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-zoom.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>