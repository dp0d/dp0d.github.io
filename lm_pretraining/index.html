<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>语言模型预训练方法 - dp0d's blog</title><meta name=Description content="dp0d's blog,where to share life as well as technology of nlp."><meta property="og:title" content="语言模型预训练方法"><meta property="og:description" content="模型权重保存重载示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 自己定义的网络 ## 方法一：只保存权重 encoder = BertModel.from_pretrained(bert_model_path) model = EffiGlobalPointer(encoder, ENT_CLS_NUM, args.pointer_num).to(device) torch.save(best_model.state_dict(), &#34;data/model_data/best.pth&#34;) prev_state_dict = torch.load('data/model_data/best.pth') model.load_state_dict(prev_state_dict) ## 方法二：保存所有的 torch.save(best_model, &#34;data/model_data/best.pt&#34;)"><meta property="og:type" content="article"><meta property="og:url" content="https://dp0d.github.io/lm_pretraining/"><meta property="og:image" content="https://dp0d.github.io/images/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-18T11:04:18+08:00"><meta property="article:modified_time" content="2022-04-18T11:04:18+08:00"><meta property="og:site_name" content="dp0d's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dp0d.github.io/images/logo.png"><meta name=twitter:title content="语言模型预训练方法"><meta name=twitter:description content="模型权重保存重载示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 自己定义的网络 ## 方法一：只保存权重 encoder = BertModel.from_pretrained(bert_model_path) model = EffiGlobalPointer(encoder, ENT_CLS_NUM, args.pointer_num).to(device) torch.save(best_model.state_dict(), &#34;data/model_data/best.pth&#34;) prev_state_dict = torch.load('data/model_data/best.pth') model.load_state_dict(prev_state_dict) ## 方法二：保存所有的 torch.save(best_model, &#34;data/model_data/best.pt&#34;)"><meta name=application-name content="dp0d's blog"><meta name=apple-mobile-web-app-title content="dp0d's blog"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://dp0d.github.io/lm_pretraining/><link rel=prev href=https://dp0d.github.io/scrapy/><link rel=next href=https://dp0d.github.io/clash/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"语言模型预训练方法","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/dp0d.github.io\/lm_pretraining\/"},"image":["https:\/\/dp0d.github.io\/images\/logo.png"],"genre":"posts","keywords":"nlp","wordcount":546,"url":"https:\/\/dp0d.github.io\/lm_pretraining\/","datePublished":"2022-04-18T11:04:18+08:00","dateModified":"2022-04-18T11:04:18+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"dp0d","logo":"https:\/\/dp0d.github.io\/images\/logo.png"},"author":{"@type":"Person","name":"dp0d"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e)}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="dp0d's blog"><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span>Natural Language Processing</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/charge/>充电 </a><a class=menu-item href=/friends/>朋友萌 </a><a class=menu-item href=/collection/>收藏 </a><a class=menu-item href=/copyright/>版权 </a><a class=menu-item href=/about/>关于我 </a><a class=menu-item href=https://github.com/dp0d title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span>
</span><a href=# onclick=return!1 class="menu-item theme-select" title=切换主题><i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title=切换主题><option value=light>浅色</option><option value=dark>深色</option><option value=black>黑色</option><option value=auto>跟随系统</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="dp0d's blog"><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span>Natural Language Processing</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=# onclick=return!1 class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/charge/ title>充电</a><a class=menu-item href=/friends/ title>朋友萌</a><a class=menu-item href=/collection/ title>收藏</a><a class=menu-item href=/copyright/ title>版权</a><a class=menu-item href=/about/ title>关于我</a><a class=menu-item href=https://github.com/dp0d title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i></a><a href=# onclick=return!1 class="menu-item theme-select" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title=切换主题><option value=light>浅色</option><option value=dark>深色</option><option value=black>黑色</option><option value=auto>跟随系统</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#huggingface官方示例>huggingface官方示例</a></li></ul><ul><li><a href=#来源网络>来源网络</a></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">语言模型预训练方法</h1><h2 class=single-subtitle>预训练方式和代码相关</h2><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=https://github.com/dp0d title=Author target=_blank rel="noopener noreffer author" class=author>dp0d</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-04-18>2022-04-18</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2022-04-18>2022-04-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 546 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 2 分钟&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#huggingface官方示例>huggingface官方示例</a></li></ul><ul><li><a href=#来源网络>来源网络</a></li></ul></nav></div></div><div class=content id=content><h1 id=模型权重保存重载示例 class=headerLink><a href=#%e6%a8%a1%e5%9e%8b%e6%9d%83%e9%87%8d%e4%bf%9d%e5%ad%98%e9%87%8d%e8%bd%bd%e7%a4%ba%e4%be%8b class=header-mark></a>模型权重保存重载示例</h1><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># 自己定义的网络</span>
</span></span><span class=line><span class=cl><span class=c1>## 方法一：只保存权重</span>
</span></span><span class=line><span class=cl><span class=nv>encoder</span> <span class=o>=</span> BertModel.from_pretrained<span class=o>(</span>bert_model_path<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>model</span> <span class=o>=</span> EffiGlobalPointer<span class=o>(</span>encoder, ENT_CLS_NUM, args.pointer_num<span class=o>)</span>.to<span class=o>(</span>device<span class=o>)</span>
</span></span><span class=line><span class=cl>torch.save<span class=o>(</span>best_model.state_dict<span class=o>()</span>, <span class=s2>&#34;data/model_data/best.pth&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>prev_state_dict</span> <span class=o>=</span> torch.load<span class=o>(</span><span class=s1>&#39;data/model_data/best.pth&#39;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>model.load_state_dict<span class=o>(</span>prev_state_dict<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=c1>## 方法二：保存所有的</span>
</span></span><span class=line><span class=cl>torch.save<span class=o>(</span>best_model, <span class=s2>&#34;data/model_data/best.pt&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>model</span> <span class=o>=</span> torch.load<span class=o>(</span><span class=s1>&#39;data/model_data/best_state_dict.pt&#39;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># BertModel内置方法</span>
</span></span><span class=line><span class=cl><span class=c1># 注意特定的模型种类需要使用不同的from_pretrained方法，否则某些权重可能不加载,具体查看模型类</span>
</span></span><span class=line><span class=cl><span class=nv>model</span> <span class=o>=</span> BertModel<span class=o>()</span>
</span></span><span class=line><span class=cl>model.save_pretrained<span class=o>(</span><span class=s1>&#39;./output/bert_saved&#39;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>model</span> <span class=o>=</span> BertModel.from_pretrained<span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div><p>查询方式</p><img src=MD_img/image-20220418192635405.png alt=image-20220418192635405 style=zoom:80%><p>权重文件内部是这样的，熟悉的W和b。</p><img src=MD_img/image-20220418141357949.png alt=image-20220418141357949 style=zoom:50%>
<img src=MD_img/image-20220418141451653.png alt=image-20220418141451653 style=zoom:50%><h1 id=bert class=headerLink><a href=#bert class=header-mark></a>BERT</h1><h2 id=huggingface官方示例 class=headerLink><a href=#huggingface%e5%ae%98%e6%96%b9%e7%a4%ba%e4%be%8b class=header-mark></a>huggingface官方示例</h2><p>可以到https://github.com/huggingface/transformers/下的例子文件中寻找demo，由于官方更新速度快，具体路径请自行定位至examples文件夹~</p><p>以pytorch版本的masked language model为例</p><p>只需要运行以下脚本即可</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python run_mlm.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --model_name_or_path roberta-base <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --dataset_name wikitext <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --dataset_config_name wikitext-2-raw-v1 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --per_device_train_batch_size <span class=m>8</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --per_device_eval_batch_size <span class=m>8</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --do_train <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --do_eval <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>    --output_dir /tmp/test-mlm
</span></span></code></pre></td></tr></table></div></div><p>其中 dataset_name为语料文件，可以是txt格式的,每行一个文本序列</p><h1 id=nezha class=headerLink><a href=#nezha class=header-mark></a>nezha</h1><h2 id=来源网络 class=headerLink><a href=#%e6%9d%a5%e6%ba%90%e7%bd%91%e7%bb%9c class=header-mark></a>来源网络</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>import sys
</span></span><span class=line><span class=cl>import os
</span></span><span class=line><span class=cl>import csv
</span></span><span class=line><span class=cl>from transformers import BertTokenizer, WEIGHTS_NAME,TrainingArguments,BertForMaskedLM,BertConfig
</span></span><span class=line><span class=cl>import tokenizers
</span></span><span class=line><span class=cl>import torch
</span></span><span class=line><span class=cl><span class=c1># from configuration_nezha import NeZhaConfig</span>
</span></span><span class=line><span class=cl>from model.modeling_nezha import NeZhaForMaskedLM
</span></span><span class=line><span class=cl>from model.configuration_nezha import NeZhaConfig
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>os.environ<span class=o>[</span><span class=s2>&#34;CUDA_VISIBLE_DEVICES&#34;</span><span class=o>]</span> <span class=o>=</span> <span class=s1>&#39;0&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>from transformers import <span class=o>(</span>
</span></span><span class=line><span class=cl>CONFIG_MAPPING,
</span></span><span class=line><span class=cl>MODEL_FOR_MASKED_LM_MAPPING,
</span></span><span class=line><span class=cl>AutoConfig,
</span></span><span class=line><span class=cl>AutoModelForMaskedLM,
</span></span><span class=line><span class=cl>AutoTokenizer,
</span></span><span class=line><span class=cl>DataCollatorForLanguageModeling,
</span></span><span class=line><span class=cl>HfArgumentParser,
</span></span><span class=line><span class=cl>Trainer,
</span></span><span class=line><span class=cl>TrainingArguments,
</span></span><span class=line><span class=cl>set_seed,
</span></span><span class=line><span class=cl>LineByLineTextDataset,
</span></span><span class=line><span class=cl>BertTokenizerFast
</span></span><span class=line><span class=cl><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>model_path</span> <span class=o>=</span> <span class=s1>&#39;../nezha-base-www&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>tokenizer</span> <span class=o>=</span> BertTokenizerFast.from_pretrained<span class=o>(</span>model_path, <span class=nv>do_lower_case</span><span class=o>=</span>True<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>config</span> <span class=o>=</span> NeZhaConfig.from_pretrained<span class=o>(</span>model_path, <span class=nv>num_labels</span><span class=o>=</span>52<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>model</span> <span class=o>=</span> NeZhaForMaskedLM.from_pretrained<span class=o>(</span>model_path, <span class=nv>config</span><span class=o>=</span>config<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>train_dataset</span><span class=o>=</span>LineByLineTextDataset<span class=o>(</span><span class=nv>tokenizer</span><span class=o>=</span>tokenizer,file_path<span class=o>=</span><span class=s1>&#39;../pretrain_unlabel_dataset/train_data/unlabeled_train_data.txt&#39;</span>,block_size<span class=o>=</span>128<span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>data_collator</span> <span class=o>=</span> DataCollatorForLanguageModeling<span class=o>(</span><span class=nv>tokenizer</span><span class=o>=</span>tokenizer, <span class=nv>mlm</span><span class=o>=</span>True, <span class=nv>mlm_probability</span><span class=o>=</span>0.15<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>pretrain_batch_size</span><span class=o>=</span><span class=m>128</span>
</span></span><span class=line><span class=cl><span class=nv>num_train_epochs</span><span class=o>=</span><span class=m>30</span>
</span></span><span class=line><span class=cl><span class=nv>training_args</span> <span class=o>=</span> TrainingArguments<span class=o>(</span>
</span></span><span class=line><span class=cl><span class=nv>output_dir</span><span class=o>=</span><span class=s1>&#39;./pretrained_nezha&#39;</span>,
</span></span><span class=line><span class=cl><span class=nv>overwrite_output_dir</span><span class=o>=</span>True,
</span></span><span class=line><span class=cl><span class=nv>num_train_epochs</span><span class=o>=</span>num_train_epochs,
</span></span><span class=line><span class=cl><span class=nv>learning_rate</span><span class=o>=</span>6e-5,
</span></span><span class=line><span class=cl><span class=nv>per_device_train_batch_size</span><span class=o>=</span>pretrain_batch_size,
</span></span><span class=line><span class=cl><span class=nv>save_total_limit</span><span class=o>=</span>10,
</span></span><span class=line><span class=cl><span class=nv>logging_dir</span><span class=o>=</span><span class=s1>&#39;./pretrained_nezha_log&#39;</span>,
</span></span><span class=line><span class=cl><span class=nv>logging_steps</span><span class=o>=</span>10000,
</span></span><span class=line><span class=cl><span class=nv>no_cuda</span><span class=o>=</span>False<span class=o>)</span><span class=c1># save_steps=10000</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>trainer</span> <span class=o>=</span> Trainer<span class=o>(</span>
</span></span><span class=line><span class=cl><span class=nv>model</span><span class=o>=</span>model, <span class=nv>args</span><span class=o>=</span>training_args, <span class=nv>data_collator</span><span class=o>=</span>data_collator, <span class=nv>train_dataset</span><span class=o>=</span>train_dataset<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>trainer.train<span class=o>()</span>
</span></span><span class=line><span class=cl>trainer.save_model<span class=o>(</span><span class=s1>&#39;./pretrained_nezha&#39;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> i in range<span class=o>(</span>100<span class=o>)</span>:
</span></span><span class=line><span class=cl>    torch.cuda.empty_cache<span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div></div><div class=sponsor><div class=sponsor-avatar><img class=lazyload data-src=/images/logo.png data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x" data-sizes=auto alt=/images/logo.png title=/images/logo.png></div><p class=sponsor-bio><em>如果你觉得这篇文章对你有所帮助，欢迎赞赏~</em></p><a href=/images/buymeacoffee.jpg title=Sponsor target=_blank class=sponsor-button rel="noopener noreferrer"><i class="far fa-heart fa-fw icon" style=color:#ec6cb9></i>
<span>赞赏</span></a></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2022-04-18</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-mardown href=/lm_pretraining/index.md target=_blank rel="noopener noreferrer">阅读原始文档</a>
</span><span>|&nbsp;<a class=link-to-report href="https://github.com/dp0d/dp0d.github.io/issues/new?title=[bug]%20%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95&body=|Field|Value|%0A|-|-|%0A|Title|%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95|%0A|Url|https://dp0d.github.io/lm_pretraining/|%0A|Filename|https://github.com/dp0d/dp0d.github.io/posts/2022/LM_pretraining/index.md|" target=_blank rel="noopener noreferrer">报告问题</a></span></div><div class=post-info-share><span><a href=# onclick=return!1 title="分享到 Twitter" data-sharer=twitter data-url=https://dp0d.github.io/lm_pretraining/ data-title=语言模型预训练方法 data-hashtags=nlp><i class="fab fa-twitter fa-fw"></i></a><a href=# onclick=return!1 title="分享到 Facebook" data-sharer=facebook data-url=https://dp0d.github.io/lm_pretraining/ data-hashtag=nlp><i class="fab fa-facebook-square fa-fw"></i></a><a href=# onclick=return!1 title="分享到 微博" data-sharer=weibo data-url=https://dp0d.github.io/lm_pretraining/ data-title=语言模型预训练方法><i class="fab fa-weibo fa-fw"></i></a><a href=# onclick=return!1 title="分享到 Telegram" data-sharer=telegram data-url=https://dp0d.github.io/lm_pretraining/ data-title=语言模型预训练方法 data-web><i class="fab fa-telegram-plane fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/nlp/>nlp</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/scrapy/ class=prev rel=prev title=Scrapy实战案例——房天下><i class="fas fa-angle-left fa-fw"></i>Scrapy实战案例——房天下</a>
<a href=/clash/ class=next rel=next title="记一次Clash for Linux 的配置">记一次Clash for Linux 的配置<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments><div id=giscus></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app/>giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/dp0d target=_blank rel="noopener noreferrer">dp0d</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href=https://beian.miit.gov.cn/ target=_blank>赣ICP备2022002313号-1</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/object-fit-images/ofi.min.js></script><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/fuse/fuse.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:100},comment:{giscus:{darkTheme:"dark",dataCategory:"General",dataCategoryId:"DIC_kwDOHB3uW84COZbL",dataEmitMetadata:"0",dataInputPosition:"top",dataLang:"zh-CN",dataMapping:"title",dataReactionsEnabled:"1",dataRepo:"dp0d/dp0d.github.io",dataRepoId:"R_kgDOHB3uWw",lightTheme:"light"}},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!0,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:50,threshold:.1,type:"fuse",useExtendedSearch:!1},sharerjs:!0,table:{sort:!0},twemoji:!0}</script><script type=text/javascript src=/js/giscus.min.js defer></script><script type=text/javascript src=/lib/twemoji/twemoji.min.js defer></script><script type=text/javascript src=/js/twemoji.min.js defer></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script></div></body></html>