<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title class=pjax-title>《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记 - dp0d's blog</title><meta name=Description content="dp0d's blog,where to share life as well as technology of nlp."><meta property="og:title" content="《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记"><meta property="og:description" content="原文链接：https://aclanthology.org/2021.findings-emnlp.203.pdf 源码链接：https://"><meta property="og:type" content="article"><meta property="og:url" content="https://dp0d.github.io/bert_embedding_and_mimaximization/"><meta property="og:image" content="https://dp0d.github.io/images/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-11T15:04:51+08:00"><meta property="article:modified_time" content="2023-07-11T15:04:51+08:00"><meta property="og:site_name" content="dp0d's blog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dp0d.github.io/images/logo.png"><meta name=twitter:title content="《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记"><meta name=twitter:description content="原文链接：https://aclanthology.org/2021.findings-emnlp.203.pdf 源码链接：https://"><meta name=application-name content="dp0d's blog"><meta name=apple-mobile-web-app-title content="dp0d's blog"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://dp0d.github.io/bert_embedding_and_mimaximization/><link rel=prev href=https://dp0d.github.io/clashtun/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/dp0d.github.io\/bert_embedding_and_mimaximization\/"},"image":["https:\/\/dp0d.github.io\/images\/logo.png"],"genre":"posts","wordcount":2149,"url":"https:\/\/dp0d.github.io\/bert_embedding_and_mimaximization\/","datePublished":"2023-07-11T15:04:51+08:00","dateModified":"2023-07-11T15:04:51+08:00","license":"This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher":{"@type":"Organization","name":"dp0d","logo":"https:\/\/dp0d.github.io\/images\/logo.png"},"author":{"@type":"Person","name":"dp0d"},"description":""}</script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e)}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="dp0d's blog"><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span>Natural Language Processing</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/charge/>充电 </a><a class=menu-item href=/friends/>朋友萌 </a><a class=menu-item href=/collection/>收藏 </a><a class=menu-item href=/copyright/>版权 </a><a class=menu-item href=/about/>关于我 </a><a class=menu-item href=https://github.com/dp0d title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i></span>
</span><a href=# onclick=return!1 class="menu-item theme-select" title=切换主题><i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title=切换主题><option value=light>浅色</option><option value=dark>深色</option><option value=black>黑色</option><option value=auto>跟随系统</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="dp0d's blog"><span class=header-title-pre><i class='far fa-edit fa-fw'></i></span>Natural Language Processing</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=# onclick=return!1 class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i></a>
<a href=# onclick=return!1 class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=# onclick=return!1 class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/charge/ title>充电</a><a class=menu-item href=/friends/ title>朋友萌</a><a class=menu-item href=/collection/ title>收藏</a><a class=menu-item href=/copyright/ title>版权</a><a class=menu-item href=/about/ title>关于我</a><a class=menu-item href=https://github.com/dp0d title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw'></i></a><a href=# onclick=return!1 class="menu-item theme-select" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title=切换主题><option value=light>浅色</option><option value=dark>深色</option><option value=black>黑色</option><option value=auto>跟随系统</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#行文动机>行文动机</a></li><li><a href=#基于对比方法引入bert嵌入>基于对比方法引入BERT嵌入</a></li><li><a href=#本文方法基于互信息最大化原则重构bert嵌入>本文方法——基于互信息最大化原则重构BERT嵌入</a><ul><li><a href=#图像领域的互信息最大化方法>图像领域的互信息最大化方法</a></li><li><a href=#构建文本局部和全局表征>构建文本局部和全局表征</a><ul><li><a href=#局部表征>局部表征</a></li><li><a href=#全局表征>全局表征</a></li></ul></li><li><a href=#基于最大化局部和全局表征的端到端的哈希>基于最大化局部和全局表征的端到端的哈希</a></li><li><a href=#实验部分>实验部分</a><ul><li><a href=#数据集信息>数据集信息</a></li><li><a href=#对比方法和细节>对比方法和细节</a></li><li><a href=#评估方法>评估方法</a></li><li><a href=#总体结果>总体结果</a></li></ul></li></ul></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("pageStyle","normal")</script><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=https://github.com/dp0d title=Author target=_blank rel="noopener noreffer author" class=author>dp0d</a>
</span>&nbsp;<span class=post-category>收录于 </span>&nbsp;<span class=post-category>类别 <a href=/categories/paper_reading/><i class="far fa-folder fa-fw"></i>论文阅读</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2023-07-11>2023-07-11</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2023-07-11>2023-07-11</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 2149 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 5 分钟&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#行文动机>行文动机</a></li><li><a href=#基于对比方法引入bert嵌入>基于对比方法引入BERT嵌入</a></li><li><a href=#本文方法基于互信息最大化原则重构bert嵌入>本文方法——基于互信息最大化原则重构BERT嵌入</a><ul><li><a href=#图像领域的互信息最大化方法>图像领域的互信息最大化方法</a></li><li><a href=#构建文本局部和全局表征>构建文本局部和全局表征</a><ul><li><a href=#局部表征>局部表征</a></li><li><a href=#全局表征>全局表征</a></li></ul></li><li><a href=#基于最大化局部和全局表征的端到端的哈希>基于最大化局部和全局表征的端到端的哈希</a></li><li><a href=#实验部分>实验部分</a><ul><li><a href=#数据集信息>数据集信息</a></li><li><a href=#对比方法和细节>对比方法和细节</a></li><li><a href=#评估方法>评估方法</a></li><li><a href=#总体结果>总体结果</a></li></ul></li></ul></li></ul></nav></div></div><div class=content id=content><p><img class=lazyload data-src=MD_img/image-20230711150857898.png data-srcset="/bert_embedding_and_mimaximization/MD_img/image-20230711150857898.png, MD_img/image-20230711150857898.png 1.5x, /bert_embedding_and_mimaximization/MD_img/image-20230711150857898.png 2x" data-sizes=auto alt=/bert_embedding_and_mimaximization/MD_img/image-20230711150857898.png title=image-20230711150857898></p><p>原文链接：https://aclanthology.org/2021.findings-emnlp.203.pdf</p><p>源码链接：https://github.com/J-zin/DHIM</p><h2 id=行文动机 class=headerLink><a href=#%e8%a1%8c%e6%96%87%e5%8a%a8%e6%9c%ba class=header-mark></a>行文动机</h2><p>大多数文本哈希方法都是基于BOW和TFIDF学得的，而这种特征<strong>缺乏词序和依存关系的相关信息</strong>。然而，由于BERT embedding在下游任务中的出色表现，本文决定抛弃“落后”的BOW和TFIDF特征，转而直接建模由BERT嵌入表征的文本。文中将使用BOW和TFIDF的使用归因于是逃避对于长依赖关系带来的挑战（其实这种基于统计的特征是带有原始检索的潜在意义的，比如关键词感知等）。</p><p>创新：引入了最大互信息原则来解决BERT嵌入中存在过于丰富的信息的问题。具体而言，BERT embedding 富含信息，其中只有一小部分和哈希有关，所以哈希码不太能在语义一致上和嵌入一致。文章指出，在2019年的一篇高引论文5900+《<a href=https://aclanthology.org/D19-1410.pdf target=_blank rel="noopener noreffer">Sentencebert: Sentence embeddings using siamese bertnetworks</a>》中就曾得出过结论，原始的BERT嵌入在余弦相似度，曼哈顿距离，欧几里得距离上的表现不尽如人意。本文受《<a href=https://arxiv.org/pdf/1808.06670.pdf target=_blank rel="noopener noreffer">Learning deep representations by mutual information estimation and maximization</a>》一文启发，通过最大互信息原则可以学得有目标区分度的表征，来<strong>更多突出语义信息以及削弱其他无关的信息</strong>。</p><h2 id=基于对比方法引入bert嵌入 class=headerLink><a href=#%e5%9f%ba%e4%ba%8e%e5%af%b9%e6%af%94%e6%96%b9%e6%b3%95%e5%bc%95%e5%85%a5bert%e5%b5%8c%e5%85%a5 class=header-mark></a>基于对比方法引入BERT嵌入</h2><p>基于BOW和TFIDF特征的文本哈希方法大都基于如下方式建模，给定一个由词序列表示的文档$\boldsymbol x={w_1, w_2,\cdots,w_{|x|}}$：
$$
p(x, z)=\prod_{w_{i} \in x} p_{\theta}\left(w_{i} \mid z\right) p(z)
$$
其中
$$
p_{\theta}\left(w_{i} \mid z\right) \triangleq \frac{\exp \left(z^{T} E w_{i}+b_{i}\right)}{\sum_{j=1}^{|V|} \exp \left(z^{T} E w_{j}+b_{j}\right)}
$$
由于BERT嵌入和BOW特征的差异性，解码过程替换为一个条件高斯分布
$$
p_{\theta}(x|z) = \frac{1}{{(2\pi\sigma^2)}^{d/2}}e^{-\frac{||\mathcal B(x) - Wz||^2}{2\sigma^2}}
$$
其中$\mathcal B(x)$为BERT嵌入，$W$为可学习的模型参数（解码网络），$d$为BERT嵌入维数（768）。</p><h2 id=本文方法基于互信息最大化原则重构bert嵌入 class=headerLink><a href=#%e6%9c%ac%e6%96%87%e6%96%b9%e6%b3%95%e5%9f%ba%e4%ba%8e%e4%ba%92%e4%bf%a1%e6%81%af%e6%9c%80%e5%a4%a7%e5%8c%96%e5%8e%9f%e5%88%99%e9%87%8d%e6%9e%84bert%e5%b5%8c%e5%85%a5 class=header-mark></a>本文方法——基于互信息最大化原则重构BERT嵌入</h2><h3 id=图像领域的互信息最大化方法 class=headerLink><a href=#%e5%9b%be%e5%83%8f%e9%a2%86%e5%9f%9f%e7%9a%84%e4%ba%92%e4%bf%a1%e6%81%af%e6%9c%80%e5%a4%a7%e5%8c%96%e6%96%b9%e6%b3%95 class=header-mark></a>图像领域的互信息最大化方法</h3><p>《<a href=https://arxiv.org/pdf/1808.06670.pdf target=_blank rel="noopener noreffer">Learning deep representations by mutual information estimation and maximization</a>》中通过最大化全局和局部表示之间的互信息来学习类别可区分的图像表示。该文第一次构造了对于图片的全局表示和众多的局部表示，它们都通过图像的卷积特征得到。既然有众多的局部表示，并且各自局部表示图像中的特定局部源。全局和局部表示的互信息使得全局表示更加关注全局语义，即所有局部语义信息的共性，而非局部表示之间的具体不同细节。</p><h3 id=构建文本局部和全局表征 class=headerLink><a href=#%e6%9e%84%e5%bb%ba%e6%96%87%e6%9c%ac%e5%b1%80%e9%83%a8%e5%92%8c%e5%85%a8%e5%b1%80%e8%a1%a8%e5%be%81 class=header-mark></a>构建文本局部和全局表征</h3><h4 id=局部表征 class=headerLink><a href=#%e5%b1%80%e9%83%a8%e8%a1%a8%e5%be%81 class=header-mark></a>局部表征</h4><p>给定一篇文档，获取每一个词的嵌入表示，最终表示一篇文档为$X = {e_1, . . . , e_T}$，其中$e_i\in \mathbb R^d$表示第$i$个词的BERT嵌入表示， $T$表示文档总词数。将文档$X$输入文本卷积神经网络CNN，其中筛选器$W\in \mathbb R^{K\times n\times d}$， $n$和$K$表示筛选器的大小和数量。<strong>这种方式能够为每个 n-gram 片段生成局部表征</strong>，具体而言，第$i$个片段的局部表征如下计算，*<strong>表示卷积运算符</strong>：</p>$$
h_i^{(n)}={\rm RELU}(W* e_{i:i+n-1})
$$<p>将其作用于所有的文本片段，则可以获得所有词对应位置的局部表征：</p>$$
H^{(n)}=\{h_1^{(n)},h_2^{(n)},\cdots,h_T^{(n)}\}
$$<h4 id=全局表征 class=headerLink><a href=#%e5%85%a8%e5%b1%80%e8%a1%a8%e5%be%81 class=header-mark></a>全局表征</h4><p>将$H^{(n)}$ 传入$\rm READOUT$函数，即平均池化函数或者更复杂的自注意力机制，可以在不受文档长度的影响下获得文档的全局表征。
$$
H = {\rm READOUT}({h_i^{(n)}}_{n\in\mathcal N})
$$</p><p>另外，为了进一步在全局表征中突出语义信息，提出采用三种不同切片方式的连接，即1-gram，3-gram，5-gram，最终的局部和全局表征可以如下计算：</p>$$
h_i = {\rm MLP(CONCAT}(\{h_i^{(n)}\}_{n\in\mathcal N})),\\
H = {\rm READOUT}(\{h_i\}_{i=1}^T),
$$<p>其中$\mathcal N$表示不同的窗口大小集合，在实验中$\mathcal N={1,3,5}$如Figure 1所示。</p><img src=MD_img/image-20230711164247539.png alt=image-20230711164247539 style=zoom:33%><h3 id=基于最大化局部和全局表征的端到端的哈希 class=headerLink><a href=#%e5%9f%ba%e4%ba%8e%e6%9c%80%e5%a4%a7%e5%8c%96%e5%b1%80%e9%83%a8%e5%92%8c%e5%85%a8%e5%b1%80%e8%a1%a8%e5%be%81%e7%9a%84%e7%ab%af%e5%88%b0%e7%ab%af%e7%9a%84%e5%93%88%e5%b8%8c class=header-mark></a>基于最大化局部和全局表征的端到端的哈希</h3><p>总体框架图如下</p><p><img class=lazyload data-src=MD_img/image-20230712193807091.png data-srcset="/bert_embedding_and_mimaximization/MD_img/image-20230712193807091.png, MD_img/image-20230712193807091.png 1.5x, /bert_embedding_and_mimaximization/MD_img/image-20230712193807091.png 2x" data-sizes=auto alt=/bert_embedding_and_mimaximization/MD_img/image-20230712193807091.png title=image-20230712193807091></p><p>首先生成基于局部和全局表征的哈希码，通过采样如下伯努利分布得到：</p>$$
b_i\sim {\rm Bernoulli}(\sigma(h_i)),\\
B\sim {\rm Bernoulli(\sigma(H))},
$$<p>$b_i$ 和$B$表示局部和全局对应的哈希码。接下来最大化他们之间的互信息。
$$
\hat{\theta}=\arg \max_{\theta}\frac{1}{T}\sum^T_{i=1}I(b_i,B),
$$
$b_i$ 和$B$表示对于特定文档而言不是唯一对应的，这使得互信息难以评估。</p><p>最近，有许多方法被踢出用来估计互信息，如MINE，infoNCE，Jensen-Shannon divergence estimator (JSDE)，其中JSDE对负样本数量较不敏感，于是本文使用JSDE来估计互信息并优化它。具体而言，互信息可以通过最小化如下公式来估计：</p>$$
\begin{aligned}
\tilde{I_{\phi}}(b_i;B)=&-{\rm softplus}(-D_{\phi}(b_i,B))\\
&-\mathbb E_{\tilde{\mathbb P}}{\rm softplus}(-D_{\phi}(\tilde{b_i},B))
\end{aligned}
$$<p>其中$\tilde{b_i}$表示<strong>从经验分布$\tilde{\mathbb P}=\mathbb P$生成的负样本</strong>的第$i$个局部表征，在实际实验中负样本$\tilde{b_i}$是在局部表征中选取的或从batch的其他文档中选取的。$D_{\phi}(\cdot,\cdot)$是由神经网络实现的判别器。${\rm softplus}(x)\triangleq\log (1+e^x)$。</p><p>上述最大化互信息框架仅仅依赖BERT对单个单词的嵌入，忽略了BERT的CLS嵌入。因此，为了提高学得哈希码中的全局语义信息,添加一个正则化项来提高哈希码和CLS嵌入之间的互信息，最终的Loss如下：
$$
\tilde{\mathcal L}(\phi,\theta)=-\frac{1}{T}\sum^T_{i=1}\tilde{I_{\phi}}(b_i;B)-\beta(E,B),
$$
$\beta$ 是超参数，$E$表示二值化后的CLS嵌入， $\theta$是构造局部和全局二值码 $b_i$ 和$B$过程中涉及的模型参数。</p><h3 id=实验部分 class=headerLink><a href=#%e5%ae%9e%e9%aa%8c%e9%83%a8%e5%88%86 class=header-mark></a>实验部分</h3><h4 id=数据集信息 class=headerLink><a href=#%e6%95%b0%e6%8d%ae%e9%9b%86%e4%bf%a1%e6%81%af class=header-mark></a>数据集信息</h4><img src=MD_img/image-20230711200845225.png alt=image-20230711200845225 style=zoom:33%><h4 id=对比方法和细节 class=headerLink><a href=#%e5%af%b9%e6%af%94%e6%96%b9%e6%b3%95%e5%92%8c%e7%bb%86%e8%8a%82 class=header-mark></a>对比方法和细节</h4><img src=MD_img/image-20230711201124451.png alt=image-20230711201124451 style=zoom:33%>
<img src=MD_img/image-20230711201148567.png alt=image-20230711201148567 style=zoom:33%>
<img src=MD_img/image-20230711201326049.png alt=image-20230711201326049 style=zoom:33%><h4 id=评估方法 class=headerLink><a href=#%e8%af%84%e4%bc%b0%e6%96%b9%e6%b3%95 class=header-mark></a>评估方法</h4><img src=MD_img/image-20230711201420024.png alt=image-20230711201420024 style=zoom:33%><h4 id=总体结果 class=headerLink><a href=#%e6%80%bb%e4%bd%93%e7%bb%93%e6%9e%9c class=header-mark></a>总体结果</h4><p>DHIM即本文模型，无类别标签监督的模型，相比基线模型有所提升，也印证了前文提到的胜澈给你模型的相关工作——单纯重构BERT嵌入哈希码面临的性能问题。</p><p><img class=lazyload data-src=MD_img/image-20230711201740218.png data-srcset="/bert_embedding_and_mimaximization/MD_img/image-20230711201740218.png, MD_img/image-20230711201740218.png 1.5x, /bert_embedding_and_mimaximization/MD_img/image-20230711201740218.png 2x" data-sizes=auto alt=/bert_embedding_and_mimaximization/MD_img/image-20230711201740218.png title=image-20230711201740218></p></div><div class=sponsor><div class=sponsor-avatar><img class=lazyload data-src=/images/logo.png data-srcset="/images/logo.png, /images/logo.png 1.5x, /images/logo.png 2x" data-sizes=auto alt=/images/logo.png title=/images/logo.png></div><p class=sponsor-bio><em>如果你觉得这篇文章对你有所帮助，欢迎赞赏~</em></p><a href=/images/buymeacoffee.jpg title=Sponsor target=_blank class=sponsor-button rel="noopener noreferrer"><i class="far fa-heart fa-fw icon" style=color:#ec6cb9></i>
<span>赞赏</span></a></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2023-07-11</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-mardown href=/bert_embedding_and_mimaximization/index.md target=_blank rel="noopener noreferrer">阅读原始文档</a>
</span><span>|&nbsp;<a class=link-to-report href="https://github.com/dp0d/dp0d.github.io/issues/new?title=[bug]%20%E3%80%8ARefining+BERT+Embeddings+for+Document+Hashing+via+Mutual+Information+Maximization%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0&body=|Field|Value|%0A|-|-|%0A|Title|%E3%80%8ARefining+BERT+Embeddings+for+Document+Hashing+via+Mutual+Information+Maximization%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0|%0A|Url|https://dp0d.github.io/bert_embedding_and_mimaximization/|%0A|Filename|https://github.com/dp0d/dp0d.github.io/posts/2023/BERT_embedding_and_MIMaximization/index.md|" target=_blank rel="noopener noreferrer">报告问题</a></span></div><div class=post-info-share><span><a href=# onclick=return!1 title="分享到 Twitter" data-sharer=twitter data-url=https://dp0d.github.io/bert_embedding_and_mimaximization/ data-title="《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记"><i class="fab fa-twitter fa-fw"></i></a><a href=# onclick=return!1 title="分享到 Facebook" data-sharer=facebook data-url=https://dp0d.github.io/bert_embedding_and_mimaximization/><i class="fab fa-facebook-square fa-fw"></i></a><a href=# onclick=return!1 title="分享到 微博" data-sharer=weibo data-url=https://dp0d.github.io/bert_embedding_and_mimaximization/ data-title="《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记"><i class="fab fa-weibo fa-fw"></i></a><a href=# onclick=return!1 title="分享到 Telegram" data-sharer=telegram data-url=https://dp0d.github.io/bert_embedding_and_mimaximization/ data-title="《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记" data-web><i class="fab fa-telegram-plane fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/clashtun/ class=prev rel=prev title="TUN模式 Clash for windows 配置教程"><i class="fas fa-angle-left fa-fw"></i>TUN模式 Clash for windows 配置教程</a></div></div><div id=comments><div id=giscus></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://giscus.app/>giscus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://github.com/dp0d target=_blank rel="noopener noreferrer">dp0d</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span><span class=icp-splitter>&nbsp;|&nbsp;</span><br class=icp-br><span class=icp><a href=https://beian.miit.gov.cn/ target=_blank>赣ICP备2022002313号-1</a></span></div><div class=footer-line></div><div class=footer-line></div></div><script>"serviceWorker"in navigator&&(navigator.serviceWorker.register("/sw.min.js",{scope:"/"}).then(function(){}),navigator.serviceWorker.ready.then(function(){}))</script></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><script type=text/javascript src=/lib/object-fit-images/ofi.min.js></script><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/fuse/fuse.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/topbar/topbar.min.js></script><script type=text/javascript src=/lib/pjax/pjax.min.js></script><script type=text/javascript src=/js/theme.min.js defer></script></div><div class=pjax-assets><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:100},comment:{giscus:{darkTheme:"dark",dataCategory:"General",dataCategoryId:"DIC_kwDOHB3uW84COZbL",dataEmitMetadata:"0",dataInputPosition:"top",dataLang:"zh-CN",dataMapping:"title",dataReactionsEnabled:"1",dataRepo:"dp0d/dp0d.github.io",dataRepoId:"R_kgDOHB3uWw",lightTheme:"light"}},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!0,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:50,threshold:.1,type:"fuse",useExtendedSearch:!1},sharerjs:!0,table:{sort:!0},twemoji:!0}</script><script type=text/javascript src=/js/giscus.min.js defer></script><script type=text/javascript src=/lib/twemoji/twemoji.min.js defer></script><script type=text/javascript src=/js/twemoji.min.js defer></script><script type=text/javascript src=/lib/lightgallery/lightgallery.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-thumbnail.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-zoom.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript></div></body></html>