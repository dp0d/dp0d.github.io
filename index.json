[{"categories":["tutorial"],"content":" 程序运行过程 让你设计一个JAVAC编译器，你会如何设计？先对.java文件进行Token分析，然后翻译成字节码文件。 类的生命周期常见的生命周期有对象的生命周期，类的生命周期，bin的生命周期。 装载：将class文件中我们需要的内容加载进JVM。 a)Class—»字节流 —»\u003e寻找器（类加载器） b)将这个字节流说代表的近态存储结构转换成方法取的运行时数据结构。 c)在java的堆中生成一个代表这个类的java.lang.class对象，作为方法区的访问入口。 2.链接 a)验证 文件及其内容正确性，文件格式，元数据，字节码，符号引用。 b)准备 c)解析 ","date":"2023-07-12","objectID":"/jvm/:0:1","series":[],"tags":[],"title":"一些JVM基础知识","uri":"/jvm/#heading"},{"categories":["tutorial"],"content":" 程序运行过程 让你设计一个JAVAC编译器，你会如何设计？先对.java文件进行Token分析，然后翻译成字节码文件。 类的生命周期常见的生命周期有对象的生命周期，类的生命周期，bin的生命周期。 装载：将class文件中我们需要的内容加载进JVM。 a)Class—»字节流 —»\u003e寻找器（类加载器） b)将这个字节流说代表的近态存储结构转换成方法取的运行时数据结构。 c)在java的堆中生成一个代表这个类的java.lang.class对象，作为方法区的访问入口。 2.链接 a)验证 文件及其内容正确性，文件格式，元数据，字节码，符号引用。 b)准备 c)解析 ","date":"2023-07-12","objectID":"/jvm/:0:1","series":[],"tags":[],"title":"一些JVM基础知识","uri":"/jvm/#程序运行过程"},{"categories":["tutorial"],"content":" 程序运行过程 让你设计一个JAVAC编译器，你会如何设计？先对.java文件进行Token分析，然后翻译成字节码文件。 类的生命周期常见的生命周期有对象的生命周期，类的生命周期，bin的生命周期。 装载：将class文件中我们需要的内容加载进JVM。 a)Class—»字节流 —»\u003e寻找器（类加载器） b)将这个字节流说代表的近态存储结构转换成方法取的运行时数据结构。 c)在java的堆中生成一个代表这个类的java.lang.class对象，作为方法区的访问入口。 2.链接 a)验证 文件及其内容正确性，文件格式，元数据，字节码，符号引用。 b)准备 c)解析 ","date":"2023-07-12","objectID":"/jvm/:0:1","series":[],"tags":[],"title":"一些JVM基础知识","uri":"/jvm/#让你设计一个javac编译器你会如何设计"},{"categories":["tutorial"],"content":" 程序运行过程 让你设计一个JAVAC编译器，你会如何设计？先对.java文件进行Token分析，然后翻译成字节码文件。 类的生命周期常见的生命周期有对象的生命周期，类的生命周期，bin的生命周期。 装载：将class文件中我们需要的内容加载进JVM。 a)Class—»字节流 —»\u003e寻找器（类加载器） b)将这个字节流说代表的近态存储结构转换成方法取的运行时数据结构。 c)在java的堆中生成一个代表这个类的java.lang.class对象，作为方法区的访问入口。 2.链接 a)验证 文件及其内容正确性，文件格式，元数据，字节码，符号引用。 b)准备 c)解析 ","date":"2023-07-12","objectID":"/jvm/:0:1","series":[],"tags":[],"title":"一些JVM基础知识","uri":"/jvm/#类的生命周期"},{"categories":["tutorial"],"content":" JAVA套件相关信息，来自JAVA官网，JDK包含了JVM。 ","date":"2023-07-12","objectID":"/jvm/:0:2","series":[],"tags":[],"title":"一些JVM基础知识","uri":"/jvm/#java套件相关信息来自java官网jdk包含了jvm"},{"categories":["paper_reading"],"content":" 原文链接：https://aclanthology.org/2021.findings-emnlp.203.pdf 源码链接：https://github.com/J-zin/DHIM ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:0:0","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#"},{"categories":["paper_reading"],"content":" 行文动机大多数文本哈希方法都是基于BOW和TFIDF学得的，而这种特征缺乏词序和依存关系的相关信息。然而，由于BERT embedding在下游任务中的出色表现，本文决定抛弃“落后”的BOW和TFIDF特征，转而直接建模由BERT嵌入表征的文本。文中将使用BOW和TFIDF的使用归因于是逃避对于长依赖关系带来的挑战（其实这种基于统计的特征是带有原始检索的潜在意义的，比如关键词感知等）。 创新：引入了最大互信息原则来解决BERT嵌入中存在过于丰富的信息的问题。具体而言，BERT embedding 富含信息，其中只有一小部分和哈希有关，所以哈希码不太能在语义一致上和嵌入一致。文章指出，在2019年的一篇高引论文5900+《Sentencebert: Sentence embeddings using siamese bertnetworks》中就曾得出过结论，原始的BERT嵌入在余弦相似度，曼哈顿距离，欧几里得距离上的表现不尽如人意。本文受《Learning deep representations by mutual information estimation and maximization》一文启发，通过最大互信息原则可以学得有目标区分度的表征，来更多突出语义信息以及削弱其他无关的信息。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:1:0","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#行文动机"},{"categories":["paper_reading"],"content":" 基于对比方法引入BERT嵌入基于BOW和TFIDF特征的文本哈希方法大都基于如下方式建模，给定一个由词序列表示的文档$\\boldsymbol x={w_1, w_2,\\cdots,w_{|x|}}$： $$ p(x, z)=\\prod_{w_{i} \\in x} p_{\\theta}\\left(w_{i} \\mid z\\right) p(z) $$ 其中 $$ p_{\\theta}\\left(w_{i} \\mid z\\right) \\triangleq \\frac{\\exp \\left(z^{T} E w_{i}+b_{i}\\right)}{\\sum_{j=1}^{|V|} \\exp \\left(z^{T} E w_{j}+b_{j}\\right)} $$ 由于BERT嵌入和BOW特征的差异性，解码过程替换为一个条件高斯分布 $$ p_{\\theta}(x|z) = \\frac{1}{{(2\\pi\\sigma^2)}^{d/2}}e^{-\\frac{||\\mathcal B(x) - Wz||^2}{2\\sigma^2}} $$ 其中$\\mathcal B(x)$为BERT嵌入，$W$为可学习的模型参数（解码网络），$d$为BERT嵌入维数（768）。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:2:0","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#基于对比方法引入bert嵌入"},{"categories":["paper_reading"],"content":" 本文方法——基于互信息最大化原则重构BERT嵌入","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:0","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#本文方法基于互信息最大化原则重构bert嵌入"},{"categories":["paper_reading"],"content":" 图像领域的互信息最大化方法《Learning deep representations by mutual information estimation and maximization》中通过最大化全局和局部表示之间的互信息来学习类别可区分的图像表示。该文第一次构造了对于图片的全局表示和众多的局部表示，它们都通过图像的卷积特征得到。既然有众多的局部表示，并且各自局部表示图像中的特定局部源。全局和局部表示的互信息使得全局表示更加关注全局语义，即所有局部语义信息的共性，而非局部表示之间的具体不同细节。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:1","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#图像领域的互信息最大化方法"},{"categories":["paper_reading"],"content":" 构建文本局部和全局表征 局部表征给定一篇文档，获取每一个词的嵌入表示，最终表示一篇文档为$X = {e_1, . . . , e_T}$，其中$e_i\\in \\mathbb R^d$表示第$i$个词的BERT嵌入表示， $T$表示文档总词数。将文档$X$输入文本卷积神经网络CNN，其中筛选器$W\\in \\mathbb R^{K\\times n\\times d}$， $n$和$K$表示筛选器的大小和数量。这种方式能够为每个 n-gram 片段生成局部表征，具体而言，第$i$个片段的局部表征如下计算，*表示卷积运算符： $$ h_i^{(n)}={\\rm RELU}(W* e_{i:i+n-1}) $$ 将其作用于所有的文本片段，则可以获得所有词对应位置的局部表征： $$ H^{(n)}=\\{h_1^{(n)},h_2^{(n)},\\cdots,h_T^{(n)}\\} $$ 全局表征将$H^{(n)}$ 传入$\\rm READOUT$函数，即平均池化函数或者更复杂的自注意力机制，可以在不受文档长度的影响下获得文档的全局表征。 $$ H = {\\rm READOUT}({h_i^{(n)}}_{n\\in\\mathcal N}) $$ 另外，为了进一步在全局表征中突出语义信息，提出采用三种不同切片方式的连接，即1-gram，3-gram，5-gram，最终的局部和全局表征可以如下计算： $$ h_i = {\\rm MLP(CONCAT}(\\{h_i^{(n)}\\}_{n\\in\\mathcal N})),\\\\ H = {\\rm READOUT}(\\{h_i\\}_{i=1}^T), $$ 其中$\\mathcal N$表示不同的窗口大小集合，在实验中$\\mathcal N={1,3,5}$如Figure 1所示。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:2","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#构建文本局部和全局表征"},{"categories":["paper_reading"],"content":" 构建文本局部和全局表征 局部表征给定一篇文档，获取每一个词的嵌入表示，最终表示一篇文档为$X = {e_1, . . . , e_T}$，其中$e_i\\in \\mathbb R^d$表示第$i$个词的BERT嵌入表示， $T$表示文档总词数。将文档$X$输入文本卷积神经网络CNN，其中筛选器$W\\in \\mathbb R^{K\\times n\\times d}$， $n$和$K$表示筛选器的大小和数量。这种方式能够为每个 n-gram 片段生成局部表征，具体而言，第$i$个片段的局部表征如下计算，*表示卷积运算符： $$ h_i^{(n)}={\\rm RELU}(W* e_{i:i+n-1}) $$ 将其作用于所有的文本片段，则可以获得所有词对应位置的局部表征： $$ H^{(n)}=\\{h_1^{(n)},h_2^{(n)},\\cdots,h_T^{(n)}\\} $$ 全局表征将$H^{(n)}$ 传入$\\rm READOUT$函数，即平均池化函数或者更复杂的自注意力机制，可以在不受文档长度的影响下获得文档的全局表征。 $$ H = {\\rm READOUT}({h_i^{(n)}}_{n\\in\\mathcal N}) $$ 另外，为了进一步在全局表征中突出语义信息，提出采用三种不同切片方式的连接，即1-gram，3-gram，5-gram，最终的局部和全局表征可以如下计算： $$ h_i = {\\rm MLP(CONCAT}(\\{h_i^{(n)}\\}_{n\\in\\mathcal N})),\\\\ H = {\\rm READOUT}(\\{h_i\\}_{i=1}^T), $$ 其中$\\mathcal N$表示不同的窗口大小集合，在实验中$\\mathcal N={1,3,5}$如Figure 1所示。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:2","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#局部表征"},{"categories":["paper_reading"],"content":" 构建文本局部和全局表征 局部表征给定一篇文档，获取每一个词的嵌入表示，最终表示一篇文档为$X = {e_1, . . . , e_T}$，其中$e_i\\in \\mathbb R^d$表示第$i$个词的BERT嵌入表示， $T$表示文档总词数。将文档$X$输入文本卷积神经网络CNN，其中筛选器$W\\in \\mathbb R^{K\\times n\\times d}$， $n$和$K$表示筛选器的大小和数量。这种方式能够为每个 n-gram 片段生成局部表征，具体而言，第$i$个片段的局部表征如下计算，*表示卷积运算符： $$ h_i^{(n)}={\\rm RELU}(W* e_{i:i+n-1}) $$ 将其作用于所有的文本片段，则可以获得所有词对应位置的局部表征： $$ H^{(n)}=\\{h_1^{(n)},h_2^{(n)},\\cdots,h_T^{(n)}\\} $$ 全局表征将$H^{(n)}$ 传入$\\rm READOUT$函数，即平均池化函数或者更复杂的自注意力机制，可以在不受文档长度的影响下获得文档的全局表征。 $$ H = {\\rm READOUT}({h_i^{(n)}}_{n\\in\\mathcal N}) $$ 另外，为了进一步在全局表征中突出语义信息，提出采用三种不同切片方式的连接，即1-gram，3-gram，5-gram，最终的局部和全局表征可以如下计算： $$ h_i = {\\rm MLP(CONCAT}(\\{h_i^{(n)}\\}_{n\\in\\mathcal N})),\\\\ H = {\\rm READOUT}(\\{h_i\\}_{i=1}^T), $$ 其中$\\mathcal N$表示不同的窗口大小集合，在实验中$\\mathcal N={1,3,5}$如Figure 1所示。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:2","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#全局表征"},{"categories":["paper_reading"],"content":" 基于最大化局部和全局表征互信息的端到端的哈希 首先生成基于局部和全局表征的哈希码，通过采样如下伯努利分布得到： $$ b_i\\sim {\\rm Bernoulli}(\\sigma(h_i)),\\\\ B\\sim {\\rm Bernoulli(\\sigma(H))}, $$ $b_i$ 和$B$表示局部和全局对应的哈希码。接下来最大化他们之间的互信息。 $$ \\hat{\\theta}=\\arg \\max_{\\theta}\\frac{1}{T}\\sum^T_{i=1}I(b_i,B), $$ $b_i$ 和$B$表示对于特定文档而言不是唯一对应的，这使得互信息难以评估。 最近，有许多方法被提出用来估计互信息，如MINE，infoNCE，Jensen-Shannon divergence estimator (JSDE)，其中JSDE对负样本数量较不敏感，于是本文使用JSDE来估计互信息并优化它。具体而言，互信息可以通过最小化如下公式来估计： $$ \\begin{aligned} \\tilde{I_{\\phi}}(b_i;B)=\u0026-{\\rm softplus}(-D_{\\phi}(b_i,B))\\\\ \u0026-\\mathbb E_{\\tilde{\\mathbb P}}{\\rm softplus}(-D_{\\phi}(\\tilde{b_i},B)) \\end{aligned} $$ 其中$\\tilde{b_i}$表示从经验分布$\\tilde{\\mathbb P}=\\mathbb P$生成的负样本的第$i$个局部表征，在实际实验中负样本$\\tilde{b_i}$是在局部表征中选取的或从batch的其他文档中选取的。$D_{\\phi}(\\cdot,\\cdot)$是由神经网络实现的判别器。${\\rm softplus}(x)\\triangleq\\log (1+e^x)$。 上述最大化互信息框架仅仅依赖BERT对单个单词的嵌入，忽略了BERT的CLS嵌入。因此，为了提高学得哈希码中的全局语义信息,添加一个正则化项来提高哈希码和CLS嵌入之间的互信息，最终的Loss如下： $$ \\tilde{\\mathcal L}(\\phi,\\theta)=-\\frac{1}{T}\\sum^T_{i=1}\\tilde{I_{\\phi}}(b_i;B)-\\beta(E,B), $$ $\\beta$ 是超参数，$E$表示二值化后的CLS嵌入， $\\theta$是构造局部和全局二值码 $b_i$ 和$B$过程中涉及的模型参数。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:3","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#基于最大化局部和全局表征互信息的端到端的哈希"},{"categories":["paper_reading"],"content":" 实验部分 数据集信息 对比方法和细节 评估方法 总体结果DHIM即本文模型，无类别标签监督的模型，相比基线模型有所提升，也印证了前文提到的胜澈给你模型的相关工作——单纯重构BERT嵌入哈希码面临的性能问题。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:4","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#实验部分"},{"categories":["paper_reading"],"content":" 实验部分 数据集信息 对比方法和细节 评估方法 总体结果DHIM即本文模型，无类别标签监督的模型，相比基线模型有所提升，也印证了前文提到的胜澈给你模型的相关工作——单纯重构BERT嵌入哈希码面临的性能问题。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:4","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#数据集信息"},{"categories":["paper_reading"],"content":" 实验部分 数据集信息 对比方法和细节 评估方法 总体结果DHIM即本文模型，无类别标签监督的模型，相比基线模型有所提升，也印证了前文提到的胜澈给你模型的相关工作——单纯重构BERT嵌入哈希码面临的性能问题。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:4","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#对比方法和细节"},{"categories":["paper_reading"],"content":" 实验部分 数据集信息 对比方法和细节 评估方法 总体结果DHIM即本文模型，无类别标签监督的模型，相比基线模型有所提升，也印证了前文提到的胜澈给你模型的相关工作——单纯重构BERT嵌入哈希码面临的性能问题。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:4","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#评估方法"},{"categories":["paper_reading"],"content":" 实验部分 数据集信息 对比方法和细节 评估方法 总体结果DHIM即本文模型，无类别标签监督的模型，相比基线模型有所提升，也印证了前文提到的胜澈给你模型的相关工作——单纯重构BERT嵌入哈希码面临的性能问题。 ","date":"2023-07-11","objectID":"/bert_embedding_and_mimaximization/:3:4","series":[],"tags":[],"title":"《Refining BERT Embeddings for Document Hashing via Mutual Information Maximization》阅读笔记","uri":"/bert_embedding_and_mimaximization/#总体结果"},{"categories":["tutorial"],"content":" TUN模式首先理解TUN模式，这是一个真正意义上的全局的代理模式，它会在你的网络适配器中新建一个Clash专用的，可以理解为软路由中转网卡，开启TUN模式如下图所示。 然后可以在网络适配器中找到clash for windows 客户端专用的网络适配器，所有进入本机的流量都会经过这个适配器，如下图所示 双击打开它进行如下设置，注意更改DNS解析，如果clash for windows 客户端给了默认的有可能导致网页无法打开，选择自动获取DNS服务器地址即可。 然后回到clash for windows 客户端，一般机场给的配置文件里都会有规则，我们切换到按规则选择节点即可，如下图所示 然后我们打开浏览器进行测试，分别访问百度和古🐶，这里不进行演示了。 访问完了之后我们到clash for windows 客户端进行检查一下看下流量怎么走的，如下图所示，可以看到前者是走了代理的，而后者是直连。 ","date":"2023-04-05","objectID":"/clashtun/:0:1","series":[],"tags":["tools","win"],"title":"TUN模式 Clash for windows 配置教程","uri":"/clashtun/#tun模式"},{"categories":["tutorial"],"content":" Proxy SwitchyOmega 插件模式如果你的#Clash规则不好用，或是#只需要浏览器代理，可以借用Proxy SwitchyOmega浏览器插件来完成代理。 以谷歌浏览器为例，Edge应该也有，先在应用商店找到这个插件，然后安装到浏览器上 完成后你的浏览器上应该会出现以下标志在插件栏里 基于Proxy SwitchyOmega，你可以选择两种规则方式，下面分别介绍这两种方式的具体设置。 方式一 使用clash自带规则 首先打开clash for windows客户端，确认clash服务端口 如下图所示，记住这个端口（可以自行设置）。 7890 然后回到浏览器，打开Proxy SwitchyOmega插件的选项卡 新建一个名为clash的情景模式，并做好如下设置保存 下图中的代理端口改为刚刚记住的端口，代理协议和服务器如图所示为HTTP和本地回环地址。 接下来我们回到clash for windows 客户端，设置按规则代理 打开浏览器，选择我们刚刚设置的插件情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 分流成功 方式二 使用网络来源规则 打开浏览器插件，在建立好方式一中浏览器clash情景模式的基础下新建一个auto switch情景模式 对这个情景模式进行如下设置 使用这个开源规则列表（也可以替换成别的） https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 完成设置后我们回到clash for windows 客户端，设置指定代理 回到浏览器打开插件的auto switch情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 可以看到，clash for windows 客户端在访问百度时压根没有connection记录，说明Proxy SwitchyOmega插件使得浏览器在访问某些网站时没有走代理，而某些网站走了代理。 分流成功 手动指定某个域名走或不走代理 如果对于规则里某些网站你希望手动设置他走代理或者不走代理，可以使用如下方式 打开浏览器，点击Proxy SwitchyOmega插件 如果是针对某个域名下的网站我都希望走或者不走代理 比如我希望百度根域名下的所有网站我都走clash情景模式下的代理，我可以如下设置。 或者直接将当前网站下 都走clash情景模式代理，比如我当前在百度主页，她会自动提示百度下的，勾选红框中的clash即可。 底层逻辑是在Proxy SwitchyOmega插件选项卡中，上述两种方式会自动在分流规则列表之前添加优先级更高的匹配规则如下图所示，因此即便使用了分流规则，目标网站也会按照你设置的方式走或不走代理。 接下来我们在此基础上再访问百度 发现百度在我们有分流规则的前提下依然走了代理。 某GPT更换节点设置方法当某GPT挂了时，按照Proxy SwitchyOmega插件方式二设置，并尝试切换不同地域节点。 注意，在更换完节点之后，需要关闭浏览器或者在clash for windows客户端中断开维持的TCP连接，这样才能实际更换代理节点。 然后重新访问某GPT即可。 如果想要某GPT每次访问都是使用US节点，可以使用方式一来设置代理，并手动改写Clash for windows客户端配置文件，方式一配置完成后，后续步骤如下。 打开Clash for windows客户端配置文件 首先找到你的配置文件中，有没有特别给出的目标节点配置的分组，如果没有可能比较麻烦，有的话就按以下配置，以美国节点为例，复制这个名称。 接下来找到Rules配置，添加以下一行即可（也可以更换成openai.com范围更广），保存并刷新clash配置文件（双击配置文件图标即可） 接下来我们选择clash for windows 客户端配置如下 访问测试 可以看到，在规则中，需要代理的网站大部分走了自动选择下的节点，而某GPT网站走了我们设置的US节点。 ","date":"2023-04-05","objectID":"/clashtun/:0:2","series":[],"tags":["tools","win"],"title":"TUN模式 Clash for windows 配置教程","uri":"/clashtun/#proxy-switchyomega-插件模式"},{"categories":["tutorial"],"content":" Proxy SwitchyOmega 插件模式如果你的#Clash规则不好用，或是#只需要浏览器代理，可以借用Proxy SwitchyOmega浏览器插件来完成代理。 以谷歌浏览器为例，Edge应该也有，先在应用商店找到这个插件，然后安装到浏览器上 完成后你的浏览器上应该会出现以下标志在插件栏里 基于Proxy SwitchyOmega，你可以选择两种规则方式，下面分别介绍这两种方式的具体设置。 方式一 使用clash自带规则 首先打开clash for windows客户端，确认clash服务端口 如下图所示，记住这个端口（可以自行设置）。 7890 然后回到浏览器，打开Proxy SwitchyOmega插件的选项卡 新建一个名为clash的情景模式，并做好如下设置保存 下图中的代理端口改为刚刚记住的端口，代理协议和服务器如图所示为HTTP和本地回环地址。 接下来我们回到clash for windows 客户端，设置按规则代理 打开浏览器，选择我们刚刚设置的插件情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 分流成功 方式二 使用网络来源规则 打开浏览器插件，在建立好方式一中浏览器clash情景模式的基础下新建一个auto switch情景模式 对这个情景模式进行如下设置 使用这个开源规则列表（也可以替换成别的） https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 完成设置后我们回到clash for windows 客户端，设置指定代理 回到浏览器打开插件的auto switch情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 可以看到，clash for windows 客户端在访问百度时压根没有connection记录，说明Proxy SwitchyOmega插件使得浏览器在访问某些网站时没有走代理，而某些网站走了代理。 分流成功 手动指定某个域名走或不走代理 如果对于规则里某些网站你希望手动设置他走代理或者不走代理，可以使用如下方式 打开浏览器，点击Proxy SwitchyOmega插件 如果是针对某个域名下的网站我都希望走或者不走代理 比如我希望百度根域名下的所有网站我都走clash情景模式下的代理，我可以如下设置。 或者直接将当前网站下 都走clash情景模式代理，比如我当前在百度主页，她会自动提示百度下的，勾选红框中的clash即可。 底层逻辑是在Proxy SwitchyOmega插件选项卡中，上述两种方式会自动在分流规则列表之前添加优先级更高的匹配规则如下图所示，因此即便使用了分流规则，目标网站也会按照你设置的方式走或不走代理。 接下来我们在此基础上再访问百度 发现百度在我们有分流规则的前提下依然走了代理。 某GPT更换节点设置方法当某GPT挂了时，按照Proxy SwitchyOmega插件方式二设置，并尝试切换不同地域节点。 注意，在更换完节点之后，需要关闭浏览器或者在clash for windows客户端中断开维持的TCP连接，这样才能实际更换代理节点。 然后重新访问某GPT即可。 如果想要某GPT每次访问都是使用US节点，可以使用方式一来设置代理，并手动改写Clash for windows客户端配置文件，方式一配置完成后，后续步骤如下。 打开Clash for windows客户端配置文件 首先找到你的配置文件中，有没有特别给出的目标节点配置的分组，如果没有可能比较麻烦，有的话就按以下配置，以美国节点为例，复制这个名称。 接下来找到Rules配置，添加以下一行即可（也可以更换成openai.com范围更广），保存并刷新clash配置文件（双击配置文件图标即可） 接下来我们选择clash for windows 客户端配置如下 访问测试 可以看到，在规则中，需要代理的网站大部分走了自动选择下的节点，而某GPT网站走了我们设置的US节点。 ","date":"2023-04-05","objectID":"/clashtun/:0:2","series":[],"tags":["tools","win"],"title":"TUN模式 Clash for windows 配置教程","uri":"/clashtun/#方式一--使用clash自带规则"},{"categories":["tutorial"],"content":" Proxy SwitchyOmega 插件模式如果你的#Clash规则不好用，或是#只需要浏览器代理，可以借用Proxy SwitchyOmega浏览器插件来完成代理。 以谷歌浏览器为例，Edge应该也有，先在应用商店找到这个插件，然后安装到浏览器上 完成后你的浏览器上应该会出现以下标志在插件栏里 基于Proxy SwitchyOmega，你可以选择两种规则方式，下面分别介绍这两种方式的具体设置。 方式一 使用clash自带规则 首先打开clash for windows客户端，确认clash服务端口 如下图所示，记住这个端口（可以自行设置）。 7890 然后回到浏览器，打开Proxy SwitchyOmega插件的选项卡 新建一个名为clash的情景模式，并做好如下设置保存 下图中的代理端口改为刚刚记住的端口，代理协议和服务器如图所示为HTTP和本地回环地址。 接下来我们回到clash for windows 客户端，设置按规则代理 打开浏览器，选择我们刚刚设置的插件情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 分流成功 方式二 使用网络来源规则 打开浏览器插件，在建立好方式一中浏览器clash情景模式的基础下新建一个auto switch情景模式 对这个情景模式进行如下设置 使用这个开源规则列表（也可以替换成别的） https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 完成设置后我们回到clash for windows 客户端，设置指定代理 回到浏览器打开插件的auto switch情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 可以看到，clash for windows 客户端在访问百度时压根没有connection记录，说明Proxy SwitchyOmega插件使得浏览器在访问某些网站时没有走代理，而某些网站走了代理。 分流成功 手动指定某个域名走或不走代理 如果对于规则里某些网站你希望手动设置他走代理或者不走代理，可以使用如下方式 打开浏览器，点击Proxy SwitchyOmega插件 如果是针对某个域名下的网站我都希望走或者不走代理 比如我希望百度根域名下的所有网站我都走clash情景模式下的代理，我可以如下设置。 或者直接将当前网站下 都走clash情景模式代理，比如我当前在百度主页，她会自动提示百度下的，勾选红框中的clash即可。 底层逻辑是在Proxy SwitchyOmega插件选项卡中，上述两种方式会自动在分流规则列表之前添加优先级更高的匹配规则如下图所示，因此即便使用了分流规则，目标网站也会按照你设置的方式走或不走代理。 接下来我们在此基础上再访问百度 发现百度在我们有分流规则的前提下依然走了代理。 某GPT更换节点设置方法当某GPT挂了时，按照Proxy SwitchyOmega插件方式二设置，并尝试切换不同地域节点。 注意，在更换完节点之后，需要关闭浏览器或者在clash for windows客户端中断开维持的TCP连接，这样才能实际更换代理节点。 然后重新访问某GPT即可。 如果想要某GPT每次访问都是使用US节点，可以使用方式一来设置代理，并手动改写Clash for windows客户端配置文件，方式一配置完成后，后续步骤如下。 打开Clash for windows客户端配置文件 首先找到你的配置文件中，有没有特别给出的目标节点配置的分组，如果没有可能比较麻烦，有的话就按以下配置，以美国节点为例，复制这个名称。 接下来找到Rules配置，添加以下一行即可（也可以更换成openai.com范围更广），保存并刷新clash配置文件（双击配置文件图标即可） 接下来我们选择clash for windows 客户端配置如下 访问测试 可以看到，在规则中，需要代理的网站大部分走了自动选择下的节点，而某GPT网站走了我们设置的US节点。 ","date":"2023-04-05","objectID":"/clashtun/:0:2","series":[],"tags":["tools","win"],"title":"TUN模式 Clash for windows 配置教程","uri":"/clashtun/#方式二--使用网络来源规则"},{"categories":["tutorial"],"content":" Proxy SwitchyOmega 插件模式如果你的#Clash规则不好用，或是#只需要浏览器代理，可以借用Proxy SwitchyOmega浏览器插件来完成代理。 以谷歌浏览器为例，Edge应该也有，先在应用商店找到这个插件，然后安装到浏览器上 完成后你的浏览器上应该会出现以下标志在插件栏里 基于Proxy SwitchyOmega，你可以选择两种规则方式，下面分别介绍这两种方式的具体设置。 方式一 使用clash自带规则 首先打开clash for windows客户端，确认clash服务端口 如下图所示，记住这个端口（可以自行设置）。 7890 然后回到浏览器，打开Proxy SwitchyOmega插件的选项卡 新建一个名为clash的情景模式，并做好如下设置保存 下图中的代理端口改为刚刚记住的端口，代理协议和服务器如图所示为HTTP和本地回环地址。 接下来我们回到clash for windows 客户端，设置按规则代理 打开浏览器，选择我们刚刚设置的插件情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 分流成功 方式二 使用网络来源规则 打开浏览器插件，在建立好方式一中浏览器clash情景模式的基础下新建一个auto switch情景模式 对这个情景模式进行如下设置 使用这个开源规则列表（也可以替换成别的） https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 完成设置后我们回到clash for windows 客户端，设置指定代理 回到浏览器打开插件的auto switch情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 可以看到，clash for windows 客户端在访问百度时压根没有connection记录，说明Proxy SwitchyOmega插件使得浏览器在访问某些网站时没有走代理，而某些网站走了代理。 分流成功 手动指定某个域名走或不走代理 如果对于规则里某些网站你希望手动设置他走代理或者不走代理，可以使用如下方式 打开浏览器，点击Proxy SwitchyOmega插件 如果是针对某个域名下的网站我都希望走或者不走代理 比如我希望百度根域名下的所有网站我都走clash情景模式下的代理，我可以如下设置。 或者直接将当前网站下 都走clash情景模式代理，比如我当前在百度主页，她会自动提示百度下的，勾选红框中的clash即可。 底层逻辑是在Proxy SwitchyOmega插件选项卡中，上述两种方式会自动在分流规则列表之前添加优先级更高的匹配规则如下图所示，因此即便使用了分流规则，目标网站也会按照你设置的方式走或不走代理。 接下来我们在此基础上再访问百度 发现百度在我们有分流规则的前提下依然走了代理。 某GPT更换节点设置方法当某GPT挂了时，按照Proxy SwitchyOmega插件方式二设置，并尝试切换不同地域节点。 注意，在更换完节点之后，需要关闭浏览器或者在clash for windows客户端中断开维持的TCP连接，这样才能实际更换代理节点。 然后重新访问某GPT即可。 如果想要某GPT每次访问都是使用US节点，可以使用方式一来设置代理，并手动改写Clash for windows客户端配置文件，方式一配置完成后，后续步骤如下。 打开Clash for windows客户端配置文件 首先找到你的配置文件中，有没有特别给出的目标节点配置的分组，如果没有可能比较麻烦，有的话就按以下配置，以美国节点为例，复制这个名称。 接下来找到Rules配置，添加以下一行即可（也可以更换成openai.com范围更广），保存并刷新clash配置文件（双击配置文件图标即可） 接下来我们选择clash for windows 客户端配置如下 访问测试 可以看到，在规则中，需要代理的网站大部分走了自动选择下的节点，而某GPT网站走了我们设置的US节点。 ","date":"2023-04-05","objectID":"/clashtun/:0:2","series":[],"tags":["tools","win"],"title":"TUN模式 Clash for windows 配置教程","uri":"/clashtun/#手动指定某个域名走或不走代理"},{"categories":["tutorial"],"content":" Proxy SwitchyOmega 插件模式如果你的#Clash规则不好用，或是#只需要浏览器代理，可以借用Proxy SwitchyOmega浏览器插件来完成代理。 以谷歌浏览器为例，Edge应该也有，先在应用商店找到这个插件，然后安装到浏览器上 完成后你的浏览器上应该会出现以下标志在插件栏里 基于Proxy SwitchyOmega，你可以选择两种规则方式，下面分别介绍这两种方式的具体设置。 方式一 使用clash自带规则 首先打开clash for windows客户端，确认clash服务端口 如下图所示，记住这个端口（可以自行设置）。 7890 然后回到浏览器，打开Proxy SwitchyOmega插件的选项卡 新建一个名为clash的情景模式，并做好如下设置保存 下图中的代理端口改为刚刚记住的端口，代理协议和服务器如图所示为HTTP和本地回环地址。 接下来我们回到clash for windows 客户端，设置按规则代理 打开浏览器，选择我们刚刚设置的插件情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 分流成功 方式二 使用网络来源规则 打开浏览器插件，在建立好方式一中浏览器clash情景模式的基础下新建一个auto switch情景模式 对这个情景模式进行如下设置 使用这个开源规则列表（也可以替换成别的） https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt 完成设置后我们回到clash for windows 客户端，设置指定代理 回到浏览器打开插件的auto switch情景模式 访问百度和古🐶 回到clash for windows 客户端查看访问测试 可以看到，clash for windows 客户端在访问百度时压根没有connection记录，说明Proxy SwitchyOmega插件使得浏览器在访问某些网站时没有走代理，而某些网站走了代理。 分流成功 手动指定某个域名走或不走代理 如果对于规则里某些网站你希望手动设置他走代理或者不走代理，可以使用如下方式 打开浏览器，点击Proxy SwitchyOmega插件 如果是针对某个域名下的网站我都希望走或者不走代理 比如我希望百度根域名下的所有网站我都走clash情景模式下的代理，我可以如下设置。 或者直接将当前网站下 都走clash情景模式代理，比如我当前在百度主页，她会自动提示百度下的，勾选红框中的clash即可。 底层逻辑是在Proxy SwitchyOmega插件选项卡中，上述两种方式会自动在分流规则列表之前添加优先级更高的匹配规则如下图所示，因此即便使用了分流规则，目标网站也会按照你设置的方式走或不走代理。 接下来我们在此基础上再访问百度 发现百度在我们有分流规则的前提下依然走了代理。 某GPT更换节点设置方法当某GPT挂了时，按照Proxy SwitchyOmega插件方式二设置，并尝试切换不同地域节点。 注意，在更换完节点之后，需要关闭浏览器或者在clash for windows客户端中断开维持的TCP连接，这样才能实际更换代理节点。 然后重新访问某GPT即可。 如果想要某GPT每次访问都是使用US节点，可以使用方式一来设置代理，并手动改写Clash for windows客户端配置文件，方式一配置完成后，后续步骤如下。 打开Clash for windows客户端配置文件 首先找到你的配置文件中，有没有特别给出的目标节点配置的分组，如果没有可能比较麻烦，有的话就按以下配置，以美国节点为例，复制这个名称。 接下来找到Rules配置，添加以下一行即可（也可以更换成openai.com范围更广），保存并刷新clash配置文件（双击配置文件图标即可） 接下来我们选择clash for windows 客户端配置如下 访问测试 可以看到，在规则中，需要代理的网站大部分走了自动选择下的节点，而某GPT网站走了我们设置的US节点。 ","date":"2023-04-05","objectID":"/clashtun/:0:2","series":[],"tags":["tools","win"],"title":"TUN模式 Clash for windows 配置教程","uri":"/clashtun/#某gpt更换节点设置方法"},{"categories":[],"content":" 混淆矩阵 ","date":"2022-11-30","objectID":"/confusion_matrix/:1:0","series":[],"tags":[],"title":"小解一波准召率","uri":"/confusion_matrix/#混淆矩阵"},{"categories":[],"content":" 精确率 Precision（通常指micro）$$ Precision = \\frac{TP}{TP+FP} $$ ","date":"2022-11-30","objectID":"/confusion_matrix/:2:0","series":[],"tags":[],"title":"小解一波准召率","uri":"/confusion_matrix/#精确率-precision通常指micro"},{"categories":[],"content":" 查全率 Recall$$ Recall = \\frac{TP}{TP+FN} $$ ","date":"2022-11-30","objectID":"/confusion_matrix/:3:0","series":[],"tags":[],"title":"小解一波准召率","uri":"/confusion_matrix/#查全率-recall"},{"categories":[],"content":" F1 (查准率和查全率的调和平均数)$$ F1 = \\frac{2\\cdot Precision \\cdot Recall}{Precision+Recall} $$ ","date":"2022-11-30","objectID":"/confusion_matrix/:4:0","series":[],"tags":[],"title":"小解一波准召率","uri":"/confusion_matrix/#f1-查准率和查全率的调和平均数"},{"categories":["tutorial"],"content":" 安装 pip install wandb ","date":"2022-11-22","objectID":"/wandb/:1:0","series":[],"tags":["tools"],"title":"wandb——深度学习的优秀伴侣","uri":"/wandb/#安装"},{"categories":["tutorial"],"content":" 记录实验结果 import wandb import argparse parser.add_argument(\"--train_batch_size\", default=100, type=int) parser.add_argument(\"--test_batch_size\", default=100, type=int) parser.add_argument(\"--num_epochs\", default=60, type=int) parser.add_argument(\"-lr\", \"--learning_rate\", default=0.0005, type=float) args = parser.parse_args() # 设置随机种子 seed_everthing(2022) # 初始化wandb wandb.init(project=\"project_name\", entity=\"dp0d\") # 记录超参 wandb.config.update(vars(args)) # vars将args转为字典类型 ... # 选择你要记录的指标 wandb.log({\"Total Loss\": total_loss.item()}) wandb.log({\"precision\": pre}) ... ","date":"2022-11-22","objectID":"/wandb/:2:0","series":[],"tags":["tools"],"title":"wandb——深度学习的优秀伴侣","uri":"/wandb/#记录实验结果"},{"categories":["tutorial"],"content":" 超参搜索 import wandb import argparse parser.add_argument(\"--train_batch_size\", default=100, type=int) parser.add_argument(\"--test_batch_size\", default=100, type=int) parser.add_argument(\"--num_epochs\", default=60, type=int) parser.add_argument(\"-lr\", \"--learning_rate\", default=0.0005, type=float) parser.add_argument(\"--m1\", default=0.01, type=float, help=\"margin 1\") parser.add_argument(\"--m2\", default=0.01, type=float, help=\"margin 2\") parser.add_argument(\"--do_sweep\", default=1, type=bool, help=\"do sweep\") args = parser.parse_args() # 设置随机种子 seed_everthing(2022) # 数据准备可以放外面 ... train_dataloader = DataLoader( train_data, batch_size=args.train_batch_size, shuffle=True, ) ... # 一定把每次必须进行的主流程放在main()里边 def main(): seed_everthing(2022) # 注意一定要在主流程函数里面再设置一遍随机种子 # 初始化wandb wandb.init(project=\"project_name\", entity=\"dp0d\") # 关键步骤，将需要搜索的超参重新赋值 if args.do_sweep: args.m1 = wandb.config.m1 args.m2 = wandb.config.m2 model = Mymodel(args) model.to(device) optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate) ... wandb.log({\"precision\": pre}) ... if args.do_sweep: sweep_config = { 'method': 'bayes', # bayes方法会根据高斯方法自动搜索超参 'name': 'sweep__2022', 'metric': {'goal': 'maximize', 'name': 'precision'}, 'parameters': { 'm1': { 'min': 0.005, 'max': 0.015 }, 'm2': { 'min': 0.005, 'max': 0.015 }, } } # Initialize sweep by passing in config. (Optional) Provide a name of the project. sweep_id = wandb.sweep(sweep=sweep_config, project='project_name') # Start sweep job through main function. wandb.agent(sweep_id, function=main, count=200) # 指定搜索次数为200和入口函数为main else: main() sweep的其他配置方法 sweep_config_1 = { 'method': 'bayes', # bayes方法会根据高斯方法自动搜索超参 'name': 'sweep__2022', 'metric': {'goal': 'maximize', 'name': 'precision'}, 'parameters': { 'm1': { 'values': [0.005,0.006] }, 'm2': { 'values': [0.005,0.001,0.0015] }, } } sweep_config_2 = { 'method': 'grid', # 网格法会检索列表里的所有超参组合 'name': 'sweep__2022', 'metric': {'goal': 'maximize', 'name': 'precision'}, 'parameters': { 'm1': { 'values': [0.005,0.006] }, 'm2': { 'values': [0.005,0.001,0.0015] }, } } sweep_config_2 = { 'method': 'random', # 随机法会随机检索列表里的所有超参组合（可能会重复）（不建议使用） 'name': 'sweep__2022', 'metric': {'goal': 'maximize', 'name': 'precision'}, 'parameters': { 'm1': { 'values': [0.005,0.006] }, 'm2': { 'values': [0.005,0.001,0.0015] }, } } 在另一台机器上启动超参搜索 ... ... # 其他部分代码不动 # 这行id注掉 # sweep_id = wandb.sweep(sweep=sweep_config, project='project_name') # 去wandb创建sweep的主页获取id填到下面，如获取的是 sr0cogu1 wandb.agent(sweep_id='dp0d/project_name/sr0cogu1', function=main, count=200) # 指定搜索次数为200和入口函数为main else: main() ","date":"2022-11-22","objectID":"/wandb/:3:0","series":[],"tags":["tools"],"title":"wandb——深度学习的优秀伴侣","uri":"/wandb/#超参搜索"},{"categories":["tutorial"],"content":" 超参搜索 import wandb import argparse parser.add_argument(\"--train_batch_size\", default=100, type=int) parser.add_argument(\"--test_batch_size\", default=100, type=int) parser.add_argument(\"--num_epochs\", default=60, type=int) parser.add_argument(\"-lr\", \"--learning_rate\", default=0.0005, type=float) parser.add_argument(\"--m1\", default=0.01, type=float, help=\"margin 1\") parser.add_argument(\"--m2\", default=0.01, type=float, help=\"margin 2\") parser.add_argument(\"--do_sweep\", default=1, type=bool, help=\"do sweep\") args = parser.parse_args() # 设置随机种子 seed_everthing(2022) # 数据准备可以放外面 ... train_dataloader = DataLoader( train_data, batch_size=args.train_batch_size, shuffle=True, ) ... # 一定把每次必须进行的主流程放在main()里边 def main(): seed_everthing(2022) # 注意一定要在主流程函数里面再设置一遍随机种子 # 初始化wandb wandb.init(project=\"project_name\", entity=\"dp0d\") # 关键步骤，将需要搜索的超参重新赋值 if args.do_sweep: args.m1 = wandb.config.m1 args.m2 = wandb.config.m2 model = Mymodel(args) model.to(device) optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate) ... wandb.log({\"precision\": pre}) ... if args.do_sweep: sweep_config = { 'method': 'bayes', # bayes方法会根据高斯方法自动搜索超参 'name': 'sweep__2022', 'metric': {'goal': 'maximize', 'name': 'precision'}, 'parameters': { 'm1': { 'min': 0.005, 'max': 0.015 }, 'm2': { 'min': 0.005, 'max': 0.015 }, } } # Initialize sweep by passing in config. (Optional) Provide a name of the project. sweep_id = wandb.sweep(sweep=sweep_config, project='project_name') # Start sweep job through main function. wandb.agent(sweep_id, function=main, count=200) # 指定搜索次数为200和入口函数为main else: main() sweep的其他配置方法 sweep_config_1 = { 'method': 'bayes', # bayes方法会根据高斯方法自动搜索超参 'name': 'sweep__2022', 'metric': {'goal': 'maximize', 'name': 'precision'}, 'parameters': { 'm1': { 'values': [0.005,0.006] }, 'm2': { 'values': [0.005,0.001,0.0015] }, } } sweep_config_2 = { 'method': 'grid', # 网格法会检索列表里的所有超参组合 'name': 'sweep__2022', 'metric': {'goal': 'maximize', 'name': 'precision'}, 'parameters': { 'm1': { 'values': [0.005,0.006] }, 'm2': { 'values': [0.005,0.001,0.0015] }, } } sweep_config_2 = { 'method': 'random', # 随机法会随机检索列表里的所有超参组合（可能会重复）（不建议使用） 'name': 'sweep__2022', 'metric': {'goal': 'maximize', 'name': 'precision'}, 'parameters': { 'm1': { 'values': [0.005,0.006] }, 'm2': { 'values': [0.005,0.001,0.0015] }, } } 在另一台机器上启动超参搜索 ... ... # 其他部分代码不动 # 这行id注掉 # sweep_id = wandb.sweep(sweep=sweep_config, project='project_name') # 去wandb创建sweep的主页获取id填到下面，如获取的是 sr0cogu1 wandb.agent(sweep_id='dp0d/project_name/sr0cogu1', function=main, count=200) # 指定搜索次数为200和入口函数为main else: main() ","date":"2022-11-22","objectID":"/wandb/:3:0","series":[],"tags":["tools"],"title":"wandb——深度学习的优秀伴侣","uri":"/wandb/#sweep的其他配置方法"},{"categories":[],"content":" dpkg是一个安装、创建、移除和管理debian包的工具。 安装软件 sudo dpkg -i *.deb ","date":"2022-11-06","objectID":"/package_manager/:1:1","series":[],"tags":[],"title":"Linux下的包管理工具介绍","uri":"/package_manager/#dpkg"},{"categories":["machine_learning"],"content":" 1.变分函数","date":"2022-09-15","objectID":"/variational_inference/:1:0","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#1变分函数"},{"categories":["machine_learning"],"content":" 微分（非严格）$$ dy = f(x_1+dx)-f(x_1) $$ $dy$是$y$的微分，也就是函数的微分，是指的$x$变化了一个极小值引起的因变量函数的变化。 ","date":"2022-09-15","objectID":"/variational_inference/:1:1","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#微分非严格"},{"categories":["machine_learning"],"content":" 泛函泛函指的是，因变量为函数的函数。 函数 泛函 $f(x_0) = y_0$ $\\phi(f_{\\theta_0}(x))=y_0$ 当自变量为一个特定值，函数加工的结果对应特定值 当自变量为一个特定的函数（可以理解为特定的分布）如$y=x$，泛函加工为一个特定的值 ","date":"2022-09-15","objectID":"/variational_inference/:1:2","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#泛函"},{"categories":["machine_learning"],"content":" 变分（实质上是泛函的变分）变分指的是，当自变量函数做出微小变化$y \\rarr y + \\delta y$时（可以理解为分布发生微小变化，如$y=x$变为$y=1.00001*x,y=x + 0.0001$），泛函的函数值变化了$\\delta J$,这个变化值就是泛函的变分。 ","date":"2022-09-15","objectID":"/variational_inference/:1:3","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#变分实质上是泛函的变分"},{"categories":["machine_learning"],"content":" 2.平均场理论$$ q(x)=\\prod_iq(x_i) $$ ","date":"2022-09-15","objectID":"/variational_inference/:2:0","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#2平均场理论"},{"categories":["machine_learning"],"content":" 3.机器学习分类（研究方法）","date":"2022-09-15","objectID":"/variational_inference/:3:0","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#3机器学习分类研究方法"},{"categories":["machine_learning"],"content":" 贝叶斯派学习的过程贝叶斯学习认为模型$p(x|\\theta)$中的参数$\\theta$不是一个确定的未知参数，而是一个随机变量，假设为$\\theta 服从 p(\\theta|\\lambda)$，所以，需要通过贝叶斯定理求得后验$p(\\theta|x)=\\frac{p(x|\\theta)p(\\theta|\\lambda)}{p(x)}$。 贝叶斯推断（Inference）需要计算$p(\\theta|x)$的值，抽样很多样本，使用期望来近似，贝叶斯公式中$p(x|\\theta)$是数据的似然和$p(\\theta|\\lambda)$ 是参数的先验分布，$p(x)$通过$\\int_{\\theta} p(x|\\theta)p(\\theta|\\lambda)d\\lambda$得到。 贝叶斯决策通过原有样本$X$来推断N个新的样本$\\hat x$，即 $$ p(\\hat x|X)=\\int_\\theta p(\\hat x,\\theta|X)d\\theta =\\int_\\theta p(\\hat x|\\theta)\\cdot p(\\theta|X)d\\theta =E_{\\theta|X}[p(\\hat x|\\theta)] $$ ","date":"2022-09-15","objectID":"/variational_inference/:3:1","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#贝叶斯派学习的过程"},{"categories":["machine_learning"],"content":" 贝叶斯派学习的过程贝叶斯学习认为模型$p(x|\\theta)$中的参数$\\theta$不是一个确定的未知参数，而是一个随机变量，假设为$\\theta 服从 p(\\theta|\\lambda)$，所以，需要通过贝叶斯定理求得后验$p(\\theta|x)=\\frac{p(x|\\theta)p(\\theta|\\lambda)}{p(x)}$。 贝叶斯推断（Inference）需要计算$p(\\theta|x)$的值，抽样很多样本，使用期望来近似，贝叶斯公式中$p(x|\\theta)$是数据的似然和$p(\\theta|\\lambda)$ 是参数的先验分布，$p(x)$通过$\\int_{\\theta} p(x|\\theta)p(\\theta|\\lambda)d\\lambda$得到。 贝叶斯决策通过原有样本$X$来推断N个新的样本$\\hat x$，即 $$ p(\\hat x|X)=\\int_\\theta p(\\hat x,\\theta|X)d\\theta =\\int_\\theta p(\\hat x|\\theta)\\cdot p(\\theta|X)d\\theta =E_{\\theta|X}[p(\\hat x|\\theta)] $$ ","date":"2022-09-15","objectID":"/variational_inference/:3:1","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#贝叶斯推断inference"},{"categories":["machine_learning"],"content":" 贝叶斯派学习的过程贝叶斯学习认为模型$p(x|\\theta)$中的参数$\\theta$不是一个确定的未知参数，而是一个随机变量，假设为$\\theta 服从 p(\\theta|\\lambda)$，所以，需要通过贝叶斯定理求得后验$p(\\theta|x)=\\frac{p(x|\\theta)p(\\theta|\\lambda)}{p(x)}$。 贝叶斯推断（Inference）需要计算$p(\\theta|x)$的值，抽样很多样本，使用期望来近似，贝叶斯公式中$p(x|\\theta)$是数据的似然和$p(\\theta|\\lambda)$ 是参数的先验分布，$p(x)$通过$\\int_{\\theta} p(x|\\theta)p(\\theta|\\lambda)d\\lambda$得到。 贝叶斯决策通过原有样本$X$来推断N个新的样本$\\hat x$，即 $$ p(\\hat x|X)=\\int_\\theta p(\\hat x,\\theta|X)d\\theta =\\int_\\theta p(\\hat x|\\theta)\\cdot p(\\theta|X)d\\theta =E_{\\theta|X}[p(\\hat x|\\theta)] $$ ","date":"2022-09-15","objectID":"/variational_inference/:3:1","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#贝叶斯决策"},{"categories":["machine_learning"],"content":" 变分推断(Variational Inference) 使用ELBO来替代，含有隐变量的概率模型中，观测数据的对数概率$\\log_{\\theta} p(x)$ 参数设定如下 $X$ : Observed data $Z$ : latent variable + parameter $(X,Z)$ : complete data + parameter 由于$x$是观测数据，$q(z|x)$简写为$q(z)$ 根据贝叶斯定理 $$ p(z|x) = \\frac{p(x|z)p(z)}{p(x)} = \\frac{p(x,z)}{p(x)} $$ 移项取log $$ \\log p(x)=\\log \\frac{p(x,z)}{p(z|x)}=\\log \\frac{\\frac{p(x,z)}{q(z)}}{\\frac {p(z|x)}{q(z)}}=\\log \\frac{p(x,z)}{q(z)} - \\log \\frac {p(z|x)}{q(z)} $$ 左右两边对$q(z)$求进行积分 $$ 左边=\\int_z q(z)\\log p(x)dz = \\log p(x) $$ $$ 右边=\\int q(z)\\log \\frac{p(x,z)}{q(z)}dz - \\int q(z) \\log \\frac {p(z|x)}{q(z)}dz $$ $$ 右边=\\underbrace{\\int q(z)\\log \\frac{p(x,z)}{q(z)}dz}_{ELBO(evidence\\;lower\\;bound)} + \\underbrace{\\int q(z) \\log \\frac {q(z)}{p(z|x)}}_{KL(q||p)}dz $$ $$ 右边=\\underbrace{\\mathcal{L}(q)}_{关于q的变分}+\\underbrace{KL(q||p)}_{\\geqslant 0} $$ 其中$\\mathcal{L}(q)$是用来定义ELBO项的函数，用以说明其输入是一个$q$函数，$q$函数是我们随意找的一个概率密度函数，所以$\\mathcal{L}(q)$是关于$q$的变分，变分推断来自于此。 说明一下，当X固定时，右边的和固定，而由于KL散度的性质，$KL(q||p)$恒大于零，所以$\\mathcal{L}(q)$最大就是$\\log p(x)$。 转为变分推断的关键问题，后验$p(z|x)$无法求得，所以需要使用$q(z)$来近似它，即使得$q(z)\\approx p(z|x)$。此时$KL(q||p)$最小，趋近于零。反过来思考，当找到一个$\\tilde {q(z)}$使得变分$\\mathcal{L}(q)$达到最大时，也能让$KL(q||p)$达到最小值，即使得$\\tilde {q(z)}\\approx p(z|x)$，形式化表达如下。 $$ \\tilde {q(z)} = \\mathop{\\arg\\max}_{q(z)}\\ \\ \\mathcal{L}(q)\\ \\ \\Rightarrow\\ \\ \\tilde {q(z)}\\approx p(z|x) $$ 平均场理论 $$ q(z) = \\prod_{i=1}^{M}q(z_i) $$ ","date":"2022-09-15","objectID":"/variational_inference/:4:0","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#变分推断variational-inference"},{"categories":["machine_learning"],"content":" 随机梯度变分推断 (Stochastic Gradient VI, SGVI) $$ \\log p_\\theta(X) = \\log \\prod_{i=1}^N p_{\\theta}(x^i)=\\sum_{i=1}^N\\log p_{\\theta}(x^i) $$ $$ \\log p_{\\theta}(x^i) = \\underbrace{ELBO}_{\\mathcal L(q)} + \\underbrace{KL(q||p)}_{\\geqslant 0}) \\geqslant \\mathcal L(q) $$ 需要求解的目标函数是 $$ \\tilde {q(z)} = \\mathop{\\arg\\min}_{q(z)}\\ \\ KL(q||p) =\\mathop{\\arg\\max}_{q(z)}\\ \\ \\mathcal{L}(q) $$ 假设$q(z)$的分布参数为$\\phi$,则变分$\\mathcal L(q)$可以写成$\\mathcal L(\\phi)$，即观测分布表示为 $$ \\log p_{\\theta}(x^i) = \\underbrace{ELBO}_{\\mathcal L(\\phi)} + \\underbrace{KL(q||p)}_{\\geqslant 0}) \\geqslant \\mathcal L(\\phi) $$ $$ ELBO = E_{q_{\\phi}(z)}[\\log \\frac{p_{\\theta}(x^i,z)}{q_{\\phi}(z)}] $$ 那么我们想要最大化$ELBO$时，$q(z)$分布的参数$\\phi$形式化表达为 $$ \\hat \\phi =\\mathop{\\arg\\max}_{\\phi} \\ \\mathcal L(\\phi) $$ 使用梯度随机梯度上升策略，需要求得$\\mathcal L(\\phi)$对$\\phi$的梯度如下。 $$ \\begin{equation} \\begin{aligned} \\nabla_{\\phi} \\mathcal L(\\phi) \u0026= \\nabla_{\\phi}E_{q_{\\phi}}[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)] \\\\ \u0026=\\nabla_{\\phi}\\int q_{\\phi}[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)]dz \\\\ \u0026=\\int \\nabla_{\\phi}q_{\\phi}\\cdot[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)]dz + \\int q_{\\phi}\\nabla_{\\phi}[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)]dz \\\\ \u0026=\\int q_{\\phi}\\cdot\\nabla_{\\phi}\\log q_{\\phi}\\cdot[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)]dz + 0 \\\\ \u0026=E_{q_{\\phi}}[\\nabla_{\\phi}\\log q_{\\phi}\\cdot[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)] \\end{aligned} \\end{equation} $$ 如果使用蒙特卡罗的方式，梯度$\\nabla_{\\phi} \\mathcal L(\\phi)$，即期望$E_{q_{\\phi}}[\\nabla_{\\phi}\\log q_{\\phi}\\cdot[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)]$可以通过从$q_{\\phi}(z)中采样L个z$得到，即 $$ z^{(l)} \\backsim q_{\\phi}(z),\\quad l=1,2,\\cdots,L \\\\ \\nabla_{\\phi} \\mathcal L(\\phi)\\approx \\frac{1}{L}\\sum_{l=1}^L \\nabla_{\\phi}\\log q_{\\phi}(z^i)\\cdot[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)] $$ 而$E_{q_{\\phi}}[\\underbrace{\\nabla_{\\phi}\\log q_{\\phi}}_{high\\ variance}\\cdot[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)]$在采样时，由于log函数的性质，当$q_{\\phi}(z)\\ \\rightarrow \\ 0$时，其中的$\\nabla_{\\phi}\\log q_{\\phi}$项会带来高方差问题。这导致了采样需要的样本量巨大，且误差较大，甚至可以认为无法采样。因此，无法采用蒙特卡洛方式来近似梯度$\\nabla_{\\phi} \\mathcal L(\\phi)$。 故现在需要降低梯度表达的方差，即Variance Reduction问题。使用重参数化技巧（Reparameterization Trick）。 ","date":"2022-09-15","objectID":"/variational_inference/:5:0","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#随机梯度变分推断-stochastic-gradient-vi-sgvi"},{"categories":["machine_learning"],"content":" Reparameterization Trick 目的是将随机变量$z$和$\\phi$的关系解耦，将$z$的随机成分转移到$\\epsilon$。 假设$z=g_{\\phi}(\\epsilon,x^i)$ ，$\\epsilon \\backsim p(\\epsilon)$，$z\\backsim q_{\\phi}(z|x^i)$，则由于$\\int q_{\\phi}(z|x^i)dz = \\int p(\\epsilon)d\\epsilon = 1$，可以得出$|q_{\\phi}(z|x^i)dz|=|p(\\epsilon)\\cdot d\\epsilon|$。 故梯度可以进行如下表达 $$ \\begin{equation} \\begin{aligned} \\nabla_{\\phi} \\mathcal L(\\phi) \u0026= \\nabla_{\\phi}\\underline{E_{q_{\\phi}}}[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)] \\\\ \u0026=\\nabla_{\\phi}\\int[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)]\\underline{q_{\\phi}dz} \\\\ \u0026=\\nabla_{\\phi}\\int[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)]\\underline{p(\\epsilon)d\\epsilon} \\\\ \u0026=\\nabla_{\\phi}\\underline{E_{p(\\epsilon)}}[\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z)] \\\\ \u0026=E_{p(\\epsilon)}[\\nabla_{\\phi}(\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z))] \\\\ 带入z=g_{\\phi}(\\epsilon,x^i)得， \\\\ \u0026=E_{p(\\epsilon)}[\\nabla_z(\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z))\\cdot \\underline{\\nabla_{\\phi}z}] \\\\ \u0026=E_{p(\\epsilon)}[\\nabla_z(\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z))\\cdot \\underline{\\nabla_{\\phi}g_{\\phi}(\\epsilon,x^i)}] \\end{aligned} \\end{equation} $$ 此时，便可以采用蒙特卡罗采样来近似梯度$\\nabla_{\\phi} \\mathcal L(\\phi)$，期望即是均值。 假设进行$L$次采样，$\\epsilon^{(l)}\\sim p(\\epsilon),\\ \\ l=1,2,\\cdots,L$。 $$ \\nabla_{\\phi} \\mathcal L(\\phi) \\approx \\widetilde {\\nabla_{\\phi} \\mathcal L(\\phi)}= \\frac{1}{L} \\sum_{l=1}^L\\nabla_z(\\log p_{\\theta}(x^i,z)-\\log q_{\\phi}(z))\\cdot \\nabla_{\\phi}g_{\\phi}(\\epsilon^{(l)},x^i) $$ SGVI训练过程如下 $$ \\phi^{(t+1)} \\leftarrow \\phi^{(t)} + \\lambda^{(t)} \\cdot \\widetilde {\\nabla_{\\phi} \\mathcal L(\\phi)}f $$ ","date":"2022-09-15","objectID":"/variational_inference/:5:1","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#reparameterization-trick"},{"categories":["machine_learning"],"content":" 变分自编码器VAE 问题场景 假设数据集$X=\\{x^{(i)}\\}^N_{i=1}$服从$N$ i.i.d.。该集合中的数据由某些随机过程生成而来，过程中含有无法观测的连续随机变量$\\bf z$。这个随机过程包含两个步骤，首先，从先验分布$p_{\\theta ^*}(\\bf z)$中生成一个$\\bf z^{(i)}$。步骤二是从条件分布$p_{\\theta ^*}(\\bf x |\\bf z)$中采样$\\bf x^{(i)}$。假设$p_{\\theta ^*}(\\bf z)$和$p_{\\theta ^*}(\\bf x |\\bf z)$来自$p_{\\theta}(\\bf z)$和$p_{\\theta}(\\bf x |\\bf z)$的参数家族，并且他们的表达在提及$\\theta$和$z$时都是可微的。这个过程的许多部分都是对我们不可见的：不论是真实的参数$\\theta ^*$还是隐变量$z^{(i)}$都是未知的。 隐变量模型 Latent Variable Model GMM 混合高斯模型，有限个高斯模型混合$z$~Categorical Dist VAE 无限个（infinite）高斯模型混合: $z \\sim N(0,\\bf I) $，$x|z \\sim N(\\mu_{\\theta}(z),\\sum_{\\theta}(z))$，得到如下建模， $$ p_{\\theta}(x) = \\int_zp_{\\theta}(x,z)dz \\ = \\ \\int_z p(z)\\cdot p_{\\theta}(x|z)dz $$ 其中$p_{\\theta}(x)$为intractable。 如果需要使用VAE生成一个样本，先从$p(z)$中采样一个$z^i$，然后使用$p_{\\theta}(x|z)$（实际采用一个神经网络来逼近，即Decoder）来得到$x^i $。 由变分推断可知 $$ \\log p(x) = ELBO + KL(q_{\\phi}(z|x)||p_{\\theta}(z|x)) $$ 优化目标如下 $$ \\begin{equation} \\begin{aligned} \u003c\\hat \\theta,\\hat \\phi\u003e \u0026= \\mathop{\\arg\\min}_{\u003c\\theta, \\phi\u003e}\\ KL(q_{\\phi}(z|x)||p_{\\theta}(z|x)) \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ ELBO \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x,z)]+H[q_{\\phi}] \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x,z)-\\log q_{\\phi}(z|x)] \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)+\\log p(z)-\\log q_{\\phi}(z|x)] \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]+\\int q_{\\phi}(z|x)\\frac{\\log p(z)}{\\log q_{\\phi}(z|x)}dz \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\underbrace{\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]}_{真正的目标函数}-\\underbrace{KL(q_{\\phi}(z|x)||p(z))}_{看做正则化项，使编码器不坍缩} \\end{aligned} \\end{equation} $$ 使用SGVI进行训练，重参数化技巧可以如下实现 $\\epsilon$可以看做采样的噪声，$\\epsilon \\backsim N(0,\\bf I)$。假设$z|x \\backsim N(\\mu_{\\phi}(x),\\Sigma_{\\phi}(x))$则 $$ z=\\mu_{\\phi}(x)+\\Sigma_{\\phi}^{\\frac{1}{2}}(x)\\cdot \\epsilon $$ ","date":"2022-09-15","objectID":"/variational_inference/:6:0","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#变分自编码器vae"},{"categories":["machine_learning"],"content":" 变分自编码器VAE 问题场景 假设数据集$X=\\{x^{(i)}\\}^N_{i=1}$服从$N$ i.i.d.。该集合中的数据由某些随机过程生成而来，过程中含有无法观测的连续随机变量$\\bf z$。这个随机过程包含两个步骤，首先，从先验分布$p_{\\theta ^*}(\\bf z)$中生成一个$\\bf z^{(i)}$。步骤二是从条件分布$p_{\\theta ^*}(\\bf x |\\bf z)$中采样$\\bf x^{(i)}$。假设$p_{\\theta ^*}(\\bf z)$和$p_{\\theta ^*}(\\bf x |\\bf z)$来自$p_{\\theta}(\\bf z)$和$p_{\\theta}(\\bf x |\\bf z)$的参数家族，并且他们的表达在提及$\\theta$和$z$时都是可微的。这个过程的许多部分都是对我们不可见的：不论是真实的参数$\\theta ^*$还是隐变量$z^{(i)}$都是未知的。 隐变量模型 Latent Variable Model GMM 混合高斯模型，有限个高斯模型混合$z$~Categorical Dist VAE 无限个（infinite）高斯模型混合: $z \\sim N(0,\\bf I) $，$x|z \\sim N(\\mu_{\\theta}(z),\\sum_{\\theta}(z))$，得到如下建模， $$ p_{\\theta}(x) = \\int_zp_{\\theta}(x,z)dz \\ = \\ \\int_z p(z)\\cdot p_{\\theta}(x|z)dz $$ 其中$p_{\\theta}(x)$为intractable。 如果需要使用VAE生成一个样本，先从$p(z)$中采样一个$z^i$，然后使用$p_{\\theta}(x|z)$（实际采用一个神经网络来逼近，即Decoder）来得到$x^i $。 由变分推断可知 $$ \\log p(x) = ELBO + KL(q_{\\phi}(z|x)||p_{\\theta}(z|x)) $$ 优化目标如下 $$ \\begin{equation} \\begin{aligned} \u003c\\hat \\theta,\\hat \\phi\u003e \u0026= \\mathop{\\arg\\min}_{\u003c\\theta, \\phi\u003e}\\ KL(q_{\\phi}(z|x)||p_{\\theta}(z|x)) \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ ELBO \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x,z)]+H[q_{\\phi}] \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x,z)-\\log q_{\\phi}(z|x)] \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)+\\log p(z)-\\log q_{\\phi}(z|x)] \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]+\\int q_{\\phi}(z|x)\\frac{\\log p(z)}{\\log q_{\\phi}(z|x)}dz \\\\ \u0026=\\mathop{\\arg\\max}_{\u003c\\theta, \\phi\u003e}\\underbrace{\\ E_{q_{\\phi}(z|x)}[\\log p_{\\theta}(x|z)]}_{真正的目标函数}-\\underbrace{KL(q_{\\phi}(z|x)||p(z))}_{看做正则化项，使编码器不坍缩} \\end{aligned} \\end{equation} $$ 使用SGVI进行训练，重参数化技巧可以如下实现 $\\epsilon$可以看做采样的噪声，$\\epsilon \\backsim N(0,\\bf I)$。假设$z|x \\backsim N(\\mu_{\\phi}(x),\\Sigma_{\\phi}(x))$则 $$ z=\\mu_{\\phi}(x)+\\Sigma_{\\phi}^{\\frac{1}{2}}(x)\\cdot \\epsilon $$ ","date":"2022-09-15","objectID":"/variational_inference/:6:0","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#问题场景"},{"categories":["machine_learning"],"content":" 参考资料https://www.bilibili.com/video/BV1DW41167vr?p=1\u0026vd_source=309d79182a0075ce59fbfe1a028281fd https://www.bilibili.com/video/BV1G34y1t7Dy/?p=2\u0026spm_id_from=pageDriver\u0026vd_source=309d79182a0075ce59fbfe1a028281fd https://zhuanlan.zhihu.com/p/345597656 ","date":"2022-09-15","objectID":"/variational_inference/:7:0","series":[],"tags":[],"title":"变分推断","uri":"/variational_inference/#参考资料"},{"categories":["machine_learning"],"content":" BERTBERT（Bidirectional Encoder Representations from Transformers）由谷歌在2018年提出[1]。在语言模型中，它的优势是采用了动态的词向量，用以解决一词多义等问题，并且在训练阶段使用多任务——MLM（Masked Language Modeling）和NSP（Next Sentence Prediction）来融入语言学知识，前者使用了双向语义信息来编码词汇，而后者有利于下游句级关系的推理；它的缺点是掩码机制的被掩词的条件独立假设，掩码使用的特殊标记与下游微调时的不一致性以及切词时可能会破坏原词等[2]。 [1] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018. [2] Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[J]. Advances in neural information processing systems, 2019, 32. ","date":"2022-09-05","objectID":"/pretrain_model/:0:0","series":[],"tags":[],"title":"预训练模型的对比","uri":"/pretrain_model/#bert"},{"categories":["machine_learning"],"content":" GPTGPT（Generative Pre-Training）由OpenAI在2018年提出[3]。GPT模型同样采用了预训练加微调的训练策略，能够实现动态词向量，且由于其是自回归模型，相比于BERT，GPT模型可以更好地应用于自然语言生成任务。然而，GPT模型仅利用了上文信息，相比于BERT，其只能捕捉单向的语义信息。在GPT的版本更新迭代中，其参数规模迅速增长，这也导致了其难以反向传播来更新权重。 [3] Radford A, Narasimhan K, Salimans T, et al. Improving laAnguage understanding by generative pre-training[J]. 2018. ","date":"2022-09-05","objectID":"/pretrain_model/:0:0","series":[],"tags":[],"title":"预训练模型的对比","uri":"/pretrain_model/#gpt"},{"categories":[],"content":" 文本哈希 数据集 20Newsgroups ","date":"2022-08-09","objectID":"/datasets/:1:0","series":[],"tags":[],"title":"有关数据集调研和介绍","uri":"/datasets/#文本哈希"},{"categories":[],"content":" 基于文档的多轮问答 任务 数据集 描述 语言 对话式问答(Conversational Question Answering, CQA) CoQA，CuQA等 基于给定文档的多轮问答，无需检索，短文。 英文 CQA OR-QuAC, MultiDoc2Dial等 基于多文档的多轮问答，需要检索，长文和多短文。 英文 CQA INSCIT 面向信息检索的多轮多文档问答 英文 CQA 【CCL 2020】多轮对话问答数据采集平台 未公开，已发邮件，长文，3000字以内，CCL2020 中文 机器阅读理解（Machine Reading Comprehension, MRC） ChineseSquad 单轮机器阅读理解。 有一类做法是将数据集进行翻译为中文，本数据集便是从SQuAD翻译过来的中文数据集。 MRC CAIL大赛阅读理解数据集,DuReader 前者是最高人民法院司改办指导举办的中国法律智能技术评测大赛（每年都有，2021年有阅读理解任务），后者是百度在2018年机器阅读理解大赛上构造的数据集。 中文 ","date":"2022-08-09","objectID":"/datasets/:2:0","series":[],"tags":[],"title":"有关数据集调研和介绍","uri":"/datasets/#基于文档的多轮问答"},{"categories":["tutorial","technology_log"],"content":" 前言本项目为个人实践项目，按实验室老师要求将项目部署上云服务器，主要实现基本的网站结构和api交互，使用nginx进行反向代理，提供web服务，后端使用spring架构。 开发环境如下， 云服务器 Ubuntu 本地 vscode，MacOS，idea ","date":"2022-07-23","objectID":"/full_stack_development/:1:0","series":[],"tags":["linux"],"title":"记一次前后端分离的开发部署实践","uri":"/full_stack_development/#前言"},{"categories":["tutorial","technology_log"],"content":" Nginx配置版本 Nginx Open Source 官方文档 https://docs.nginx.com/nginx/admin-guide/installing-nginx/installing-nginx-open-source/ 一键安装 sudo apt install -y nginx 手动安装方法（https://blog.csdn.net/lsygood/article/details/）（不推荐） 大致说下做法 先安装依赖 apt install -y gcc pcre pcre-devel zlib zlib-devel openssl openssl-devel 到这下一个nginx压缩包 http://nginx.org/ 解压之后进入该文件夹 然后 # 执行命令 ./configure # 执行make命令 make # 执行make install命令 make install #进入nginx启动目录启动 ./nginx 查看版本 sudo nginx -v out: nginx version: nginx/1.20.1 修改/etc/nginx 下的nginx.conf文件，手动安装的在conf文件夹下面 server { listen 9000; # 改为你想要的端口号 listen [::]:9000; # 改为你想要的端口号 server_name _; root /usr/share/nginx/html; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } 重启nginx sudo nginx -s reload 打开浏览器访问http://浏览器ip:9000（你设置的端口号） 不是无法访问就行，这个页面对应配置文件里root变量对应文件夹下的哪个index.html里的内容。 其他命令（调试使用） # 干掉所有的nginx进程 killall -9 nginx # 查看所有监听端口的进程 netstat -nltp # 查看所有进程 ps -aux ","date":"2022-07-23","objectID":"/full_stack_development/:2:0","series":[],"tags":["linux"],"title":"记一次前后端分离的开发部署实践","uri":"/full_stack_development/#nginx配置"},{"categories":["tutorial","technology_log"],"content":" 创建Vite项目注：如果本地已有vite项目，请在vite项目环境下直接创建dist即可。 具体操作如下 # 第一步 ## 删除node_modules文件夹和package-lock.json文件 # 第二步 ## 安装依赖(需要安装nodejs) npm install --unsafe-perm --registry=https://registry.npm.taobao.org # 第三步 ## 启动项目(需要知道可执行的东西有哪些，在package.json里有声明serve) npm run serve 使用vue编写前端页面 创建vite项目 npm init vite 一路回车并按如下选择 创建成功 进入vite项目中安装依赖 cd my_vite_project/ npm install 安装完成…… 启动vite项目 npm run dev 点击链接 出现如上页面，证明访问成功 创建dist npm run build 创建成功之后在my_vite_project 目录下会产生一个dist目录, 后续我们将nginx的root路径指向这个目录即可。 修改/etc/nginx 下的nginx.conf文件，手动安装的在conf文件夹下面 …… # 更改nginx的user为有权限访问vite项目的用户 user lighthouse; # 改为你的用户名 …… # server部分 server { listen 9000; listen [::]:9000; server_name _; root /home/lighthouse/vue_project/my_vite_project/dist; # 修改这个路径为你项目地址的dist目录 try_files $uri $uri/ /index.html # 加上这句便可以在网址后面接目录 # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } …… 重启nginx sudo nginx -s reload ","date":"2022-07-23","objectID":"/full_stack_development/:3:0","series":[],"tags":["linux"],"title":"记一次前后端分离的开发部署实践","uri":"/full_stack_development/#创建vite项目"},{"categories":["tutorial","technology_log"],"content":" Mysql配置如果没有mysql，到官网下载https://www.mysql.com/，然后按说明安装即可 ","date":"2022-07-23","objectID":"/full_stack_development/:4:0","series":[],"tags":["linux"],"title":"记一次前后端分离的开发部署实践","uri":"/full_stack_development/#mysql配置"},{"categories":["tutorial","technology_log"],"content":" 有mysql，密码未知如果有mysql（云服务器一般自带mysql），不知道密码的话，按照以下步骤 到mysql的安装目录下，找到my.cnf文件，一般是/etc/mysql，可以用whereis mysql命令查找，添加如下配置 [mysqld] skip-grant-tables 重启mysql服务 sudo service mysql restart 进入mysql，输入以下指令回车即可，以root登录 mysql -uroot -p 修改root密码为 kkk_123 此处有坑，mysql 8.0版本以下 mysql\u003e USE mysql; mysql\u003e UPDATE user SET authentication_string=password('Kkk_123') WHERE User='root'; mysql\u003e flush privileges; mysql\u003e quit 重启mysql服务 sudo service mysql restart 8.0及以上此处有坑，host搞清楚，是’localhost’还是’%’，查user表 mysql\u003e select User,Host from user; +------------------+-----------+ | User | Host | +------------------+-----------+ | root | % | | debian-sys-maint | localhost | | mysql.infoschema | localhost | | mysql.session | localhost | | mysql.sys | localhost | +------------------+-----------+ 5 rows in set (0.00 sec) 首先置空密码 mysql\u003e USE mysql; mysql\u003e UPDATE user SET authentication_string='' where USE='root'; 然后到mysql的安装目录下，找到my.cnf文件，一般是/etc/mysql，可以用whereis mysql命令查找，将刚刚指定的跳过验证注释掉 #[mysqld] #skip-grant-tables 重启mysql服务 sudo service mysql restart 由于密码时空，所以也可以直接进入mysql sudo mysql -uroot 重置密码为Kkk_123 mysql\u003e alter user 'root'@'%' identified by 'Kkk_123'; 重启mysql服务 sudo service mysql restart 用刚刚的密码登录mysql sudo mysql -uroot -pKkk_123 成功。 ","date":"2022-07-23","objectID":"/full_stack_development/:4:1","series":[],"tags":["linux"],"title":"记一次前后端分离的开发部署实践","uri":"/full_stack_development/#有mysql密码未知"},{"categories":["tutorial","technology_log"],"content":" 有mysql，密码未知如果有mysql（云服务器一般自带mysql），不知道密码的话，按照以下步骤 到mysql的安装目录下，找到my.cnf文件，一般是/etc/mysql，可以用whereis mysql命令查找，添加如下配置 [mysqld] skip-grant-tables 重启mysql服务 sudo service mysql restart 进入mysql，输入以下指令回车即可，以root登录 mysql -uroot -p 修改root密码为 kkk_123 此处有坑，mysql 8.0版本以下 mysql\u003e USE mysql; mysql\u003e UPDATE user SET authentication_string=password('Kkk_123') WHERE User='root'; mysql\u003e flush privileges; mysql\u003e quit 重启mysql服务 sudo service mysql restart 8.0及以上此处有坑，host搞清楚，是’localhost’还是’%’，查user表 mysql\u003e select User,Host from user; +------------------+-----------+ | User | Host | +------------------+-----------+ | root | % | | debian-sys-maint | localhost | | mysql.infoschema | localhost | | mysql.session | localhost | | mysql.sys | localhost | +------------------+-----------+ 5 rows in set (0.00 sec) 首先置空密码 mysql\u003e USE mysql; mysql\u003e UPDATE user SET authentication_string='' where USE='root'; 然后到mysql的安装目录下，找到my.cnf文件，一般是/etc/mysql，可以用whereis mysql命令查找，将刚刚指定的跳过验证注释掉 #[mysqld] #skip-grant-tables 重启mysql服务 sudo service mysql restart 由于密码时空，所以也可以直接进入mysql sudo mysql -uroot 重置密码为Kkk_123 mysql\u003e alter user 'root'@'%' identified by 'Kkk_123'; 重启mysql服务 sudo service mysql restart 用刚刚的密码登录mysql sudo mysql -uroot -pKkk_123 成功。 ","date":"2022-07-23","objectID":"/full_stack_development/:4:1","series":[],"tags":["linux"],"title":"记一次前后端分离的开发部署实践","uri":"/full_stack_development/#此处有坑mysql-80版本以下"},{"categories":["tutorial","technology_log"],"content":" 有mysql，密码未知如果有mysql（云服务器一般自带mysql），不知道密码的话，按照以下步骤 到mysql的安装目录下，找到my.cnf文件，一般是/etc/mysql，可以用whereis mysql命令查找，添加如下配置 [mysqld] skip-grant-tables 重启mysql服务 sudo service mysql restart 进入mysql，输入以下指令回车即可，以root登录 mysql -uroot -p 修改root密码为 kkk_123 此处有坑，mysql 8.0版本以下 mysql\u003e USE mysql; mysql\u003e UPDATE user SET authentication_string=password('Kkk_123') WHERE User='root'; mysql\u003e flush privileges; mysql\u003e quit 重启mysql服务 sudo service mysql restart 8.0及以上此处有坑，host搞清楚，是’localhost’还是’%’，查user表 mysql\u003e select User,Host from user; +------------------+-----------+ | User | Host | +------------------+-----------+ | root | % | | debian-sys-maint | localhost | | mysql.infoschema | localhost | | mysql.session | localhost | | mysql.sys | localhost | +------------------+-----------+ 5 rows in set (0.00 sec) 首先置空密码 mysql\u003e USE mysql; mysql\u003e UPDATE user SET authentication_string='' where USE='root'; 然后到mysql的安装目录下，找到my.cnf文件，一般是/etc/mysql，可以用whereis mysql命令查找，将刚刚指定的跳过验证注释掉 #[mysqld] #skip-grant-tables 重启mysql服务 sudo service mysql restart 由于密码时空，所以也可以直接进入mysql sudo mysql -uroot 重置密码为Kkk_123 mysql\u003e alter user 'root'@'%' identified by 'Kkk_123'; 重启mysql服务 sudo service mysql restart 用刚刚的密码登录mysql sudo mysql -uroot -pKkk_123 成功。 ","date":"2022-07-23","objectID":"/full_stack_development/:4:1","series":[],"tags":["linux"],"title":"记一次前后端分离的开发部署实践","uri":"/full_stack_development/#80及以上"},{"categories":["tutorial","technology_log"],"content":" 后端配置使用idea将写好的项目打一个jar包 在项目文件的target文件夹中找到生成的jar包，并运行。 配置完成。 ","date":"2022-07-23","objectID":"/full_stack_development/:5:0","series":[],"tags":["linux"],"title":"记一次前后端分离的开发部署实践","uri":"/full_stack_development/#后端配置"},{"categories":["machine_learning"],"content":" 权重冻结在使用预训练语言模型进行下游任务的微调时，有时数据量难以使得其收敛，此时我们可以选择固定住某些层的参数，使其仍然保持在预训练语料上的知识积累（通常是编码器），而仅对如分类层等进行微调。 做法如下 # 方法1： 设置requires_grad = False for param in model.parameters(): param.requires_grad = False # 方法2： torch.no_grad() class net(nn.Module): def __init__(): ...... # 方法1可以在此设置进行初始化。 def forward(self.x): with torch.no_grad(): # no_grad下参数不会迭代 x = self.layer(x) ...... x = self.fc(x) return x ","date":"2022-07-09","objectID":"/finetune_trick/:1:0","series":[],"tags":["py"],"title":"预训练模型的finetune技巧","uri":"/finetune_trick/#权重冻结"},{"categories":["machine_learning"],"content":" 参考链接https://zhuanlan.zhihu.com/p/524036087 ","date":"2022-07-09","objectID":"/finetune_trick/:2:0","series":[],"tags":["py"],"title":"预训练模型的finetune技巧","uri":"/finetune_trick/#参考链接"},{"categories":["machine_learning"],"content":" Tensor是什么？Tensor又叫张量，与标量，向量等的区别如下： 标量其实就是一个独立存在的数，比如在线性代数中一个实数 5 就可以被看作一个标量，所以标量的运算相对简单，与我们平常做的数字算术运算类似。 向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。我们可以将向量看做空间中的点，每个元素就是空间中的坐标。 矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称。如果我们现在有N个用户的数据，每条数据含有M个特征，那其实它对应的就是一个N*M的矩阵 几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。 例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。 ","date":"2022-07-08","objectID":"/tensor/:0:0","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#tensor是什么"},{"categories":["machine_learning"],"content":" Torch","date":"2022-07-08","objectID":"/tensor/:1:0","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#torch"},{"categories":["machine_learning"],"content":" torch.argmax返回最大值索引 返回所有值的最大值位置索引 \u003e\u003e\u003e a = torch.randn(4, 4) \u003e\u003e\u003e a tensor([[ 1.3398, 0.2663, -0.2686, 0.2450], [-0.7401, -0.8805, -0.3402, -1.1936], [ 0.4907, -1.3948, -1.0691, -0.3132], [-1.6092, 0.5419, -0.2993, 0.3195]]) \u003e\u003e\u003e torch.argmax(a) tensor(0) 返回指定维度最大值的位置索引 \u003e\u003e\u003e a = torch.randn(4, 4) \u003e\u003e\u003e a tensor([[ 1.3398, 0.2663, -0.2686, 0.2450], [-0.7401, -0.8805, -0.3402, -1.1936], [ 0.4907, -1.3948, -1.0691, -0.3132], [-1.6092, 0.5419, -0.2993, 0.3195]]) \u003e\u003e\u003e torch.argmax(a, dim=1) tensor([ 0, 2, 0, 1]) ","date":"2022-07-08","objectID":"/tensor/:1:1","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#torchargmax"},{"categories":["machine_learning"],"content":" torch.topk返回特定维的top-k最大值的索引 \u003e\u003e\u003e x = torch.arange(1., 6.) \u003e\u003e\u003e x tensor([ 1., 2., 3., 4., 5.]) \u003e\u003e\u003e torch.topk(x, 3) torch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2])) top-k索引取得方式举例 torch.topk(x, dim=1).indices.to('cpu').numpy() ","date":"2022-07-08","objectID":"/tensor/:1:2","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#torchtopk"},{"categories":["machine_learning"],"content":" torch.cat拼接tensor \u003e\u003e\u003e x = torch.randn(2, 3) \u003e\u003e\u003e x tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]]) \u003e\u003e\u003e torch.cat((x, x, x), 0) tensor([[ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497], [ 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497]]) \u003e\u003e\u003e torch.cat((x, x, x), 1) tensor([[ 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614, 0.6580, -1.0969, -0.4614], [-0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497, -0.1034, -0.5790, 0.1497]]) ","date":"2022-07-08","objectID":"/tensor/:1:3","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#torchcat"},{"categories":["machine_learning"],"content":" torch.stack将一个tensor序列组合Concatenates为一个新的tensor，所有的tensor需要是相同的size（shape） \u003e\u003e\u003e tensor_lis = [] \u003e\u003e\u003e for i in range(4): ... x = torch.randn(3,) ... tensor_lis.append(x) ... \u003e\u003e\u003e tensor_lis [tensor([ 0.4239, -0.4117, -1.4423]), tensor([ 0.0388, 0.2642, -0.0006]), tensor([-0.4351, 0.4171, -1.1180]), tensor([ 0.2685, 0.6888, -1.6707])] \u003e\u003e\u003e a = torch.stack(tensor_lis,dim=0) \u003e\u003e\u003e a tensor([[ 4.2391e-01, -4.1169e-01, -1.4423e+00], [ 3.8823e-02, 2.6416e-01, -6.1708e-04], [-4.3513e-01, 4.1708e-01, -1.1180e+00], [ 2.6853e-01, 6.8878e-01, -1.6707e+00]]) ","date":"2022-07-08","objectID":"/tensor/:1:4","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#torchstack"},{"categories":["machine_learning"],"content":" torch.squeezesqueeze是英文 挤 的意思 返回一个张量，其中所有input大小为1的维度都已删除。 例如，如果输入是形状： $(A \\times 1 \\times B \\times C \\times 1 \\times D)$那么输出张量将具有以下形状：（$A \\times B \\times C\\times D$）. dim给定时，仅在给定维度上进行挤压操作。如果输入是形状：$(A \\times 1 \\times B)$，squeeze(input, 0) 语句将保持张量不变，但squeeze(input, 1)语句会将张量压缩到形状$(A \\times B)$. \u003e\u003e\u003e x = torch.zeros(2, 1, 2, 1, 2) \u003e\u003e\u003e x.size() torch.Size([2, 1, 2, 1, 2]) \u003e\u003e\u003e y = torch.squeeze(x) \u003e\u003e\u003e y.size() torch.Size([2, 2, 2]) \u003e\u003e\u003e y = torch.squeeze(x, 0) \u003e\u003e\u003e y.size() torch.Size([2, 1, 2, 1, 2]) \u003e\u003e\u003e y = torch.squeeze(x, 1) \u003e\u003e\u003e y.size() torch.Size([2, 2, 1, 2]) ","date":"2022-07-08","objectID":"/tensor/:1:5","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#torchsqueeze"},{"categories":["machine_learning"],"content":" torch.tensor.view改变tensor的形状，注意，调用的时候是作用在tensor上的，比如tensor_x.view()，而不是torch.view()，有所区别。 \u003e\u003e\u003e x = torch.randn(4, 4) \u003e\u003e\u003e x.size() torch.Size([4, 4]) \u003e\u003e\u003e y = x.view(16) \u003e\u003e\u003e y.size() torch.Size([16]) \u003e\u003e\u003e z = x.view(-1, 8) # the size -1 is inferred from other dimensions \u003e\u003e\u003e z.size() torch.Size([2, 8]) \u003e\u003e\u003e a = torch.randn(1, 2, 3, 4) \u003e\u003e\u003e a.size() torch.Size([1, 2, 3, 4]) \u003e\u003e\u003e b = a.transpose(1, 2) # Swaps 2nd and 3rd dimension \u003e\u003e\u003e b.size() torch.Size([1, 3, 2, 4]) \u003e\u003e\u003e c = a.view(1, 3, 2, 4) # Does not change tensor layout in memory \u003e\u003e\u003e c.size() torch.Size([1, 3, 2, 4]) \u003e\u003e\u003e torch.equal(b, c) False 改变数据类型，通常用于控制浮点数 \u003e\u003e\u003e x = torch.randn(4, 4) \u003e\u003e\u003e x tensor([[ 0.9482, -0.0310, 1.4999, -0.5316], [-0.1520, 0.7472, 0.5617, -0.8649], [-2.4724, -0.0334, -0.2976, -0.8499], [-0.2109, 1.9913, -0.9607, -0.6123]]) \u003e\u003e\u003e x.dtype torch.float32 \u003e\u003e\u003e y = x.view(torch.int32) \u003e\u003e\u003e y tensor([[ 1064483442, -1124191867, 1069546515, -1089989247], [-1105482831, 1061112040, 1057999968, -1084397505], [-1071760287, -1123489973, -1097310419, -1084649136], [-1101533110, 1073668768, -1082790149, -1088634448]], dtype=torch.int32) \u003e\u003e\u003e y[0, 0] = 1000000000 \u003e\u003e\u003e x tensor([[ 0.0047, -0.0310, 1.4999, -0.5316], [-0.1520, 0.7472, 0.5617, -0.8649], [-2.4724, -0.0334, -0.2976, -0.8499], [-0.2109, 1.9913, -0.9607, -0.6123]]) \u003e\u003e\u003e x.view(torch.cfloat) tensor([[ 0.0047-0.0310j, 1.4999-0.5316j], [-0.1520+0.7472j, 0.5617-0.8649j], [-2.4724-0.0334j, -0.2976-0.8499j], [-0.2109+1.9913j, -0.9607-0.6123j]]) \u003e\u003e\u003e x.view(torch.cfloat).size() torch.Size([4, 2]) \u003e\u003e\u003e x.view(torch.uint8) tensor([[ 0, 202, 154, 59, 182, 243, 253, 188, 185, 252, 191, 63, 240, 22, 8, 191], [227, 165, 27, 190, 128, 72, 63, 63, 146, 203, 15, 63, 22, 106, 93, 191], [205, 59, 30, 192, 112, 206, 8, 189, 7, 95, 152, 190, 12, 147, 89, 191], [ 43, 246, 87, 190, 235, 226, 254, 63, 111, 240, 117, 191, 177, 191, 28, 191]], dtype=torch.uint8) \u003e\u003e\u003e x.view(torch.uint8).size() torch.Size([4, 16]) ","date":"2022-07-08","objectID":"/tensor/:1:6","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#torchtensorview"},{"categories":["machine_learning"],"content":" Tensor归一化操作 按维度1归一化 \u003e\u003e\u003e import torch \u003e\u003e\u003e import torch.nn.functional as F \u003e\u003e\u003e x = torch.rand(2,4) \u003e\u003e\u003e x tensor([[0.1780, 0.0885, 0.0239, 0.1663], [0.5253, 0.1764, 0.6815, 0.1032]]) \u003e\u003e\u003e norm_x = F.softmax(x, dim=1) \u003e\u003e\u003e norm_x tensor([[0.2660, 0.2432, 0.2280, 0.2629], [0.2833, 0.1998, 0.3312, 0.1857]]) ","date":"2022-07-08","objectID":"/tensor/:1:7","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#tensor归一化操作"},{"categories":["machine_learning"],"content":" Tensor的多条件选取 \u003e\u003e\u003e a = [1., 2., 3, 4] \u003e\u003e\u003e b = [-1., -2., -3., -4.] \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float) \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float) # 单条件选取 \u003e\u003e\u003e print(a_tensor[a_tensor \u003e 1.]) tensor([2., 3., 4.]) # 多条件选取 \u003e\u003e\u003e condition1 = torch.where((a_tensor \u003e 1.) \u0026 (a_tensor \u003c 4.)) \u003e\u003e\u003e condition2 = torch.where((a_tensor == 1.) | (a_tensor == 4.)) \u003e\u003e\u003e print(a_tensor[condition1]) tensor([2., 3.]) # 反条件选取 \u003e\u003e\u003e condition = torch.where((a_tensor \u003e 1.) \u0026 (a_tensor \u003c 4.), True, False) # 首先转为bool类型 \u003e\u003e\u003e print(a_tensor[~condition]) # 然后取反 tensor([1., 4.]) \u003e\u003e\u003e print(a_tensor[condition2]) tensor([1., 4.]) # 条件选另一个tensor里的值(a,b等维) \u003e\u003e\u003e print(b_tensor[condition1]) tensor([-2., -3.]) # mask 机制，列表选值 \u003e\u003e\u003e x = torch.tensor([[1,2,0],[2,3,0]]) \u003e\u003e\u003e y = torch.tensor([[2,3,0],[2,3,0]]) \u003e\u003e\u003e select = torch.tensor([1,2]) \u003e\u003e\u003e mask = torch.isin(x, select) \u003e\u003e\u003e mask tensor([[ True, True, False], [ True, False, False]]) \u003e\u003e\u003e y[mask] tensor([2, 3, 2]) ","date":"2022-07-08","objectID":"/tensor/:1:8","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#tensor的多条件选取"},{"categories":["machine_learning"],"content":" 不同形状的Tensor之间计算 \u003e\u003e\u003e a = [[1., 2., 3, 4], [1., 2., 3, 4]] \u003e\u003e\u003e b = [[[-1., -2., -3., -4.], [-1., -2., -3., -4.], [-1., -2., -3., -4.]], [[-1., -2., -3., -4.], [-1., -2., -3., -4.], [-1., -2., -3., -4.]]] \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float) # shape torch.size([2,4]) \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float) # shape torch.size([2,3,4]) \u003e\u003e\u003e a_tensor = a_tensor.unsqueeze(1) # 扩展一个id=1的维度 \u003e\u003e\u003e print(a_tensor) tensor([[[1., 2., 3., 4.]], [[1., 2., 3., 4.]]]) \u003e\u003e\u003e print(a_tensor-b_tensor) tensor([[[2., 4., 6., 8.], [2., 4., 6., 8.], [2., 4., 6., 8.]], [[2., 4., 6., 8.], [2., 4., 6., 8.], [2., 4., 6., 8.]]]) \u003e\u003e\u003e print((a_tensor-b_tensor)**2) tensor([[[ 4., 16., 36., 64.], [ 4., 16., 36., 64.], [ 4., 16., 36., 64.]], [[ 4., 16., 36., 64.], [ 4., 16., 36., 64.], [ 4., 16., 36., 64.]]]) # 二范式，dim2维度的所有值平方，求和，然后开根号 \u003e\u003e\u003e print(torch.norm(a_tensor-b_tensor, p=2, dim=2)) tensor([[10.9545, 10.9545, 10.9545], [10.9545, 10.9545, 10.9545]]) # 二范式，dim2维度的所有值平方，求和，然后开根号，和上面结果一致 \u003e\u003e\u003e print(torch.linalg.norm(a_tensor-b_tensor, dim=2)) tensor([[10.9545, 10.9545, 10.9545], [10.9545, 10.9545, 10.9545]]) ","date":"2022-07-08","objectID":"/tensor/:1:9","series":[],"tags":["py"],"title":"Tensor的简单使用","uri":"/tensor/#不同形状的tensor之间计算"},{"categories":["tutorial"],"content":" 简介pyecharts 的呈现十分美观，尽量搭配jupyternotebook使用，jupyterlab上的显示尚有瑕疵。在使用中，可以将jupyternotebook转为html来静态保存pyecharts的图像，做可视化分析，神秘代码如下 jupyter nbconvert --to html --no-input data_analy_4.ipynb 会生成一个data_analy_4.html，它里面仅仅包含md和输出部分，code部分会被隐藏。 ","date":"2022-07-08","objectID":"/pyecharts/:1:0","series":["python_data_analysis"],"tags":["py"],"title":"Pyecharts的简单使用","uri":"/pyecharts/#简介"},{"categories":["tutorial"],"content":" 使用作为python的第三方库使用即可，pip install pyecharts ","date":"2022-07-08","objectID":"/pyecharts/:2:0","series":["python_data_analysis"],"tags":["py"],"title":"Pyecharts的简单使用","uri":"/pyecharts/#使用"},{"categories":["tutorial"],"content":" 图例(基于pyecharts1.9.1) # ============= # Library # ============= from pyecharts.charts import Bar,Line,Tab, Grid,Scatter from pyecharts import options as opts from pyecharts.globals import ThemeType ","date":"2022-07-08","objectID":"/pyecharts/:3:0","series":["python_data_analysis"],"tags":["py"],"title":"Pyecharts的简单使用","uri":"/pyecharts/#图例基于pyecharts191"},{"categories":["tutorial"],"content":" 直方图 神秘代码 x_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] y_data = [0.804, 0.882, 0.906, 0.934, 0.958, 0.964, 0.972, 0.976, 0.976, 0.976] bar = ( Bar(init_opts=opts.InitOpts(width='400px', height='400px', theme=ThemeType.DARK)) .add_xaxis(x_data,) .add_yaxis(\"top_k准确率\", y_data) .set_global_opts( xaxis_opts=opts.AxisOpts( name='Top_K' ), yaxis_opts=opts.AxisOpts( name='ACC' ), ) ) bar.render_notebook() ","date":"2022-07-08","objectID":"/pyecharts/:3:1","series":["python_data_analysis"],"tags":["py"],"title":"Pyecharts的简单使用","uri":"/pyecharts/#直方图"},{"categories":["tutorial"],"content":" 折线图 神秘代码 x_data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # 需要注意pyecharts的折线图中x轴的数据需要转为str类型，否则图像会出现偏移 x_data = [str(x) for x in x_data] y_data = [0.804, 0.882, 0.906, 0.934, 0.958, 0.964, 0.972, 0.976, 0.976, 0.976] line = ( Line(init_opts=opts.InitOpts(width='400px', height='400px', theme=ThemeType.DARK)) .add_xaxis(x_data,) .add_yaxis(\"top_k准确率\", y_data, is_smooth=False, is_symbol_show=True) .set_global_opts( xaxis_opts=opts.AxisOpts( name='Top_K' ), yaxis_opts=opts.AxisOpts( name='ACC' ), ) ) line.render_notebook() ","date":"2022-07-08","objectID":"/pyecharts/:3:2","series":["python_data_analysis"],"tags":["py"],"title":"Pyecharts的简单使用","uri":"/pyecharts/#折线图"},{"categories":["tutorial"],"content":" 散点图 神秘代码 x_lis = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] y_lis = [2, 2, 3, 4, 5, 3, 7, 8, 9, 5] # 设置散点图 s=Scatter(init_opts=opts.InitOpts( width=\"300px\", height=\"300px\" ) ) s.add_xaxis(x_lis) s.add_yaxis(\"标签\",y_lis, label_opts=opts.LabelOpts(is_show=True)) # max_interval 指定坐标轴间隔 s.set_global_opts( title_opts=opts.TitleOpts(title=\"散点图\"), xaxis_opts=opts.AxisOpts( type_='value', splitline_opts=opts.SplitLineOpts(is_show=True), max_interval=1 ), yaxis_opts=opts.AxisOpts( splitline_opts=opts.SplitLineOpts(is_show=True), axistick_opts=opts.AxisTickOpts(is_show=True,length=10), max_interval=1 ), ) # 设置参考线 line1 = ( Line() .add_xaxis([0,10]) .add_yaxis( \"\", list([0,10]), label_opts=opts.LabelOpts(is_show=False), ) .set_global_opts( xaxis_opts=opts.AxisOpts( type_='value', ) ) ) # 设置overlap overlap_1 = s.overlap(line1) grid = ( Grid(init_opts=opts.InitOpts(width=\"300px\", height=\"300px\")) .add( overlap_1, grid_opts=opts.GridOpts(), is_control_axis_index=True ) .render_notebook() ) grid 更多图例 https://pyecharts.org/#/ ","date":"2022-07-08","objectID":"/pyecharts/:3:3","series":["python_data_analysis"],"tags":["py"],"title":"Pyecharts的简单使用","uri":"/pyecharts/#散点图"},{"categories":["machine_learning","tutorial"],"content":"改变枯燥单一的进度条","date":"2022-07-03","objectID":"/bravo_processbar/","series":[],"tags":["py"],"title":"炫酷的processbar","uri":"/bravo_processbar/"},{"categories":["machine_learning","tutorial"],"content":" 远古的我(远古时期没有截图)：[00:00:00 \u003c 9999:9999:9999] ","date":"2022-07-03","objectID":"/bravo_processbar/:0:1","series":[],"tags":["py"],"title":"炫酷的processbar","uri":"/bravo_processbar/#远古的我远古时期没有截图"},{"categories":["machine_learning","tutorial"],"content":" 之前的我 ","date":"2022-07-03","objectID":"/bravo_processbar/:0:2","series":[],"tags":["py"],"title":"炫酷的processbar","uri":"/bravo_processbar/#之前的我"},{"categories":["machine_learning","tutorial"],"content":" 现在的我 Processbar实现 import time import sys import emoji class ProgressBar(object): ''' super progress bar Example: \u003e\u003e\u003e pbar = ProgressBar(n_total=len(train_dataloader), desc=f'EPOCH {_e}/{num_epochs}') \u003e\u003e\u003e pbar(step=step,info={'loss':20}) ''' def __init__(self, n_total, width=6, desc='Training'): self.width = width self.n_total = n_total self.desc = desc self.start_time = time.time() def reset(self): \"\"\"Method to reset internal variables.\"\"\" self.start_time = time.time() def _time_info(self, now, current): time_per_unit = (now - self.start_time) / current if current \u003c self.n_total: eta = time_per_unit * (self.n_total - current) if eta \u003e 3600: eta_format = ('%d:%02d:%02d' % (eta // 3600, (eta % 3600) // 60, eta % 60)) elif eta \u003e 60: eta_format = '%d:%02d' % (eta // 60, eta % 60) else: eta_format = '%ds' % eta time_info = f' - ETA: {eta_format}' else: if time_per_unit \u003e= 1: time_info = f' {time_per_unit:.1f}s/step' elif time_per_unit \u003e= 1e-3: time_info = f' {time_per_unit * 1e3:.1f}ms/step' else: time_info = f' {time_per_unit * 1e6:.1f}us/step' return time_info def _bar(self, current): # ⚡ ❓ # you can define your own symbol here ^ ^ # finished symbol f_symbol = emoji.emojize('🚀') # running symbol r_symbol = emoji.emojize('⚡') recv_per = current / self.n_total bar = f'[{self.desc}] {current}/{self.n_total} [' if recv_per \u003e= 1: recv_per = 1 prog_width = int(self.width * recv_per) if prog_width \u003e 0: bar += f_symbol * (prog_width - 1) if current \u003c self.n_total: bar += r_symbol else: bar += f_symbol bar += '.' * (self.width - prog_width) bar += ']' return bar def __call__(self, step, info={}): now = time.time() current = step + 1 bar = self._bar(current) show_bar = f\"\\r{bar}\" + self._time_info(now, current) if len(info) != 0: show_bar = f'{show_bar} ' + \" [\" + \"-\".join( [f' {key}={value:.4f} ' for key, value in info.items()]) + \"]\" if current \u003e= self.n_total: show_bar += '\\n' sys.stdout.write(show_bar) sys.stdout.flush() ","date":"2022-07-03","objectID":"/bravo_processbar/:0:3","series":[],"tags":["py"],"title":"炫酷的processbar","uri":"/bravo_processbar/#现在的我"},{"categories":["machine_learning","tutorial"],"content":" 现在的我 Processbar实现 import time import sys import emoji class ProgressBar(object): ''' super progress bar Example: \u003e\u003e\u003e pbar = ProgressBar(n_total=len(train_dataloader), desc=f'EPOCH {_e}/{num_epochs}') \u003e\u003e\u003e pbar(step=step,info={'loss':20}) ''' def __init__(self, n_total, width=6, desc='Training'): self.width = width self.n_total = n_total self.desc = desc self.start_time = time.time() def reset(self): \"\"\"Method to reset internal variables.\"\"\" self.start_time = time.time() def _time_info(self, now, current): time_per_unit = (now - self.start_time) / current if current \u003c self.n_total: eta = time_per_unit * (self.n_total - current) if eta \u003e 3600: eta_format = ('%d:%02d:%02d' % (eta // 3600, (eta % 3600) // 60, eta % 60)) elif eta \u003e 60: eta_format = '%d:%02d' % (eta // 60, eta % 60) else: eta_format = '%ds' % eta time_info = f' - ETA: {eta_format}' else: if time_per_unit \u003e= 1: time_info = f' {time_per_unit:.1f}s/step' elif time_per_unit \u003e= 1e-3: time_info = f' {time_per_unit * 1e3:.1f}ms/step' else: time_info = f' {time_per_unit * 1e6:.1f}us/step' return time_info def _bar(self, current): # ⚡ ❓ # you can define your own symbol here ^ ^ # finished symbol f_symbol = emoji.emojize('🚀') # running symbol r_symbol = emoji.emojize('⚡') recv_per = current / self.n_total bar = f'[{self.desc}] {current}/{self.n_total} [' if recv_per \u003e= 1: recv_per = 1 prog_width = int(self.width * recv_per) if prog_width \u003e 0: bar += f_symbol * (prog_width - 1) if current \u003c self.n_total: bar += r_symbol else: bar += f_symbol bar += '.' * (self.width - prog_width) bar += ']' return bar def __call__(self, step, info={}): now = time.time() current = step + 1 bar = self._bar(current) show_bar = f\"\\r{bar}\" + self._time_info(now, current) if len(info) != 0: show_bar = f'{show_bar} ' + \" [\" + \"-\".join( [f' {key}={value:.4f} ' for key, value in info.items()]) + \"]\" if current \u003e= self.n_total: show_bar += '\\n' sys.stdout.write(show_bar) sys.stdout.flush() ","date":"2022-07-03","objectID":"/bravo_processbar/:0:3","series":[],"tags":["py"],"title":"炫酷的processbar","uri":"/bravo_processbar/#processbar实现"},{"categories":["machine_learning","tutorial"],"content":" 常用emoji分享佬分享传送门 https://www.94rg.com/article/1881 🌹🍀🍎💰📱🌙🍁🍂🍃🌷💎🔪🔫🏀⚽⚡👄👍🔥 emoji表情 😀😁😂😃😄😅😆😉😊😋😎😍😘😗😙😚☺😇😐😑😶😏😣😥😮😯😪😫😴😌😛😜😝😒😓😔😕😲😷😖😞😟😤😢😭😦😧😨😬😰😱😳😵😡😠 emoji人物 👦👧👨👩👴👵👶👱👮👲👳👷👸💂🎅👰👼💆💇🙍🙎🙅🙆💁🙋🙇🙌🙏👤👥🚶🏃👯💃👫👬👭💏💑👪 emoji手势 💪👈👉☝👆👇✌✋👌👍👎✊👊👋👏👐✍ emoji日常 👣👀👂👃👅👄💋👓👔👕👖👗👘👙👚👛👜👝🎒💼👞👟👠👡👢👑👒🎩🎓💄💅💍🌂 emoji手机 ​ 📱📲📶📳📴☎📞📟📠 emoji公共 ♻🏧🚮🚰♿🚹🚺🚻🚼🚾⚠🚸⛔🚫🚳🚭🚯🚱🚷🔞💈 emoji动物 🙈🙉🙊🐵🐒🐶🐕🐩🐺🐱😺😸😹😻😼😽🙀😿😾🐈🐯🐅🐆🐴🐎🐮🐂🐃🐄🐷🐖🐗🐽🐏🐑🐐🐪🐫🐘🐭🐁🐀🐹🐰🐇🐻🐨🐼🐾🐔🐓🐣🐤🐥🐦🐧🐸🐊🐢🐍🐲🐉🐳🐋🐬🐟🐠🐡🐙🐚🐌🐛🐜🐝🐞🦋 emoji植物 💐🌸💮🌹🌺🌻🌼🌷🌱🌲🌳🌴🌵🌾🌿🍀🍁🍂🍃 emoji自然 🌍🌎🌏🌐🌑🌒🌓🌔🌕🌖🌗🌘🌙🌚🌛🌜☀🌝🌞⭐🌟🌠☁⛅☔⚡❄🔥💧🌊 emoji饮食 🍇🍈🍉🍊🍋🍌🍍🍎🍏🍐🍑🍒🍓🍅🍆🌽🍄🌰🍞🍖🍗🍔🍟🍕🍳🍲🍱🍘🍙🍚🍛🍜🍝🍠🍢🍣🍤🍥🍡🍦🍧🍨🍩🍪🎂🍰🍫🍬🍭🍮🍯🍼☕🍵🍶🍷🍸🍹🍺🍻🍴 emoji文体 🎪🎭🎨🎰🚣🛀🎫🏆⚽⚾🏀🏈🏉🎾🎱🎳⛳🎣🎽🎿🏂🏄🏇🏊🚴🚵🎯🎮🎲🎷🎸🎺🎻🎬 emoji恐怖 😈👿👹👺💀☠👻👽👾💣 emoji旅游 🌋🗻🏠🏡🏢🏣🏤🏥🏦🏨🏩🏪🏫🏬🏭🏯🏰💒🗼🗽⛪⛲🌁🌃🌆🌇🌉🌌🎠🎡🎢🚂🚃🚄🚅🚆🚇🚈🚉🚊🚝🚞🚋🚌🚍🚎🚏🚐🚑🚒🚓🚔🚕🚖🚗🚘🚚🚛🚜🚲⛽🚨🚥🚦🚧⚓⛵🚤🚢✈💺🚁🚟🚠🚡🚀🎑🗿🛂🛃🛄🛅 emoji物品 💌💎🔪💈🚪🚽🚿🛁⌛⏳⌚⏰🎈🎉🎊🎎🎏🎐🎀🎁📯📻📱📲☎📞📟📠🔋🔌💻💽💾💿📀🎥📺📷📹📼🔍🔎🔬🔭📡💡🔦🏮📔📕📖📗📘📙📚📓📃📜📄📰📑🔖💰💴💵💶💷💸💳✉📧📨📩📤📥📦📫📪📬📭📮✏✒📝📁📂📅📆📇📈📉📊📋📌📍📎📏📐✂🔒🔓🔏🔐🔑🔨🔫🔧🔩🔗💉💊🚬🔮🚩🎌💦💨 emoji标志 ♠♥♦♣🀄🎴🔇🔈🔉🔊📢📣💤💢💬💭♨🌀🔔🔕✡✝🔯📛🔰🔱⭕✅☑✔✖❌❎➕➖➗➰➿〽✳✴❇‼⁉❓❔❕❗©®™🎦🔅🔆💯🔠🔡🔢🔣🔤🅰🆎🅱🆑🆒🆓ℹ🆔Ⓜ🆕🆖🅾🆗🅿🆘🆙🆚🈁🈂🈷🈶🈯🉐🈹🈚🈲🉑🈸🈴🈳㊗㊙🈺🈵▪▫◻◼◽◾⬛⬜🔶🔷🔸🔹🔺🔻💠🔲🔳⚪⚫🔴🔵 emoji生肖 🐁🐂🐅🐇🐉🐍🐎🐐🐒🐓🐕🐖 emoji星座 ♈♉♊♋♌♍♎♏♐♑♒♓⛎ emoji钟表 🕛🕧🕐🕜🕑🕝🕒🕞🕓🕟🕔🕠🕕🕡🕖🕢🕗🕣🕘🕤🕙🕥🕚🕦⌛⏳⌚⏰⏱⏲🕰 emoji心形 💘❤💓💔💕💖💗💙💚💛💜💝💞💟❣ emoji花草 💐🌸💮🌹🌺🌻🌼🌷🌱🌿🍀 emoji树叶 🌿🍀🍁🍂🍃 emoji月亮 🌑🌒🌓🌔🌕🌖🌗🌘🌙🌚🌛🌜🌝 emoji水果 🍇🍈🍉🍊🍋🍌🍍🍎🍏🍐🍑🍒🍓 emoji钱币 💴💵💶💷💰💸💳 emoji交通 🚂🚃🚄🚅🚆🚇🚈🚉🚊🚝🚞🚋🚌🚍🚎🚏🚐🚑🚒🚓🚔🚕🚖🚗🚘🚚🚛🚜🚲⛽🚨🚥🚦🚧⚓⛵🚣🚤🚢✈💺🚁🚟🚠🚡🚀 emoji建筑 🏠🏡🏢🏣🏤🏥🏦🏨🏩🏪🏫🏬🏭🏯🏰💒🗼🗽⛪🌆🌇🌉 emoji办公 📱📲☎📞📟📠🔋🔌💻💽💾💿📀🎥📺📷📹📼🔍🔎🔬🔭📡📔📕📖📗📘📙📚📓📃📜📄📰📑🔖💳✉📧📨📩📤📥📦📫📪📬📭📮✏✒📝📁📂📅📆📇📈📉📊📋📌📍📎📏📐✂🔒🔓🔏🔐🔑 emoji箭头 ⬆↗➡↘⬇↙⬅↖↕↔↩↪⤴⤵🔃🔄🔙🔚🔛🔜🔝 ","date":"2022-07-03","objectID":"/bravo_processbar/:0:4","series":[],"tags":["py"],"title":"炫酷的processbar","uri":"/bravo_processbar/#常用emoji分享"},{"categories":["machine_learning"],"content":" 什么是K折交叉验证？如果你有五个土豆，你想要教会一个小盆友，这些是土豆，好了，现在你拿出四个土豆让他逐一认识，最后拿出第五个土豆来测试他是否真正地认识了土豆，那么土豆奇形怪状，第五个土豆可以是五个中的任意一个，那么这就让我们拥有了五套方案来教会小朋友如何认识土豆，即用其中的四个做认识学习，剩下的一个作为测试学习成果。这就是一个五折交叉验证的结果，我们可以将每一种学习方案的学习效果（可以是孩子确信第五个土豆是土豆的置信度）加起来取平均来代表这个小盆友学习认识土豆能力的总体评分，从而减小对数据的bias。 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:1:0","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#什么是k折交叉验证"},{"categories":["machine_learning"],"content":" model_selection 实现交叉验证数据集划分 # ==================================================== # Library # ==================================================== import pandas as pd from sklearn.model_selection import train_test_split, KFold, GroupKFold, StratifiedKFold, StratifiedGroupKFold ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:0","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#model_selection-实现交叉验证数据集划分"},{"categories":["machine_learning"],"content":" 简单的数据划分(不涉及多折) df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) train, test = train_test_split(df, train_size=0.9, shuffle=True) train out: featrues labels 4 big 1 0 long 1 5 small 0 2 long 1 1 high 1 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:1","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#简单的数据划分不涉及多折"},{"categories":["machine_learning"],"content":" KFold list data X = [\"a\", \"b\", \"c\", \"d\"] # n_splits=2 代表分成两折 kf = KFold(n_splits=2, shuffle=True, random_state=42) for train, test in kf.split(X): print(\"%s %s\" % (train, test)) out: [0 2] [1 3] [1 3] [0 2] pd data df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) print(df.head()) out: featrues labels 0 long 1 1 high 1 2 long 1 3 short 0 4 big 1 kf = KFold(n_splits=4, shuffle=True, random_state=42) for train, test in kf.split(df): print(\"%s %s\" % (train, test)) out: [2 3 4 5] [0 1] [0 1 3 4] [2 5] [0 1 2 3 5] [4] [0 1 2 4 5] [3] # 新建一列 df['fold'] = -1 # 给split的每一折用enumerate遍历编号，比如这里是0,1,2,3 for idx, (train, test) in enumerate(kf.split(df)): print(idx, train, test) out: 0 [2 3 4 5] [0 1] 1 [0 1 3 4] [2 5] 2 [0 1 2 3 5] [4] 3 [0 1 2 4 5] [3] 为每一折的数据行打标记如下(完整示例) df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df['fold'] = -1 kf = KFold(n_splits=4, shuffle=True, random_state=42) for idx, (train, test) in enumerate(kf.split(df)): df.loc[test,'fold'] = idx print(df.head()) out: featrues labels fold 0 long 1 0 1 high 1 0 2 long 1 1 3 short 0 3 4 big 1 2 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:2","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#kfold"},{"categories":["machine_learning"],"content":" KFold list data X = [\"a\", \"b\", \"c\", \"d\"] # n_splits=2 代表分成两折 kf = KFold(n_splits=2, shuffle=True, random_state=42) for train, test in kf.split(X): print(\"%s %s\" % (train, test)) out: [0 2] [1 3] [1 3] [0 2] pd data df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) print(df.head()) out: featrues labels 0 long 1 1 high 1 2 long 1 3 short 0 4 big 1 kf = KFold(n_splits=4, shuffle=True, random_state=42) for train, test in kf.split(df): print(\"%s %s\" % (train, test)) out: [2 3 4 5] [0 1] [0 1 3 4] [2 5] [0 1 2 3 5] [4] [0 1 2 4 5] [3] # 新建一列 df['fold'] = -1 # 给split的每一折用enumerate遍历编号，比如这里是0,1,2,3 for idx, (train, test) in enumerate(kf.split(df)): print(idx, train, test) out: 0 [2 3 4 5] [0 1] 1 [0 1 3 4] [2 5] 2 [0 1 2 3 5] [4] 3 [0 1 2 4 5] [3] 为每一折的数据行打标记如下(完整示例) df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df['fold'] = -1 kf = KFold(n_splits=4, shuffle=True, random_state=42) for idx, (train, test) in enumerate(kf.split(df)): df.loc[test,'fold'] = idx print(df.head()) out: featrues labels fold 0 long 1 0 1 high 1 0 2 long 1 1 3 short 0 3 4 big 1 2 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:2","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#list-data"},{"categories":["machine_learning"],"content":" KFold list data X = [\"a\", \"b\", \"c\", \"d\"] # n_splits=2 代表分成两折 kf = KFold(n_splits=2, shuffle=True, random_state=42) for train, test in kf.split(X): print(\"%s %s\" % (train, test)) out: [0 2] [1 3] [1 3] [0 2] pd data df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) print(df.head()) out: featrues labels 0 long 1 1 high 1 2 long 1 3 short 0 4 big 1 kf = KFold(n_splits=4, shuffle=True, random_state=42) for train, test in kf.split(df): print(\"%s %s\" % (train, test)) out: [2 3 4 5] [0 1] [0 1 3 4] [2 5] [0 1 2 3 5] [4] [0 1 2 4 5] [3] # 新建一列 df['fold'] = -1 # 给split的每一折用enumerate遍历编号，比如这里是0,1,2,3 for idx, (train, test) in enumerate(kf.split(df)): print(idx, train, test) out: 0 [2 3 4 5] [0 1] 1 [0 1 3 4] [2 5] 2 [0 1 2 3 5] [4] 3 [0 1 2 4 5] [3] 为每一折的数据行打标记如下(完整示例) df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df['fold'] = -1 kf = KFold(n_splits=4, shuffle=True, random_state=42) for idx, (train, test) in enumerate(kf.split(df)): df.loc[test,'fold'] = idx print(df.head()) out: featrues labels fold 0 long 1 0 1 high 1 0 2 long 1 1 3 short 0 3 4 big 1 2 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:2","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#pd-data"},{"categories":["machine_learning"],"content":" StratifiedKFold按标签比例来划分每一折的验证集和训练集，观察下列数据的标签，容易得到，在未划分数据集之前，标签比例为\"1\": “0” —\u003e 2:1 df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df['fold'] = -1 skf = StratifiedKFold(n_splits=2) for idx, (train, test) in enumerate(skf.split(X=df, y=df[\"labels\"])): df.loc[test,'fold'] = idx print(df) out: featrues labels fold 0 long 1 0 1 high 1 0 2 long 1 1 3 short 0 0 4 big 1 1 5 small 0 1 可以看到，分折之后，第0折中，标签\"1\":“0” 也呈现为2:1的比例，第1折也是如此。 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:3","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#stratifiedkfold"},{"categories":["machine_learning"],"content":" GroupKFold pd data按照group列的值来分折，也就是说，group列内的值的种类需要大于等于n_splits的值。 df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"2\", \"1\", \"0\", \"1\", \"0\"]}) # label有三类值，可以用这个作为group列，也可以自己定义新建group列 df['fold'] = -1 gkf = GroupKFold(n_splits=2) # n_splits要满足 2 \u003c= n_splits \u003c= group列中的唯一值数量 for idx, (train, test) in enumerate(gkf.split(X=df, y=df[\"labels\"], groups=df[\"labels\"])): df.loc[test,'fold'] = idx print(df.head()) out: featrues labels fold 0 long 1 0 1 high 2 1 2 long 1 0 3 short 0 1 4 big 1 0 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:4","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#groupkfold"},{"categories":["machine_learning"],"content":" GroupKFold pd data按照group列的值来分折，也就是说，group列内的值的种类需要大于等于n_splits的值。 df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"2\", \"1\", \"0\", \"1\", \"0\"]}) # label有三类值，可以用这个作为group列，也可以自己定义新建group列 df['fold'] = -1 gkf = GroupKFold(n_splits=2) # n_splits要满足 2 \u003c= n_splits \u003c= group列中的唯一值数量 for idx, (train, test) in enumerate(gkf.split(X=df, y=df[\"labels\"], groups=df[\"labels\"])): df.loc[test,'fold'] = idx print(df.head()) out: featrues labels fold 0 long 1 0 1 high 2 1 2 long 1 0 3 short 0 1 4 big 1 0 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:4","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#pd-data-1"},{"categories":["machine_learning"],"content":" StratifiedGroupKFold list data此方法结合了StratifiedKFold和GroupKFold，在按group进行分折的同时尽量保证每一折分出的数据接近划分前的标签分布(自然，剩下的也是由其他折组成，也自然符合标签的数据分布)。 from sklearn.model_selection import StratifiedGroupKFold X = list(range(18)) y = [1] * 6 + [0] * 12 groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6] sgkf = StratifiedGroupKFold(n_splits=3) for train, test in sgkf.split(X, y, groups=groups): print(\"%s %s\" % (train, test)) out: 实践说明。 在目前的实现中，大多数情况下不可能实现完全洗牌。当shuffle=True时，会发生以下情况。 所有的group都被打乱了。 使用稳定的排序法，按照类的标准偏差对组进行排序。 排序后的组被迭代，并分配给每一折。 这意味着只有具有相同的类分布标准差的组才会被打乱，这在每个组只有一个类时可能很有用。 该算法贪婪地将每个组分配到n_splits测试集中的一个，选择测试集，使各测试集的类分布差异最小。组的分配从类别频率方差最大的组到最小的组进行，也就是说，在一个或几个类别上达到峰值的大组被首先分配。 这种分法在某种意义上是次优的，它可能产生不平衡的分法，即使完美的分层是可能的。如果你在每个组中的类的分布相对接近，使用GroupKFold会更好。 pd data df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df['fold'] = -1 sgkf = StratifiedGroupKFold(n_splits=2) for idx, (train, test) in enumerate(sgkf.split(X=df, y=df[\"labels\"], groups=df[\"labels\"])): df.loc[test,'fold'] = idx print(df) featrues labels fold 0 long 1 0 1 high 1 0 2 long 1 0 3 short 0 1 4 big 1 0 5 small 0 1 更多详见官方文档 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:5","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#stratifiedgroupkfold"},{"categories":["machine_learning"],"content":" StratifiedGroupKFold list data此方法结合了StratifiedKFold和GroupKFold，在按group进行分折的同时尽量保证每一折分出的数据接近划分前的标签分布(自然，剩下的也是由其他折组成，也自然符合标签的数据分布)。 from sklearn.model_selection import StratifiedGroupKFold X = list(range(18)) y = [1] * 6 + [0] * 12 groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6] sgkf = StratifiedGroupKFold(n_splits=3) for train, test in sgkf.split(X, y, groups=groups): print(\"%s %s\" % (train, test)) out: 实践说明。 在目前的实现中，大多数情况下不可能实现完全洗牌。当shuffle=True时，会发生以下情况。 所有的group都被打乱了。 使用稳定的排序法，按照类的标准偏差对组进行排序。 排序后的组被迭代，并分配给每一折。 这意味着只有具有相同的类分布标准差的组才会被打乱，这在每个组只有一个类时可能很有用。 该算法贪婪地将每个组分配到n_splits测试集中的一个，选择测试集，使各测试集的类分布差异最小。组的分配从类别频率方差最大的组到最小的组进行，也就是说，在一个或几个类别上达到峰值的大组被首先分配。 这种分法在某种意义上是次优的，它可能产生不平衡的分法，即使完美的分层是可能的。如果你在每个组中的类的分布相对接近，使用GroupKFold会更好。 pd data df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df['fold'] = -1 sgkf = StratifiedGroupKFold(n_splits=2) for idx, (train, test) in enumerate(sgkf.split(X=df, y=df[\"labels\"], groups=df[\"labels\"])): df.loc[test,'fold'] = idx print(df) featrues labels fold 0 long 1 0 1 high 1 0 2 long 1 0 3 short 0 1 4 big 1 0 5 small 0 1 更多详见官方文档 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:5","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#list-data-1"},{"categories":["machine_learning"],"content":" StratifiedGroupKFold list data此方法结合了StratifiedKFold和GroupKFold，在按group进行分折的同时尽量保证每一折分出的数据接近划分前的标签分布(自然，剩下的也是由其他折组成，也自然符合标签的数据分布)。 from sklearn.model_selection import StratifiedGroupKFold X = list(range(18)) y = [1] * 6 + [0] * 12 groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6] sgkf = StratifiedGroupKFold(n_splits=3) for train, test in sgkf.split(X, y, groups=groups): print(\"%s %s\" % (train, test)) out: 实践说明。 在目前的实现中，大多数情况下不可能实现完全洗牌。当shuffle=True时，会发生以下情况。 所有的group都被打乱了。 使用稳定的排序法，按照类的标准偏差对组进行排序。 排序后的组被迭代，并分配给每一折。 这意味着只有具有相同的类分布标准差的组才会被打乱，这在每个组只有一个类时可能很有用。 该算法贪婪地将每个组分配到n_splits测试集中的一个，选择测试集，使各测试集的类分布差异最小。组的分配从类别频率方差最大的组到最小的组进行，也就是说，在一个或几个类别上达到峰值的大组被首先分配。 这种分法在某种意义上是次优的，它可能产生不平衡的分法，即使完美的分层是可能的。如果你在每个组中的类的分布相对接近，使用GroupKFold会更好。 pd data df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df['fold'] = -1 sgkf = StratifiedGroupKFold(n_splits=2) for idx, (train, test) in enumerate(sgkf.split(X=df, y=df[\"labels\"], groups=df[\"labels\"])): df.loc[test,'fold'] = idx print(df) featrues labels fold 0 long 1 0 1 high 1 0 2 long 1 0 3 short 0 1 4 big 1 0 5 small 0 1 更多详见官方文档 ","date":"2022-06-29","objectID":"/kfold_cross_validation/:2:5","series":["python_data_analysis"],"tags":["py"],"title":"K折交叉验证","uri":"/kfold_cross_validation/#pd-data-2"},{"categories":[],"content":" 命令行的使用","date":"2022-06-04","objectID":"/kaggle/:1:0","series":["data_science_contest"],"tags":[],"title":"Kaggle使用介绍","uri":"/kaggle/#命令行的使用"},{"categories":[],"content":" Kaggle API 安装 第一步，本地 pip install kaggle 第二步，kaggle 点击头像-\u003eAccount-\u003eAPI-\u003eCreate New API Token（如果想要删除以往使用过的Token可以点击API下的Expire API Token） 第三步，本地 把下载的kaggle.json放在用户根目录的.kaggle/文件夹下 安装完成。 ","date":"2022-06-04","objectID":"/kaggle/:1:1","series":["data_science_contest"],"tags":[],"title":"Kaggle使用介绍","uri":"/kaggle/#kaggle-api-安装"},{"categories":[],"content":" 关于数据集数据集可以用来上传本地的数据以及在本地训练好的模型文件。 创建并上传数据集 第一步，本地初始化 ，将data_dir换成你自己的路径名字 # 上传不含子文件夹的数据文件夹 kaggle datasets init -p 'data_dir' # 上传含有子文件夹的数据文件夹不压缩 kaggle datasets init -p 'data_dir' -r tar 第二步，数据集改名 第一步之后会在data_dir目录下生成一个dataset-metadata.json文件内容如下，把title和id中的title部分改成你想要的名字，比如我改为了best-model。 { \"title\": \"best-model\", \"id\": \"oliverlionado/best-model\", \"licenses\": [ { \"name\": \"CC0-1.0\" } ] } 第三步，上传数据集 kaggle datasets create -p model_data/ 然后你的数据集里就会出现一个新的数据集，以本例为例，数据集名字叫做best-model，里面的内容就是model_data/目录下的文件（不含model_data/文件夹）。 更新某个数据集比如我想使用model_data/文件夹下的文件去更新 kaggle datasets version -p model_data/ -m \"update\" 注意——如果你想更新一个不是在当前目录下创建的数据集（目录里没有dataset-metadata.json文件） 首先你需要下载json文件 kaggle datasets metadata -p model_data/ oliverlionado/best-model 其中oliverlionado/best-model指的是已存在数据集的id 然后再执行上一条命令~ ","date":"2022-06-04","objectID":"/kaggle/:1:2","series":["data_science_contest"],"tags":[],"title":"Kaggle使用介绍","uri":"/kaggle/#关于数据集"},{"categories":[],"content":" 关于数据集数据集可以用来上传本地的数据以及在本地训练好的模型文件。 创建并上传数据集 第一步，本地初始化 ，将data_dir换成你自己的路径名字 # 上传不含子文件夹的数据文件夹 kaggle datasets init -p 'data_dir' # 上传含有子文件夹的数据文件夹不压缩 kaggle datasets init -p 'data_dir' -r tar 第二步，数据集改名 第一步之后会在data_dir目录下生成一个dataset-metadata.json文件内容如下，把title和id中的title部分改成你想要的名字，比如我改为了best-model。 { \"title\": \"best-model\", \"id\": \"oliverlionado/best-model\", \"licenses\": [ { \"name\": \"CC0-1.0\" } ] } 第三步，上传数据集 kaggle datasets create -p model_data/ 然后你的数据集里就会出现一个新的数据集，以本例为例，数据集名字叫做best-model，里面的内容就是model_data/目录下的文件（不含model_data/文件夹）。 更新某个数据集比如我想使用model_data/文件夹下的文件去更新 kaggle datasets version -p model_data/ -m \"update\" 注意——如果你想更新一个不是在当前目录下创建的数据集（目录里没有dataset-metadata.json文件） 首先你需要下载json文件 kaggle datasets metadata -p model_data/ oliverlionado/best-model 其中oliverlionado/best-model指的是已存在数据集的id 然后再执行上一条命令~ ","date":"2022-06-04","objectID":"/kaggle/:1:2","series":["data_science_contest"],"tags":[],"title":"Kaggle使用介绍","uri":"/kaggle/#创建并上传数据集"},{"categories":[],"content":" 关于数据集数据集可以用来上传本地的数据以及在本地训练好的模型文件。 创建并上传数据集 第一步，本地初始化 ，将data_dir换成你自己的路径名字 # 上传不含子文件夹的数据文件夹 kaggle datasets init -p 'data_dir' # 上传含有子文件夹的数据文件夹不压缩 kaggle datasets init -p 'data_dir' -r tar 第二步，数据集改名 第一步之后会在data_dir目录下生成一个dataset-metadata.json文件内容如下，把title和id中的title部分改成你想要的名字，比如我改为了best-model。 { \"title\": \"best-model\", \"id\": \"oliverlionado/best-model\", \"licenses\": [ { \"name\": \"CC0-1.0\" } ] } 第三步，上传数据集 kaggle datasets create -p model_data/ 然后你的数据集里就会出现一个新的数据集，以本例为例，数据集名字叫做best-model，里面的内容就是model_data/目录下的文件（不含model_data/文件夹）。 更新某个数据集比如我想使用model_data/文件夹下的文件去更新 kaggle datasets version -p model_data/ -m \"update\" 注意——如果你想更新一个不是在当前目录下创建的数据集（目录里没有dataset-metadata.json文件） 首先你需要下载json文件 kaggle datasets metadata -p model_data/ oliverlionado/best-model 其中oliverlionado/best-model指的是已存在数据集的id 然后再执行上一条命令~ ","date":"2022-06-04","objectID":"/kaggle/:1:2","series":["data_science_contest"],"tags":[],"title":"Kaggle使用介绍","uri":"/kaggle/#更新某个数据集"},{"categories":["technology_log"],"content":" 报错lookup 报错 原因 解决方案 RuntimeError: CUDA error: no kernel image is available for execution on the device torch版本与CUDA不匹配 找对应版本的torch进行安装 ","date":"2022-05-23","objectID":"/cuda/:0:1","series":[],"tags":["env"],"title":"CUDA相关问题和设置","uri":"/cuda/#报错lookup"},{"categories":[],"content":" Pandas import pandas as pd ","date":"2022-05-23","objectID":"/pandas/:1:0","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#pandas"},{"categories":[],"content":" 数据读取 csv格式 有表头模式 df = pd.read_csv('file') 无表头模式 df = pd.read_csv('file', header=None) 读取之后为各列取表头（列名） df = pd.read_csv('data/dataset/raw_data/data.csv', header=None) df.columns = ['label_text', 'chapter', 'section', 'subsection', 'text'] ","date":"2022-05-23","objectID":"/pandas/:1:1","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#数据读取"},{"categories":[],"content":" 形状处理 打印开头几行数据 df.head() input_path = 'data/contest_data/' #input need be adjusted df = pd.read_csv(input_path+'train.csv') df_title = pd.read_csv(input_path+'titles.csv') 去除空值数据行 df = df.dropna() 根据特定列数据特征删除数据行例如，删除unit列值为’参考价’的行 df = df.drop(df[df['unit']=='参考价'].index) 合并两个Dataframe第一个df df id anchor target context score 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 ... ... ... ... ... ... 第二个df df_title code title section class subclass group main_group 0 A HUMAN NECESSITIES A NaN NaN NaN NaN ... ... ... ... ... ... ... ... 将它们分别按照context列内容和code列内容对应相等进行拼接，df在左，dftitle（code值唯一）可重复使用，相当于查表。 df = df.merge(df_title, how='left', left_on='context', right_on='code') df = df[['id', 'anchor', 'target', 'context', 'score', 'title']] 拼接完成 df id anchor target context score title 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO… … … … … … … … ","date":"2022-05-23","objectID":"/pandas/:1:2","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#形状处理"},{"categories":[],"content":" 形状处理 打印开头几行数据 df.head() input_path = 'data/contest_data/' #input need be adjusted df = pd.read_csv(input_path+'train.csv') df_title = pd.read_csv(input_path+'titles.csv') 去除空值数据行 df = df.dropna() 根据特定列数据特征删除数据行例如，删除unit列值为’参考价’的行 df = df.drop(df[df['unit']=='参考价'].index) 合并两个Dataframe第一个df df id anchor target context score 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 ... ... ... ... ... ... 第二个df df_title code title section class subclass group main_group 0 A HUMAN NECESSITIES A NaN NaN NaN NaN ... ... ... ... ... ... ... ... 将它们分别按照context列内容和code列内容对应相等进行拼接，df在左，dftitle（code值唯一）可重复使用，相当于查表。 df = df.merge(df_title, how='left', left_on='context', right_on='code') df = df[['id', 'anchor', 'target', 'context', 'score', 'title']] 拼接完成 df id anchor target context score title 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO… … … … … … … … ","date":"2022-05-23","objectID":"/pandas/:1:2","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#打印开头几行数据"},{"categories":[],"content":" 形状处理 打印开头几行数据 df.head() input_path = 'data/contest_data/' #input need be adjusted df = pd.read_csv(input_path+'train.csv') df_title = pd.read_csv(input_path+'titles.csv') 去除空值数据行 df = df.dropna() 根据特定列数据特征删除数据行例如，删除unit列值为’参考价’的行 df = df.drop(df[df['unit']=='参考价'].index) 合并两个Dataframe第一个df df id anchor target context score 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 ... ... ... ... ... ... 第二个df df_title code title section class subclass group main_group 0 A HUMAN NECESSITIES A NaN NaN NaN NaN ... ... ... ... ... ... ... ... 将它们分别按照context列内容和code列内容对应相等进行拼接，df在左，dftitle（code值唯一）可重复使用，相当于查表。 df = df.merge(df_title, how='left', left_on='context', right_on='code') df = df[['id', 'anchor', 'target', 'context', 'score', 'title']] 拼接完成 df id anchor target context score title 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO… … … … … … … … ","date":"2022-05-23","objectID":"/pandas/:1:2","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#去除空值数据行"},{"categories":[],"content":" 形状处理 打印开头几行数据 df.head() input_path = 'data/contest_data/' #input need be adjusted df = pd.read_csv(input_path+'train.csv') df_title = pd.read_csv(input_path+'titles.csv') 去除空值数据行 df = df.dropna() 根据特定列数据特征删除数据行例如，删除unit列值为’参考价’的行 df = df.drop(df[df['unit']=='参考价'].index) 合并两个Dataframe第一个df df id anchor target context score 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 ... ... ... ... ... ... 第二个df df_title code title section class subclass group main_group 0 A HUMAN NECESSITIES A NaN NaN NaN NaN ... ... ... ... ... ... ... ... 将它们分别按照context列内容和code列内容对应相等进行拼接，df在左，dftitle（code值唯一）可重复使用，相当于查表。 df = df.merge(df_title, how='left', left_on='context', right_on='code') df = df[['id', 'anchor', 'target', 'context', 'score', 'title']] 拼接完成 df id anchor target context score title 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO… … … … … … … … ","date":"2022-05-23","objectID":"/pandas/:1:2","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#根据特定列数据特征删除数据行"},{"categories":[],"content":" 形状处理 打印开头几行数据 df.head() input_path = 'data/contest_data/' #input need be adjusted df = pd.read_csv(input_path+'train.csv') df_title = pd.read_csv(input_path+'titles.csv') 去除空值数据行 df = df.dropna() 根据特定列数据特征删除数据行例如，删除unit列值为’参考价’的行 df = df.drop(df[df['unit']=='参考价'].index) 合并两个Dataframe第一个df df id anchor target context score 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 ... ... ... ... ... ... 第二个df df_title code title section class subclass group main_group 0 A HUMAN NECESSITIES A NaN NaN NaN NaN ... ... ... ... ... ... ... ... 将它们分别按照context列内容和code列内容对应相等进行拼接，df在左，dftitle（code值唯一）可重复使用，相当于查表。 df = df.merge(df_title, how='left', left_on='context', right_on='code') df = df[['id', 'anchor', 'target', 'context', 'score', 'title']] 拼接完成 df id anchor target context score title 0 37d61fd2272659b1 abatement abatement of pollution A47 0.50 FURNITURE; DOMESTIC ARTICLES OR APPLIANCES; CO… … … … … … … … ","date":"2022-05-23","objectID":"/pandas/:1:2","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#合并两个dataframe"},{"categories":[],"content":" 数据处理 隐函数方法举个最简单的例子，total里都是899万元的形式，把万元替换掉，并转为float格式，其实隐函数干的远不止这些…… format_fun = lambda x: float(x.replace('万元', '')) df[\"price\"] = df[\"total\"].apply(format_fun) 合并两列字符串 df[\"Full Name\"] = df[\"province\"].map(str) +','+ df[\"city\"] city_lis = list(set(list(df['Full Name'].values))) print(city_lis[:3]) ['山东,青岛', '江西,赣州', '甘肃,兰州'] 相同功能 df[\"Full Name\"] = df[\"First\"] + \" \" + df[\"Last\"] df['Full Name'] = df[['First', 'Last']].apply(' '.join, axis=1) df['Full Name'] = df['First'].str.cat(df['Last'],sep=\" \") df['Full Name'] = df[['First', 'Last']].agg(' '.join, axis=1) 列间数据计算举个例子，通过总价和面积得到单价列 df[\"total_price\"] = df[\"total\"].apply(lambda x: float(x.replace('万元', ''))) df['area_num'] = df[\"area\"].apply(lambda x: float(x.replace('㎡', ''))) df['unit_pice'] = df['total_price'].map(float) / df['area_num'] * 10000 分组数据计算 计算均值举例，按列Full Name 分组，计算各组的unit_price均值 df.groupby('Full Name')['unit_pice'].mean() 数据转换强转就好了dict(),int(),float()…… dic = dict(df.groupby('Full Name')['unit_pice'].mean()) 转化为numpy.ndarray array array = df['text'].values array = df['text'].values.astype(str) loc方法如果不存在fold列，新建’fold’列，并在位置[0, 5]上赋值为1。 import pandas as pd df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df.loc[[0, 5], 'fold'] = 1 df featrues labels fold 0 long 1 1.0 1 high 1 NaN 2 long 1 NaN 3 short 0 NaN 4 big 1 NaN ","date":"2022-05-23","objectID":"/pandas/:1:3","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#数据处理"},{"categories":[],"content":" 数据处理 隐函数方法举个最简单的例子，total里都是899万元的形式，把万元替换掉，并转为float格式，其实隐函数干的远不止这些…… format_fun = lambda x: float(x.replace('万元', '')) df[\"price\"] = df[\"total\"].apply(format_fun) 合并两列字符串 df[\"Full Name\"] = df[\"province\"].map(str) +','+ df[\"city\"] city_lis = list(set(list(df['Full Name'].values))) print(city_lis[:3]) ['山东,青岛', '江西,赣州', '甘肃,兰州'] 相同功能 df[\"Full Name\"] = df[\"First\"] + \" \" + df[\"Last\"] df['Full Name'] = df[['First', 'Last']].apply(' '.join, axis=1) df['Full Name'] = df['First'].str.cat(df['Last'],sep=\" \") df['Full Name'] = df[['First', 'Last']].agg(' '.join, axis=1) 列间数据计算举个例子，通过总价和面积得到单价列 df[\"total_price\"] = df[\"total\"].apply(lambda x: float(x.replace('万元', ''))) df['area_num'] = df[\"area\"].apply(lambda x: float(x.replace('㎡', ''))) df['unit_pice'] = df['total_price'].map(float) / df['area_num'] * 10000 分组数据计算 计算均值举例，按列Full Name 分组，计算各组的unit_price均值 df.groupby('Full Name')['unit_pice'].mean() 数据转换强转就好了dict(),int(),float()…… dic = dict(df.groupby('Full Name')['unit_pice'].mean()) 转化为numpy.ndarray array array = df['text'].values array = df['text'].values.astype(str) loc方法如果不存在fold列，新建’fold’列，并在位置[0, 5]上赋值为1。 import pandas as pd df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df.loc[[0, 5], 'fold'] = 1 df featrues labels fold 0 long 1 1.0 1 high 1 NaN 2 long 1 NaN 3 short 0 NaN 4 big 1 NaN ","date":"2022-05-23","objectID":"/pandas/:1:3","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#隐函数方法"},{"categories":[],"content":" 数据处理 隐函数方法举个最简单的例子，total里都是899万元的形式，把万元替换掉，并转为float格式，其实隐函数干的远不止这些…… format_fun = lambda x: float(x.replace('万元', '')) df[\"price\"] = df[\"total\"].apply(format_fun) 合并两列字符串 df[\"Full Name\"] = df[\"province\"].map(str) +','+ df[\"city\"] city_lis = list(set(list(df['Full Name'].values))) print(city_lis[:3]) ['山东,青岛', '江西,赣州', '甘肃,兰州'] 相同功能 df[\"Full Name\"] = df[\"First\"] + \" \" + df[\"Last\"] df['Full Name'] = df[['First', 'Last']].apply(' '.join, axis=1) df['Full Name'] = df['First'].str.cat(df['Last'],sep=\" \") df['Full Name'] = df[['First', 'Last']].agg(' '.join, axis=1) 列间数据计算举个例子，通过总价和面积得到单价列 df[\"total_price\"] = df[\"total\"].apply(lambda x: float(x.replace('万元', ''))) df['area_num'] = df[\"area\"].apply(lambda x: float(x.replace('㎡', ''))) df['unit_pice'] = df['total_price'].map(float) / df['area_num'] * 10000 分组数据计算 计算均值举例，按列Full Name 分组，计算各组的unit_price均值 df.groupby('Full Name')['unit_pice'].mean() 数据转换强转就好了dict(),int(),float()…… dic = dict(df.groupby('Full Name')['unit_pice'].mean()) 转化为numpy.ndarray array array = df['text'].values array = df['text'].values.astype(str) loc方法如果不存在fold列，新建’fold’列，并在位置[0, 5]上赋值为1。 import pandas as pd df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df.loc[[0, 5], 'fold'] = 1 df featrues labels fold 0 long 1 1.0 1 high 1 NaN 2 long 1 NaN 3 short 0 NaN 4 big 1 NaN ","date":"2022-05-23","objectID":"/pandas/:1:3","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#合并两列字符串"},{"categories":[],"content":" 数据处理 隐函数方法举个最简单的例子，total里都是899万元的形式，把万元替换掉，并转为float格式，其实隐函数干的远不止这些…… format_fun = lambda x: float(x.replace('万元', '')) df[\"price\"] = df[\"total\"].apply(format_fun) 合并两列字符串 df[\"Full Name\"] = df[\"province\"].map(str) +','+ df[\"city\"] city_lis = list(set(list(df['Full Name'].values))) print(city_lis[:3]) ['山东,青岛', '江西,赣州', '甘肃,兰州'] 相同功能 df[\"Full Name\"] = df[\"First\"] + \" \" + df[\"Last\"] df['Full Name'] = df[['First', 'Last']].apply(' '.join, axis=1) df['Full Name'] = df['First'].str.cat(df['Last'],sep=\" \") df['Full Name'] = df[['First', 'Last']].agg(' '.join, axis=1) 列间数据计算举个例子，通过总价和面积得到单价列 df[\"total_price\"] = df[\"total\"].apply(lambda x: float(x.replace('万元', ''))) df['area_num'] = df[\"area\"].apply(lambda x: float(x.replace('㎡', ''))) df['unit_pice'] = df['total_price'].map(float) / df['area_num'] * 10000 分组数据计算 计算均值举例，按列Full Name 分组，计算各组的unit_price均值 df.groupby('Full Name')['unit_pice'].mean() 数据转换强转就好了dict(),int(),float()…… dic = dict(df.groupby('Full Name')['unit_pice'].mean()) 转化为numpy.ndarray array array = df['text'].values array = df['text'].values.astype(str) loc方法如果不存在fold列，新建’fold’列，并在位置[0, 5]上赋值为1。 import pandas as pd df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df.loc[[0, 5], 'fold'] = 1 df featrues labels fold 0 long 1 1.0 1 high 1 NaN 2 long 1 NaN 3 short 0 NaN 4 big 1 NaN ","date":"2022-05-23","objectID":"/pandas/:1:3","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#列间数据计算"},{"categories":[],"content":" 数据处理 隐函数方法举个最简单的例子，total里都是899万元的形式，把万元替换掉，并转为float格式，其实隐函数干的远不止这些…… format_fun = lambda x: float(x.replace('万元', '')) df[\"price\"] = df[\"total\"].apply(format_fun) 合并两列字符串 df[\"Full Name\"] = df[\"province\"].map(str) +','+ df[\"city\"] city_lis = list(set(list(df['Full Name'].values))) print(city_lis[:3]) ['山东,青岛', '江西,赣州', '甘肃,兰州'] 相同功能 df[\"Full Name\"] = df[\"First\"] + \" \" + df[\"Last\"] df['Full Name'] = df[['First', 'Last']].apply(' '.join, axis=1) df['Full Name'] = df['First'].str.cat(df['Last'],sep=\" \") df['Full Name'] = df[['First', 'Last']].agg(' '.join, axis=1) 列间数据计算举个例子，通过总价和面积得到单价列 df[\"total_price\"] = df[\"total\"].apply(lambda x: float(x.replace('万元', ''))) df['area_num'] = df[\"area\"].apply(lambda x: float(x.replace('㎡', ''))) df['unit_pice'] = df['total_price'].map(float) / df['area_num'] * 10000 分组数据计算 计算均值举例，按列Full Name 分组，计算各组的unit_price均值 df.groupby('Full Name')['unit_pice'].mean() 数据转换强转就好了dict(),int(),float()…… dic = dict(df.groupby('Full Name')['unit_pice'].mean()) 转化为numpy.ndarray array array = df['text'].values array = df['text'].values.astype(str) loc方法如果不存在fold列，新建’fold’列，并在位置[0, 5]上赋值为1。 import pandas as pd df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df.loc[[0, 5], 'fold'] = 1 df featrues labels fold 0 long 1 1.0 1 high 1 NaN 2 long 1 NaN 3 short 0 NaN 4 big 1 NaN ","date":"2022-05-23","objectID":"/pandas/:1:3","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#分组数据计算"},{"categories":[],"content":" 数据处理 隐函数方法举个最简单的例子，total里都是899万元的形式，把万元替换掉，并转为float格式，其实隐函数干的远不止这些…… format_fun = lambda x: float(x.replace('万元', '')) df[\"price\"] = df[\"total\"].apply(format_fun) 合并两列字符串 df[\"Full Name\"] = df[\"province\"].map(str) +','+ df[\"city\"] city_lis = list(set(list(df['Full Name'].values))) print(city_lis[:3]) ['山东,青岛', '江西,赣州', '甘肃,兰州'] 相同功能 df[\"Full Name\"] = df[\"First\"] + \" \" + df[\"Last\"] df['Full Name'] = df[['First', 'Last']].apply(' '.join, axis=1) df['Full Name'] = df['First'].str.cat(df['Last'],sep=\" \") df['Full Name'] = df[['First', 'Last']].agg(' '.join, axis=1) 列间数据计算举个例子，通过总价和面积得到单价列 df[\"total_price\"] = df[\"total\"].apply(lambda x: float(x.replace('万元', ''))) df['area_num'] = df[\"area\"].apply(lambda x: float(x.replace('㎡', ''))) df['unit_pice'] = df['total_price'].map(float) / df['area_num'] * 10000 分组数据计算 计算均值举例，按列Full Name 分组，计算各组的unit_price均值 df.groupby('Full Name')['unit_pice'].mean() 数据转换强转就好了dict(),int(),float()…… dic = dict(df.groupby('Full Name')['unit_pice'].mean()) 转化为numpy.ndarray array array = df['text'].values array = df['text'].values.astype(str) loc方法如果不存在fold列，新建’fold’列，并在位置[0, 5]上赋值为1。 import pandas as pd df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df.loc[[0, 5], 'fold'] = 1 df featrues labels fold 0 long 1 1.0 1 high 1 NaN 2 long 1 NaN 3 short 0 NaN 4 big 1 NaN ","date":"2022-05-23","objectID":"/pandas/:1:3","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#计算均值"},{"categories":[],"content":" 数据处理 隐函数方法举个最简单的例子，total里都是899万元的形式，把万元替换掉，并转为float格式，其实隐函数干的远不止这些…… format_fun = lambda x: float(x.replace('万元', '')) df[\"price\"] = df[\"total\"].apply(format_fun) 合并两列字符串 df[\"Full Name\"] = df[\"province\"].map(str) +','+ df[\"city\"] city_lis = list(set(list(df['Full Name'].values))) print(city_lis[:3]) ['山东,青岛', '江西,赣州', '甘肃,兰州'] 相同功能 df[\"Full Name\"] = df[\"First\"] + \" \" + df[\"Last\"] df['Full Name'] = df[['First', 'Last']].apply(' '.join, axis=1) df['Full Name'] = df['First'].str.cat(df['Last'],sep=\" \") df['Full Name'] = df[['First', 'Last']].agg(' '.join, axis=1) 列间数据计算举个例子，通过总价和面积得到单价列 df[\"total_price\"] = df[\"total\"].apply(lambda x: float(x.replace('万元', ''))) df['area_num'] = df[\"area\"].apply(lambda x: float(x.replace('㎡', ''))) df['unit_pice'] = df['total_price'].map(float) / df['area_num'] * 10000 分组数据计算 计算均值举例，按列Full Name 分组，计算各组的unit_price均值 df.groupby('Full Name')['unit_pice'].mean() 数据转换强转就好了dict(),int(),float()…… dic = dict(df.groupby('Full Name')['unit_pice'].mean()) 转化为numpy.ndarray array array = df['text'].values array = df['text'].values.astype(str) loc方法如果不存在fold列，新建’fold’列，并在位置[0, 5]上赋值为1。 import pandas as pd df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df.loc[[0, 5], 'fold'] = 1 df featrues labels fold 0 long 1 1.0 1 high 1 NaN 2 long 1 NaN 3 short 0 NaN 4 big 1 NaN ","date":"2022-05-23","objectID":"/pandas/:1:3","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#数据转换"},{"categories":[],"content":" 数据处理 隐函数方法举个最简单的例子，total里都是899万元的形式，把万元替换掉，并转为float格式，其实隐函数干的远不止这些…… format_fun = lambda x: float(x.replace('万元', '')) df[\"price\"] = df[\"total\"].apply(format_fun) 合并两列字符串 df[\"Full Name\"] = df[\"province\"].map(str) +','+ df[\"city\"] city_lis = list(set(list(df['Full Name'].values))) print(city_lis[:3]) ['山东,青岛', '江西,赣州', '甘肃,兰州'] 相同功能 df[\"Full Name\"] = df[\"First\"] + \" \" + df[\"Last\"] df['Full Name'] = df[['First', 'Last']].apply(' '.join, axis=1) df['Full Name'] = df['First'].str.cat(df['Last'],sep=\" \") df['Full Name'] = df[['First', 'Last']].agg(' '.join, axis=1) 列间数据计算举个例子，通过总价和面积得到单价列 df[\"total_price\"] = df[\"total\"].apply(lambda x: float(x.replace('万元', ''))) df['area_num'] = df[\"area\"].apply(lambda x: float(x.replace('㎡', ''))) df['unit_pice'] = df['total_price'].map(float) / df['area_num'] * 10000 分组数据计算 计算均值举例，按列Full Name 分组，计算各组的unit_price均值 df.groupby('Full Name')['unit_pice'].mean() 数据转换强转就好了dict(),int(),float()…… dic = dict(df.groupby('Full Name')['unit_pice'].mean()) 转化为numpy.ndarray array array = df['text'].values array = df['text'].values.astype(str) loc方法如果不存在fold列，新建’fold’列，并在位置[0, 5]上赋值为1。 import pandas as pd df = pd.DataFrame({\"featrues\": [\"long\", \"high\", \"long\", \"short\", \"big\", \"small\"], \"labels\": [\"1\", \"1\", \"1\", \"0\", \"1\", \"0\"]}) df.loc[[0, 5], 'fold'] = 1 df featrues labels fold 0 long 1 1.0 1 high 1 NaN 2 long 1 NaN 3 short 0 NaN 4 big 1 NaN ","date":"2022-05-23","objectID":"/pandas/:1:3","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#loc方法"},{"categories":[],"content":" 打印属性 显示所有行和列 #显示所有列 pd.set_option('display.max_columns', None) #显示所有行 pd.set_option('display.max_rows', None) #设置value的显示长度为100，默认为50 pd.set_option('max_colwidth',100) ","date":"2022-05-23","objectID":"/pandas/:1:4","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#打印属性"},{"categories":[],"content":" 打印属性 显示所有行和列 #显示所有列 pd.set_option('display.max_columns', None) #显示所有行 pd.set_option('display.max_rows', None) #设置value的显示长度为100，默认为50 pd.set_option('max_colwidth',100) ","date":"2022-05-23","objectID":"/pandas/:1:4","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#显示所有行和列"},{"categories":[],"content":" 采样方法 随机采样 # 随机采样100个 \u003e\u003e\u003e df_sampled = df.sample(n=100) # 随机采样50%的样本 \u003e\u003e\u003e df_sampled = df.sample(frac=0.5) 条件采样 # 选苹果的行 \u003e\u003e\u003e df_sampled = df[df['fruit'] == 'apple'] 分组采样 \u003e\u003e\u003e df = pd.DataFrame( ... {\"a\": [\"red\"] * 2 + [\"blue\"] * 2 + [\"black\"] * 2, \"b\": range(6)} ... ) \u003e\u003e\u003e df a b 0 red 0 1 red 1 2 blue 2 3 blue 3 4 black 4 5 black 5 # 每组里头采样一个数据 \u003e\u003e\u003e df.groupby(\"a\").sample(n=1, random_state=1) a b 4 black 4 2 blue 2 1 red 1 # 按列\"a\"的值分组采样，只保留列\"b\"的值 \u003e\u003e\u003e df.groupby(\"a\")[\"b\"].sample(frac=0.5, random_state=2) 5 5 2 2 0 0 Name: b, dtype: int64 # 为每个样本赋予采样权重 \u003e\u003e\u003e df.groupby(\"a\").sample( ... n=1, ... weights=[1, 1, 1, 0, 0, 1], ... random_state=1, ... ) a b 5 black 5 2 blue 2 0 red 0 ","date":"2022-05-23","objectID":"/pandas/:1:5","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#采样方法"},{"categories":[],"content":" 采样方法 随机采样 # 随机采样100个 \u003e\u003e\u003e df_sampled = df.sample(n=100) # 随机采样50%的样本 \u003e\u003e\u003e df_sampled = df.sample(frac=0.5) 条件采样 # 选苹果的行 \u003e\u003e\u003e df_sampled = df[df['fruit'] == 'apple'] 分组采样 \u003e\u003e\u003e df = pd.DataFrame( ... {\"a\": [\"red\"] * 2 + [\"blue\"] * 2 + [\"black\"] * 2, \"b\": range(6)} ... ) \u003e\u003e\u003e df a b 0 red 0 1 red 1 2 blue 2 3 blue 3 4 black 4 5 black 5 # 每组里头采样一个数据 \u003e\u003e\u003e df.groupby(\"a\").sample(n=1, random_state=1) a b 4 black 4 2 blue 2 1 red 1 # 按列\"a\"的值分组采样，只保留列\"b\"的值 \u003e\u003e\u003e df.groupby(\"a\")[\"b\"].sample(frac=0.5, random_state=2) 5 5 2 2 0 0 Name: b, dtype: int64 # 为每个样本赋予采样权重 \u003e\u003e\u003e df.groupby(\"a\").sample( ... n=1, ... weights=[1, 1, 1, 0, 0, 1], ... random_state=1, ... ) a b 5 black 5 2 blue 2 0 red 0 ","date":"2022-05-23","objectID":"/pandas/:1:5","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#随机采样"},{"categories":[],"content":" 采样方法 随机采样 # 随机采样100个 \u003e\u003e\u003e df_sampled = df.sample(n=100) # 随机采样50%的样本 \u003e\u003e\u003e df_sampled = df.sample(frac=0.5) 条件采样 # 选苹果的行 \u003e\u003e\u003e df_sampled = df[df['fruit'] == 'apple'] 分组采样 \u003e\u003e\u003e df = pd.DataFrame( ... {\"a\": [\"red\"] * 2 + [\"blue\"] * 2 + [\"black\"] * 2, \"b\": range(6)} ... ) \u003e\u003e\u003e df a b 0 red 0 1 red 1 2 blue 2 3 blue 3 4 black 4 5 black 5 # 每组里头采样一个数据 \u003e\u003e\u003e df.groupby(\"a\").sample(n=1, random_state=1) a b 4 black 4 2 blue 2 1 red 1 # 按列\"a\"的值分组采样，只保留列\"b\"的值 \u003e\u003e\u003e df.groupby(\"a\")[\"b\"].sample(frac=0.5, random_state=2) 5 5 2 2 0 0 Name: b, dtype: int64 # 为每个样本赋予采样权重 \u003e\u003e\u003e df.groupby(\"a\").sample( ... n=1, ... weights=[1, 1, 1, 0, 0, 1], ... random_state=1, ... ) a b 5 black 5 2 blue 2 0 red 0 ","date":"2022-05-23","objectID":"/pandas/:1:5","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#条件采样"},{"categories":[],"content":" 采样方法 随机采样 # 随机采样100个 \u003e\u003e\u003e df_sampled = df.sample(n=100) # 随机采样50%的样本 \u003e\u003e\u003e df_sampled = df.sample(frac=0.5) 条件采样 # 选苹果的行 \u003e\u003e\u003e df_sampled = df[df['fruit'] == 'apple'] 分组采样 \u003e\u003e\u003e df = pd.DataFrame( ... {\"a\": [\"red\"] * 2 + [\"blue\"] * 2 + [\"black\"] * 2, \"b\": range(6)} ... ) \u003e\u003e\u003e df a b 0 red 0 1 red 1 2 blue 2 3 blue 3 4 black 4 5 black 5 # 每组里头采样一个数据 \u003e\u003e\u003e df.groupby(\"a\").sample(n=1, random_state=1) a b 4 black 4 2 blue 2 1 red 1 # 按列\"a\"的值分组采样，只保留列\"b\"的值 \u003e\u003e\u003e df.groupby(\"a\")[\"b\"].sample(frac=0.5, random_state=2) 5 5 2 2 0 0 Name: b, dtype: int64 # 为每个样本赋予采样权重 \u003e\u003e\u003e df.groupby(\"a\").sample( ... n=1, ... weights=[1, 1, 1, 0, 0, 1], ... random_state=1, ... ) a b 5 black 5 2 blue 2 0 red 0 ","date":"2022-05-23","objectID":"/pandas/:1:5","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#分组采样"},{"categories":[],"content":" 获取索引 按照条件取特定行的索引 pandas 1.5.2 fruit = [\"apple\", \"peach\", \"peach\", \"watermelon\"] \u003e\u003e\u003e df = pd.DataFrame({ ... \"fruit\": fruit ... }) \u003e\u003e\u003e idx = df.index[df[\"fruit\"]==\"peach\"] \u003e\u003e\u003e print(id) Int64Index([1, 2], dtype='int64') \u003e\u003e\u003e print(idx.tolist()) [1, 2] ","date":"2022-05-23","objectID":"/pandas/:1:6","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#获取索引"},{"categories":[],"content":" 索引取值 根据pandas索引类型或list类型的索引取值，可以多个 pandas 1.5.2 fruit = [\"apple\", \"peach\", \"peach\", \"watermelon\"] \u003e\u003e\u003e df = pd.DataFrame({ ... \"fruit\": fruit ... }) \u003e\u003e\u003e idx = df.index[df[\"fruit\"]==\"peach\"] \u003e\u003e\u003e print(df.loc[idx]) fruit 1 peach 2 peach \u003e\u003e\u003e print(df.loc[idx.tolist()]) fruit 1 peach 2 peach ","date":"2022-05-23","objectID":"/pandas/:1:7","series":["python_data_analysis"],"tags":["tools","py"],"title":"Pandas的使用方法","uri":"/pandas/#索引取值"},{"categories":["nlp"],"content":" Task Description link SNLI Stanford language inference task. 斯坦福大学语言推理任务。给定一个前提和一个假设，目标是预测假设与前提的关系是包含的、中性的，还是矛盾的。 MNLI Language inference dataset on multigenre texts.多类型文本的语言推断数据集，包括转录语音、流行小说和政府报告(Wilmams等人，2018)，句子对，一个前提，一个是假设。前提和假设的关系有三种情况：蕴含（entailment），矛盾（contradiction），中立（neutral）。句子对三分类问题。与SNLI数据集相比，不同的写作和口语风格文本更为复杂，包括与训练域匹配的验证数据和与训练域不匹配的验证数据（一个是matched(m)，另一个是mismatched（mm）即训练和验证不在同一个数据集上进行比如语音和小说等）。 Yelp(Yelp Polarity) Document-level sentiment classification on positive and negative reviews. IMDB Document-level sentiment classification dataset on positive and negative movie reviews, where the average sequence length is longer than the Yelp dataset.篇章级别的影评，判断评论为正向或者负向。 Click here to download AG(AG’s News) Setence-level classification with regard to four news topics: World, Sports, Business, and Science/Technology. Click here to download(Kaggle) Fake(Fake News Detection) Document-level classification on whether a news article is fake or not. The datasetcomes from the Kaggle Fake News Challenge Click here to download(Kaggle) ","date":"2022-05-23","objectID":"/nlp_task/:0:0","series":[],"tags":["task"],"title":"自然语言处理相关任务与数据集","uri":"/nlp_task/#"},{"categories":["tutorial"],"content":"Jupyter 已经被人们广泛使用来作为生产力工具，由于其能够步运行和可视化等，带来了良好的教学和交互体验。 ","date":"2022-05-22","objectID":"/jupyter/:0:0","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#"},{"categories":["tutorial"],"content":" Jupyter Lab","date":"2022-05-22","objectID":"/jupyter/:1:0","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#jupyter-lab"},{"categories":["tutorial"],"content":" 安装和配置 安装 pip install jupyterlab 配置 命令行指定 jupyter lab --allow-root # 允许root权限 --no-browser # 启动不打开窗口 --ip '*' # 不指定IP --port '8889' # 指定开放端口 /home/oliver/CONTEST/kaggle_patent_phrase_matching2022 # 指定启动目录 --config='~/.jupyter/jupyter_notebook_config_special.py' # 指定配置文件(以上配置都可以写入此) 设置登录密码 jupyter lab password 生成配置文件，运行后一般在~/.jupyter目录下会生成 jupyter lab --generate-config jupyterlab配置文件设置 c.ServerApp.allow_remote_access = True # 允许远程登录 c.ServerApp.ip = '*' # 不指定ip c.LabApp.open_browser = False c.ServerApp.allow_root = True c.NotebookApp.notebook_dir = '/home/' # 启动目录，这个要用命令行指定才有效 \"\"\" \u003e\u003e\u003e from notebook.auth import passwd \u003e\u003e\u003e passwd() Enter password: Verify password: 'argon2:$argon2id$v=19$m=10240,t=10,p=8$jlFeqL9E3EPEFV2CyPETWQ$lzw9f8pTcjmXCW1uh0K0mzeQqZsGbHeaQSv4/6BJ0ys' \"\"\" c.NotebookApp.password = u'argon2:$argon2id......(换成你的秘钥)' #就是把生成的密码json文件里面的一串密码放这里 给它配一个服务 sudo vi /etc/systemd/system/jupyterlab.service 写入以下内容 [Unit] Description=contest jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/patent_phrase/bin/jupyter-lab --allow-root --no-browser --ip '*' --port '8889' --config='/home/oliver/.jupyter/jupyter_notebook_config_special.py' /home/oliver Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 让这个服务开机启动 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ","date":"2022-05-22","objectID":"/jupyter/:1:1","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#安装和配置"},{"categories":["tutorial"],"content":" 安装和配置 安装 pip install jupyterlab 配置 命令行指定 jupyter lab --allow-root # 允许root权限 --no-browser # 启动不打开窗口 --ip '*' # 不指定IP --port '8889' # 指定开放端口 /home/oliver/CONTEST/kaggle_patent_phrase_matching2022 # 指定启动目录 --config='~/.jupyter/jupyter_notebook_config_special.py' # 指定配置文件(以上配置都可以写入此) 设置登录密码 jupyter lab password 生成配置文件，运行后一般在~/.jupyter目录下会生成 jupyter lab --generate-config jupyterlab配置文件设置 c.ServerApp.allow_remote_access = True # 允许远程登录 c.ServerApp.ip = '*' # 不指定ip c.LabApp.open_browser = False c.ServerApp.allow_root = True c.NotebookApp.notebook_dir = '/home/' # 启动目录，这个要用命令行指定才有效 \"\"\" \u003e\u003e\u003e from notebook.auth import passwd \u003e\u003e\u003e passwd() Enter password: Verify password: 'argon2:$argon2id$v=19$m=10240,t=10,p=8$jlFeqL9E3EPEFV2CyPETWQ$lzw9f8pTcjmXCW1uh0K0mzeQqZsGbHeaQSv4/6BJ0ys' \"\"\" c.NotebookApp.password = u'argon2:$argon2id......(换成你的秘钥)' #就是把生成的密码json文件里面的一串密码放这里 给它配一个服务 sudo vi /etc/systemd/system/jupyterlab.service 写入以下内容 [Unit] Description=contest jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/patent_phrase/bin/jupyter-lab --allow-root --no-browser --ip '*' --port '8889' --config='/home/oliver/.jupyter/jupyter_notebook_config_special.py' /home/oliver Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 让这个服务开机启动 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ","date":"2022-05-22","objectID":"/jupyter/:1:1","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#安装"},{"categories":["tutorial"],"content":" 安装和配置 安装 pip install jupyterlab 配置 命令行指定 jupyter lab --allow-root # 允许root权限 --no-browser # 启动不打开窗口 --ip '*' # 不指定IP --port '8889' # 指定开放端口 /home/oliver/CONTEST/kaggle_patent_phrase_matching2022 # 指定启动目录 --config='~/.jupyter/jupyter_notebook_config_special.py' # 指定配置文件(以上配置都可以写入此) 设置登录密码 jupyter lab password 生成配置文件，运行后一般在~/.jupyter目录下会生成 jupyter lab --generate-config jupyterlab配置文件设置 c.ServerApp.allow_remote_access = True # 允许远程登录 c.ServerApp.ip = '*' # 不指定ip c.LabApp.open_browser = False c.ServerApp.allow_root = True c.NotebookApp.notebook_dir = '/home/' # 启动目录，这个要用命令行指定才有效 \"\"\" \u003e\u003e\u003e from notebook.auth import passwd \u003e\u003e\u003e passwd() Enter password: Verify password: 'argon2:$argon2id$v=19$m=10240,t=10,p=8$jlFeqL9E3EPEFV2CyPETWQ$lzw9f8pTcjmXCW1uh0K0mzeQqZsGbHeaQSv4/6BJ0ys' \"\"\" c.NotebookApp.password = u'argon2:$argon2id......(换成你的秘钥)' #就是把生成的密码json文件里面的一串密码放这里 给它配一个服务 sudo vi /etc/systemd/system/jupyterlab.service 写入以下内容 [Unit] Description=contest jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/patent_phrase/bin/jupyter-lab --allow-root --no-browser --ip '*' --port '8889' --config='/home/oliver/.jupyter/jupyter_notebook_config_special.py' /home/oliver Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 让这个服务开机启动 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ","date":"2022-05-22","objectID":"/jupyter/:1:1","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#配置"},{"categories":["tutorial"],"content":" 设置代理 方法一（便捷指数1）不难知道，直接在jupyter的开头写上这句有效。 import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" import requests requests.get(\"http://google.com\") \u003cResponse [200]\u003e ok，正常访问。 方法二（便捷指数2）此方法设置完成后需要重新启动jupyter程序。 将这几行代码写入jupyter对应的配置文件。 vi ~/.jupyter/jupyter_lab_config.py import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" 方法三（便捷指数3）来源 找了很多实现方法，以下方法有效。 因为jupyter是基于ipython模式的，所以在ipython的启动文件中创建如下文件，前缀越小代表优先级越高。 vi ~/.ipython/profile_default/startup/00-first.py 向其中写入 import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" 然后重新运行notebook，注意要把kernel关掉重开，否则不会执行此文件。 然后每次启动notebook之前就会运行这个文件。 ","date":"2022-05-22","objectID":"/jupyter/:1:2","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#设置代理"},{"categories":["tutorial"],"content":" 设置代理 方法一（便捷指数1）不难知道，直接在jupyter的开头写上这句有效。 import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" import requests requests.get(\"http://google.com\") ok，正常访问。 方法二（便捷指数2）此方法设置完成后需要重新启动jupyter程序。 将这几行代码写入jupyter对应的配置文件。 vi ~/.jupyter/jupyter_lab_config.py import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" 方法三（便捷指数3）来源 找了很多实现方法，以下方法有效。 因为jupyter是基于ipython模式的，所以在ipython的启动文件中创建如下文件，前缀越小代表优先级越高。 vi ~/.ipython/profile_default/startup/00-first.py 向其中写入 import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" 然后重新运行notebook，注意要把kernel关掉重开，否则不会执行此文件。 然后每次启动notebook之前就会运行这个文件。 ","date":"2022-05-22","objectID":"/jupyter/:1:2","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#方法一便捷指数1"},{"categories":["tutorial"],"content":" 设置代理 方法一（便捷指数1）不难知道，直接在jupyter的开头写上这句有效。 import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" import requests requests.get(\"http://google.com\") ok，正常访问。 方法二（便捷指数2）此方法设置完成后需要重新启动jupyter程序。 将这几行代码写入jupyter对应的配置文件。 vi ~/.jupyter/jupyter_lab_config.py import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" 方法三（便捷指数3）来源 找了很多实现方法，以下方法有效。 因为jupyter是基于ipython模式的，所以在ipython的启动文件中创建如下文件，前缀越小代表优先级越高。 vi ~/.ipython/profile_default/startup/00-first.py 向其中写入 import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" 然后重新运行notebook，注意要把kernel关掉重开，否则不会执行此文件。 然后每次启动notebook之前就会运行这个文件。 ","date":"2022-05-22","objectID":"/jupyter/:1:2","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#方法二便捷指数2"},{"categories":["tutorial"],"content":" 设置代理 方法一（便捷指数1）不难知道，直接在jupyter的开头写上这句有效。 import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" import requests requests.get(\"http://google.com\") ok，正常访问。 方法二（便捷指数2）此方法设置完成后需要重新启动jupyter程序。 将这几行代码写入jupyter对应的配置文件。 vi ~/.jupyter/jupyter_lab_config.py import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" 方法三（便捷指数3）来源 找了很多实现方法，以下方法有效。 因为jupyter是基于ipython模式的，所以在ipython的启动文件中创建如下文件，前缀越小代表优先级越高。 vi ~/.ipython/profile_default/startup/00-first.py 向其中写入 import os os.environ['HTTP_PROXY']=\"http://127.0.0.1:7890\" os.environ['HTTPS_PROXY']=\"http://127.0.0.1:7890\" 然后重新运行notebook，注意要把kernel关掉重开，否则不会执行此文件。 然后每次启动notebook之前就会运行这个文件。 ","date":"2022-05-22","objectID":"/jupyter/:1:2","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#方法三便捷指数3"},{"categories":["tutorial"],"content":" 将ipynb文件输出为其他格式文件 Html jupyter nbconvert --to html notebook.ipynb 只展示markdown，适用于生成数据分析报告 jupyter nbconvert --to html --no-input data_analy_4.ipynb Markdown jupyter nbconvert --to markdown notebook.ipynb PDF jupyter nbconvert --to pdf notebook.ipynb ","date":"2022-05-22","objectID":"/jupyter/:1:3","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#将ipynb文件输出为其他格式文件"},{"categories":["tutorial"],"content":" 将ipynb文件输出为其他格式文件 Html jupyter nbconvert --to html notebook.ipynb 只展示markdown，适用于生成数据分析报告 jupyter nbconvert --to html --no-input data_analy_4.ipynb Markdown jupyter nbconvert --to markdown notebook.ipynb PDF jupyter nbconvert --to pdf notebook.ipynb ","date":"2022-05-22","objectID":"/jupyter/:1:3","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#html"},{"categories":["tutorial"],"content":" 将ipynb文件输出为其他格式文件 Html jupyter nbconvert --to html notebook.ipynb 只展示markdown，适用于生成数据分析报告 jupyter nbconvert --to html --no-input data_analy_4.ipynb Markdown jupyter nbconvert --to markdown notebook.ipynb PDF jupyter nbconvert --to pdf notebook.ipynb ","date":"2022-05-22","objectID":"/jupyter/:1:3","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#markdown"},{"categories":["tutorial"],"content":" 将ipynb文件输出为其他格式文件 Html jupyter nbconvert --to html notebook.ipynb 只展示markdown，适用于生成数据分析报告 jupyter nbconvert --to html --no-input data_analy_4.ipynb Markdown jupyter nbconvert --to markdown notebook.ipynb PDF jupyter nbconvert --to pdf notebook.ipynb ","date":"2022-05-22","objectID":"/jupyter/:1:3","series":["jupyter"],"tags":["py","tools"],"title":"Jupyter使用教程","uri":"/jupyter/#pdf"},{"categories":["technology_log"],"content":" 说明通常，我们在项目中，需要把代码文件作为一个包来用，比如代码文件是code文件夹，如果使用Pycharm等IDE，只需要在里面加入一个空的文件来说明这是一个包文件，就能在与code文件夹的同级目录下import code了，然而， 如果直接在系统环境下，却常常会报找不到这个包，解决方法如下 ","date":"2022-05-20","objectID":"/py_project_env/:1:0","series":[],"tags":["py","linux"],"title":"Python项目包引用与环境设置","uri":"/py_project_env/#说明"},{"categories":["technology_log"],"content":" 项目入口是py文件在py文件顶端加入如下代码 import sys project_path = '/home/mw/project' # 这里设置自己的项目路径 sys.path.append(project_path) from code import * import code 如果报错：ModuleNotFoundError: No module named code code is not a package，在考虑有没有进入code同级目录下的同时，是否系统环境中含有code名字冲突等。 验证是否存在code包思路 import code print(code.__file__) 如果存在，有两种思路 1.更换自己的代码文件夹code为mycode等名字来解决。 import sys project_path = '/home/mw/project' # 这里设置自己的项目路径 sys.path.append(project_path) from mycode import * import mycode 2.指定上级目录进行导包 比如目录树是这样子 import sys project_path = '/home/GAIIC2022-64_1700' # 这里设置自己的项目路径 sys.path.append(project_path) from project.code import xxx ","date":"2022-05-20","objectID":"/py_project_env/:2:0","series":[],"tags":["py","linux"],"title":"Python项目包引用与环境设置","uri":"/py_project_env/#项目入口是py文件"},{"categories":["technology_log"],"content":" 项目入口是sh文件在sh文件顶部加上如下代码，将当前项目目录加入PYTHON环境变量。 # 导入当前项目作为环境变量 project_path=\"$(pwd)\" export PYTHONPATH=$project_path:$PYTHONPATH echo $PYTHONPATH source $HOME/.bashrc ","date":"2022-05-20","objectID":"/py_project_env/:3:0","series":[],"tags":["py","linux"],"title":"Python项目包引用与环境设置","uri":"/py_project_env/#项目入口是sh文件"},{"categories":["technology_log"],"content":"今天是2022年5月19日，忙碌了一上午的小弟激动地开始train了，结果，本来五分钟一轮的乖儿子， 变成了两个多小时，What？！！儿子感冒了吗？ 查看儿子占用情况。 nvidia-smi Failed to initialize NVML: Driver/library version mismatch What？！！看看儿子的内核版本 cat /proc/driver/nvidia/version NVRM version: NVIDIA UNIX x86_64 Kernel Module 510.60.02 Wed Mar 16 11:24:05 UTC 2022 GCC version: gcc version 11.2.0 (Ubuntu 11.2.0-7ubuntu2) 看看儿子翻滚日记 cat /var/log/dpkg.log | grep nvidia 2022-05-19 06:04:29 upgrade nvidia-driver-510:amd64 510.60.02-0ubuntu0.21.10.2 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:29 status half-configured nvidia-driver-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:29 status unpacked nvidia-driver-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:29 status half-installed nvidia-driver-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:29 status unpacked nvidia-driver-510:amd64 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:29 upgrade libnvidia-gl-510:amd64 510.60.02-0ubuntu0.21.10.2 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:29 status half-configured libnvidia-gl-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:29 status unpacked libnvidia-gl-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:29 status half-configured libnvidia-gl-510:i386 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:29 status half-installed libnvidia-gl-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:30 status unpacked libnvidia-gl-510:amd64 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:30 upgrade libnvidia-gl-510:i386 510.60.02-0ubuntu0.21.10.2 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:30 status unpacked libnvidia-gl-510:i386 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:30 status half-installed libnvidia-gl-510:i386 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:31 status unpacked libnvidia-gl-510:i386 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:31 upgrade nvidia-dkms-510:amd64 510.60.02-0ubuntu0.21.10.2 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:31 status half-configured nvidia-dkms-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status unpacked nvidia-dkms-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status half-installed nvidia-dkms-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status unpacked nvidia-dkms-510:amd64 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:41 upgrade nvidia-kernel-source-510:amd64 510.60.02-0ubuntu0.21.10.2 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:41 status half-configured nvidia-kernel-source-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status unpacked nvidia-kernel-source-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status half-installed nvidia-kernel-source-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status unpacked nvidia-kernel-source-510:amd64 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:41 upgrade nvidia-kernel-common-510:amd64 510.60.02-0ubuntu0.21.10.2 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:41 status half-configured nvidia-kernel-common-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status unpacked nvidia-kernel-common-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status half-installed nvidia-kernel-common-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status unpacked nvidia-kernel-common-510:amd64 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:41 upgrade libnvidia-decode-510:i386 510.60.02-0ubuntu0.21.10.2 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:41 status half-configured libnvidia-decode-510:i386 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status unpacked libnvidia-decode-510:i386 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status half-configured libnvidia-decode-510:amd64 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status half-installed libnvidia-decode-510:i386 510.60.02-0ubuntu0.21.10.2 2022-05-19 06:04:41 status unpacked libnvidia-decode-510:i386 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:41 upgrade libnvidia-decode-510:amd64 510.60.02-0ubuntu0.21.10.2 510.73.05-0ubuntu0.21.10.1 2022-05-19 06:04:41 status unpacked libnvidia-decode-510:amd64 510.60.02-0ubun","date":"2022-05-19","objectID":"/production_catastrophe/:0:0","series":[],"tags":["linux"],"title":"记一次生产事故","uri":"/production_catastrophe/#"},{"categories":["machine_learning"],"content":" 训练优化策略","date":"2022-05-18","objectID":"/deep_learning_optimize/:1:0","series":[],"tags":[],"title":"深度学习优化策略","uri":"/deep_learning_optimize/#训练优化策略"},{"categories":["machine_learning"],"content":" 梯度裁剪梯度裁剪策略是解决模型训练过程中梯度爆炸策略的一个有效方法，为什么会导致梯度爆炸呢？我们知道，如果模型如下 $$ f(x)=wx+b $$ 省略b的情况下，梯度$\\Delta = x$,当$x$值过大情况下，则可能会发生梯度爆炸现象，这时，我们可以对梯度进行裁剪，使之规范在指定范围内，如此一来可避免类似现象。 ","date":"2022-05-18","objectID":"/deep_learning_optimize/:1:1","series":[],"tags":[],"title":"深度学习优化策略","uri":"/deep_learning_optimize/#梯度裁剪"},{"categories":[],"content":" *args 和 **kwargs传参方法","date":"2022-05-18","objectID":"/python_using/:1:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#args-和-kwargs传参方法"},{"categories":[],"content":" *args元组传参 def add(*args): print(args, type(args)) add(2, 3) (2, 3) \u003cclass 'tuple'\u003e def add(*numbers): total = 0 for num in numbers: total += num return total print(add(2, 3)) print(add(2, 3, 5)) print(add(2, 3, 5, 7)) print(add(2, 3, 5, 7, 9)) 5 10 17 26 ","date":"2022-05-18","objectID":"/python_using/:1:1","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#args"},{"categories":[],"content":" **kwargs字典传参 def total_fruits(**kwargs): print(kwargs, type(kwargs)) total_fruits(banana=5, mango=7, apple=8) {'banana': 5, 'mango': 7, 'apple': 8} \u003cclass 'dict'\u003e def total_fruits(**fruits): total = 0 for amount in fruits.values(): total += amount return total print(total_fruits(banana=5, mango=7, apple=8)) print(total_fruits(banana=5, mango=7, apple=8, oranges=10)) print(total_fruits(banana=5, mango=7)) 20 30 12 ","date":"2022-05-18","objectID":"/python_using/:1:2","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#kwargs"},{"categories":[],"content":" yeild返回方法和return返回不同，yeild返回的是一个可迭代对象，可以通过next(yeild返回object)来取下一个元素 def test_yield(): for i in range(10): yield i print(type(test_yield())) _iter = test_yield() print(next(_iter)) print(next(_iter)) print(next(_iter)) \u003cclass 'generator'\u003e 0 1 2 def test_yield(): for i in [3, 5, 6, 1, 3, 516, 6321, 1]: yield i _iter = test_yield() num = 0 try: while True: num += 1 print(next(_iter)) except Exception as ex: pass print(\"列表的元素个数%d\" % num) 3 5 6 1 3 516 6321 1 列表的元素个数9 ==问，为什么要这么干，直接len()不就好了吗？答，除了获取长度，还可以进行其他操作，而不用先获取列表长度进行。== ","date":"2022-05-18","objectID":"/python_using/:2:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#yeild返回方法"},{"categories":[],"content":" 数据类型转换","date":"2022-05-18","objectID":"/python_using/:3:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#数据类型转换"},{"categories":[],"content":" 列表内转换数据类型 \u003e\u003e\u003e # 返回一个迭代器 \u003e\u003e\u003e var = map(str, [3, 1, 2]) \u003e\u003e\u003e print(list(var)) \u003e\u003e\u003e print(list(var)) # 迭代器走完就没有值了 \u003e\u003e\u003e begin = time.time() # 记录转换耗时 \u003e\u003e\u003e # 返回一个列表(推荐使用，速度更快，因未出现显式循环) \u003e\u003e\u003e var_s = list(map(str, [3, 1, 2])) \u003e\u003e\u003e print(var_s, \"转换耗时: \", time.time()-begin) \u003e\u003e\u003e begin = time.time() # 记录转换耗时 \u003e\u003e\u003e # 返回一个列表 \u003e\u003e\u003e var_s = [str(i) for i in [3, 1, 2]] \u003e\u003e\u003e print(var_s, \"转换耗时: \", time.time()-begin) ['3', '1', '2'] [] ['3', '1', '2'] 转换耗时: 1.430511474609375e-06 ['3', '1', '2'] 转换耗时: 1.6689300537109375e-06 ","date":"2022-05-18","objectID":"/python_using/:3:1","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#列表内转换数据类型"},{"categories":[],"content":" 谨慎的赋值python的执行中，等号右边先执行，如下代码中，python先创建出数组然后再让a指向该数组，如果简单用等号将a 赋值给b，则b也指向该数组，修改b的同时，原来的a也会随之改变，如果不想要a 发生改变，需使用b = a.copy() \u003e\u003e\u003e # 第一种赋值方式 \u003e\u003e\u003e a = [3, 1, 2] \u003e\u003e\u003e b = a \u003e\u003e\u003e b[0] = 0 \u003e\u003e\u003e print(a, b) \u003e\u003e\u003e # 第二种赋值方式 \u003e\u003e\u003e a = [3, 1, 2] \u003e\u003e\u003e b = a.copy() # 复制法 \u003e\u003e\u003e b[0] = 0 \u003e\u003e\u003e print(a, b) out: [0, 1, 2] [0, 1, 2] [3, 1, 2] [0, 1, 2] ","date":"2022-05-18","objectID":"/python_using/:4:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#谨慎的赋值"},{"categories":[],"content":" 漂亮的表格单纯输出列表效果不好看，可以做成表格形式。 \u003e\u003e\u003e # 列表显示为表格 \u003e\u003e\u003e from prettytable import PrettyTable \u003e\u003e\u003e fruit = [['apple', 'red', '100g'], ['banana', 'yellow', '80g']] \u003e\u003e\u003e print(\"直接打印列表\\n\", fruit) \u003e\u003e\u003e x = PrettyTable() # 创建表格实例 \u003e\u003e\u003e x.field_names = ['fruit', 'color', 'weight'] # 定义表头 \u003e\u003e\u003e x.add_rows(fruit) \u003e\u003e\u003e print(\"打印成表格\\n\", x) out: 直接打印列表 [['apple', 'red', '100g'], ['banana', 'yellow', '80g']] 打印成表格 +--------+--------+--------+ | fruit | color | weight | +--------+--------+--------+ | apple | red | 100g | | banana | yellow | 80g | +--------+--------+--------+ ","date":"2022-05-18","objectID":"/python_using/:5:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#漂亮的表格"},{"categories":[],"content":" zip的妙用 \u003e\u003e\u003e a = [[1, 2], [3, 4], [5, 6]] \u003e\u003e\u003e print(*a) \u003e\u003e\u003e print(zip(*a)) \u003e\u003e\u003e print(list(zip(*a))) out: [1, 2] [3, 4] [5, 6] \u003czip object at 0x7fb6cd7e7f00\u003e [(1, 3, 5), (2, 4, 6)] ","date":"2022-05-18","objectID":"/python_using/:6:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#zip的妙用"},{"categories":[],"content":" 内置filter快速筛选出列表中的一类值 \u003e\u003e\u003e a = [3, 2, None, \"\", 0] \u003e\u003e\u003e # 剔除假值（python中,None、0、\"\" 都为假”） \u003e\u003e\u003e print(list(filter(bool, a))) \u003e\u003e\u003e # 剔除 3 \u003e\u003e\u003e print(list(filter(lambda i: i != 3, a))) # 筛选出奇数 b = [1, 2, 3, 4, 5] print(list(filter(lambda i: i % 2, b))) out: [3, 2] [2, None, '', 0] [1, 3, 5] ","date":"2022-05-18","objectID":"/python_using/:7:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#内置filter快速筛选出列表中的一类值"},{"categories":[],"content":" 查看变量占用的内存空间 \u003e\u003e\u003e import sys \u003e\u003e\u003e a = [3, 2, None, \"\", 0] \u003e\u003e\u003e b = 9 \u003e\u003e\u003e c = \"python\" \u003e\u003e\u003e print(\"变量a占用的内存空间为{}字节，\\n变量b占用的内存空间为{}字节，\\n变量c占用内存空间为{}字 节。\".format( \u003e\u003e\u003e sys.getsizeof(a), \u003e\u003e\u003e sys.getsizeof(b), \u003e\u003e\u003e sys.getsizeof(c), \u003e\u003e\u003e )) out: 变量a占用的内存空间为112字节， 变量b占用的内存空间为28字节， 变量c占用内存空间为55字节。 ","date":"2022-05-18","objectID":"/python_using/:8:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#查看变量占用的内存空间"},{"categories":[],"content":" 严格控制打印小数点后两位 \u003e\u003e\u003e print(f\"{5/2:.2f}\") out: 2.50 ","date":"2022-05-18","objectID":"/python_using/:9:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#严格控制打印小数点后两位"},{"categories":[],"content":" 效率的注重法则","date":"2022-05-18","objectID":"/python_using/:10:0","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#效率的注重法则"},{"categories":[],"content":" 法则一：尽量少写显式循环 import time s = time.time() np.sum(distance) print(f\"np求和耗时 {time.time() - s}\") sum = 0 total = distance.shape[0] s = time.time() for i in range(total): sum += distance[i] print(f\"循环求和耗时 {time.time() - s}\") np求和耗时 0.00003552436828613281e-05 循环求和耗时 0.0009284019470214844 法则二：排序算法最好上GPU distance = np.random.random((7532, 11314)) import time s = time.time() sort_idx = np.argsort(distance, axis=-1) # 独立将每一行元素从小到达排列（即相似性从大到小）的下标 (7532, 11314) print(f\"np排序耗时{time.time() - s}\") s = time.time() sort_idx = np.array(torch.argsort(torch.tensor(distance).cuda(), dim=-1).detach().cpu().numpy()) print(f\"tensor排序耗时{time.time() - s}\") np排序耗时3.7687795162200928 tensor排序耗时3.2619576454162598 ","date":"2022-05-18","objectID":"/python_using/:10:1","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#法则一尽量少写显式循环"},{"categories":[],"content":" 法则一：尽量少写显式循环 import time s = time.time() np.sum(distance) print(f\"np求和耗时 {time.time() - s}\") sum = 0 total = distance.shape[0] s = time.time() for i in range(total): sum += distance[i] print(f\"循环求和耗时 {time.time() - s}\") np求和耗时 0.00003552436828613281e-05 循环求和耗时 0.0009284019470214844 法则二：排序算法最好上GPU distance = np.random.random((7532, 11314)) import time s = time.time() sort_idx = np.argsort(distance, axis=-1) # 独立将每一行元素从小到达排列（即相似性从大到小）的下标 (7532, 11314) print(f\"np排序耗时{time.time() - s}\") s = time.time() sort_idx = np.array(torch.argsort(torch.tensor(distance).cuda(), dim=-1).detach().cpu().numpy()) print(f\"tensor排序耗时{time.time() - s}\") np排序耗时3.7687795162200928 tensor排序耗时3.2619576454162598 ","date":"2022-05-18","objectID":"/python_using/:10:1","series":[],"tags":["py"],"title":"Python中一些语法的使用","uri":"/python_using/#法则二排序算法最好上gpu"},{"categories":[],"content":" re用法示例 # 过多换行替换为'￥￥￥中English$$$' sub_text = re.sub(r'\\n{2,}','￥￥￥中English$$$',sub_text) # 找到 chapter_con 中的section_pattern_extra模式串 _section_title = re.findall(section_pattern_extra,chapter_con) ","date":"2022-05-13","objectID":"/regular_expression/:1:0","series":[],"tags":[],"title":"正则表达式","uri":"/regular_expression/#re用法示例"},{"categories":[],"content":" 通过开头结尾匹配 # (?=〈更).*?(?=\\\\) 匹配以“〈更”开头，“\\”结尾的字符串，包含开头，不包含结尾 # (?\u003c=〈更).*?(?=\\\\) 匹配以“〈更”开头，“\\”结尾的字符串，不包含开头，不包含结尾 chapter_pattern_extra = r'\\n第[\\u4e00-\\u9fa5]{1,2}章.*\\n' #前后换行，在第 和 章之间有一到两个中文字符 ","date":"2022-05-13","objectID":"/regular_expression/:2:0","series":[],"tags":[],"title":"正则表达式","uri":"/regular_expression/#通过开头结尾匹配"},{"categories":["machine_learning"],"content":" 经典损失函数","date":"2022-04-29","objectID":"/loss_function/:1:0","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#经典损失函数"},{"categories":["machine_learning"],"content":" 距离度量 F 范数（Frobenius 范数）矩阵所有元素的平方和再开方，他是是向量二范式的拓展类比。 $$ ||A||_F = \\sqrt{\\sum a_{ij}^2} $$ 如何作为距离度量呢？ 可以利用如下方式，如果标签矩阵为$B$（通常为一个batch里的多个样本标签向量组成，如标签向量为4维，batch_size 为128）则$B$的形状为（128，4）），距离度量公式为 $$ \\mathcal L_F = ||A-B|| $$ 核范数（nuclear norm）作为距离度量方式不详 $$ \\begin{aligned} ||A||_*=\\operatorname{tr}\\left(\\sqrt{A^{T} A}\\right) \u0026=\\operatorname{tr}\\left(\\sqrt{\\left(U \\Sigma V^{T}\\right)^{T} U \\Sigma V^{T}}\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V \\Sigma^{T} U^{T} U \\Sigma V^{T}}\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V \\Sigma^{2} V^{T}}\\right)\\left(\\Sigma^{T}=\\Sigma\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V^{T} V \\Sigma^{2}}\\right) \\ \u0026=\\operatorname{tr}(\\Sigma) \\end{aligned} $$ 参考代码 import torch label = np.array([[1, 2, 3, 4], [1, 2, 3, 4]]) pred = np.array([[2, 3, 4, 5], [2, 3, 4, 5]]) print(\"F范数矩阵\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"fro\", dim=-1)) print(\"2范数矩阵\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=2, dim=-1)) print(\"核范数矩阵\") # 核范数作为Loss使用方法不详 print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"nuc\")) print(\"平均损失计算：每个样本的损失相加，再除以总样本数\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"fro\", dim=-1).sum() / label.shape[0]) print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=2, dim=-1).sum() / label.shape[0]) 输出： F范数矩阵 tensor([2., 2.], device='cuda:0') 2范数矩阵 tensor([2., 2.], device='cuda:0') 核范数矩阵 tensor(2.8284, device='cuda:0') 平均损失计算：每个样本的损失相加，再除以总样本数 tensor(2., device='cuda:0') tensor(2., device='cuda:0') Process finished with exit code 0 \u003e\u003e\u003e # 不同shape的欧距计算 作者：https://blog.csdn.net/sinat_24899403/article/details/119249822 \u003e\u003e\u003e import torch \u003e\u003e\u003e from torch import nn \u003e\u003e\u003e a=torch.tensor([[1,1,1], [2,2,2]]) \u003e\u003e\u003e b=torch.tensor([[2,2,2], [1,1,1], [2,2,2], [1,1,1], [2,2,2]]) \u003e\u003e\u003e def pdist(a: torch.Tensor, b: torch.Tensor, p: int = 2) -\u003e torch.Tensor: return (a-b).abs().pow(p).sum(-1).pow(1/p) \u003e\u003e\u003e a_=a.unsqueeze(1) \u003e\u003e\u003e b_=b.unsqueeze(0) \u003e\u003e\u003e print(pdist(a_,b_)) tensor([[1.7321, 0.0000, 1.7321, 0.0000, 1.7321], [0.0000, 1.7321, 0.0000, 1.7321, 0.0000]]) 汉明距离 如果是多对多，且shape不一致，使用numpy计算是最快的，考虑原因可能是torch未封装int类型的不同shape数据的计算方式，如果shape相同可以使用pairwise的方法 形状相同（用于标签预测） \u003e\u003e\u003e b = torch.tensor([[0, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e c = torch.tensor([[1, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e metric = BinaryHammingDistance(multidim_average='samplewise') \u003e\u003e\u003e print(metric(b, c)) tensor([0.3333, 0.0000]) 形状不同（用于检索） \u003e\u003e\u003e from sklearn.neighbors import DistanceMetric \u003e\u003e\u003e import time \u003e\u003e\u003e a = np.array([[0, 0, 2],]) \u003e\u003e\u003e b = np.array([[0, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e s1 = time.time() \u003e\u003e\u003e print(DistanceMetric.get_metric(\"hamming\").pairwise(a,b)) \u003e\u003e\u003e print(f\"耗时{time.time()-s1}\") [[0.33333333 0.66666667]] 耗时0.0003008842468261719 \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float) \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float) \u003e\u003e\u003e s2 = time.time() \u003e\u003e\u003e print(torch.cdist(a_tensor, b_tensor, p=0)) \u003e\u003e\u003e print(f\"耗时{time.time()-s2}\") tensor([[1., 2.]]) 耗时0.0003383159637451172 \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float).cuda() \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float).cuda() \u003e\u003e\u003e s3 = time.time() \u003e\u003e\u003e print(torch.cdist(a_tensor, b_tensor, p=0)) \u003e\u003e\u003e print(f\"耗时{time.time()-s3}\") tensor([[1., 2.]], device='cuda:0') 耗时0.0015494823455810547 ","date":"2022-04-29","objectID":"/loss_function/:1:1","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#距离度量"},{"categories":["machine_learning"],"content":" 距离度量 F 范数（Frobenius 范数）矩阵所有元素的平方和再开方，他是是向量二范式的拓展类比。 $$ ||A||_F = \\sqrt{\\sum a_{ij}^2} $$ 如何作为距离度量呢？ 可以利用如下方式，如果标签矩阵为$B$（通常为一个batch里的多个样本标签向量组成，如标签向量为4维，batch_size 为128）则$B$的形状为（128，4）），距离度量公式为 $$ \\mathcal L_F = ||A-B|| $$ 核范数（nuclear norm）作为距离度量方式不详 $$ \\begin{aligned} ||A||_*=\\operatorname{tr}\\left(\\sqrt{A^{T} A}\\right) \u0026=\\operatorname{tr}\\left(\\sqrt{\\left(U \\Sigma V^{T}\\right)^{T} U \\Sigma V^{T}}\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V \\Sigma^{T} U^{T} U \\Sigma V^{T}}\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V \\Sigma^{2} V^{T}}\\right)\\left(\\Sigma^{T}=\\Sigma\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V^{T} V \\Sigma^{2}}\\right) \\ \u0026=\\operatorname{tr}(\\Sigma) \\end{aligned} $$ 参考代码 import torch label = np.array([[1, 2, 3, 4], [1, 2, 3, 4]]) pred = np.array([[2, 3, 4, 5], [2, 3, 4, 5]]) print(\"F范数矩阵\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"fro\", dim=-1)) print(\"2范数矩阵\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=2, dim=-1)) print(\"核范数矩阵\") # 核范数作为Loss使用方法不详 print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"nuc\")) print(\"平均损失计算：每个样本的损失相加，再除以总样本数\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"fro\", dim=-1).sum() / label.shape[0]) print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=2, dim=-1).sum() / label.shape[0]) 输出： F范数矩阵 tensor([2., 2.], device='cuda:0') 2范数矩阵 tensor([2., 2.], device='cuda:0') 核范数矩阵 tensor(2.8284, device='cuda:0') 平均损失计算：每个样本的损失相加，再除以总样本数 tensor(2., device='cuda:0') tensor(2., device='cuda:0') Process finished with exit code 0 \u003e\u003e\u003e # 不同shape的欧距计算 作者：https://blog.csdn.net/sinat_24899403/article/details/119249822 \u003e\u003e\u003e import torch \u003e\u003e\u003e from torch import nn \u003e\u003e\u003e a=torch.tensor([[1,1,1], [2,2,2]]) \u003e\u003e\u003e b=torch.tensor([[2,2,2], [1,1,1], [2,2,2], [1,1,1], [2,2,2]]) \u003e\u003e\u003e def pdist(a: torch.Tensor, b: torch.Tensor, p: int = 2) -\u003e torch.Tensor: return (a-b).abs().pow(p).sum(-1).pow(1/p) \u003e\u003e\u003e a_=a.unsqueeze(1) \u003e\u003e\u003e b_=b.unsqueeze(0) \u003e\u003e\u003e print(pdist(a_,b_)) tensor([[1.7321, 0.0000, 1.7321, 0.0000, 1.7321], [0.0000, 1.7321, 0.0000, 1.7321, 0.0000]]) 汉明距离 如果是多对多，且shape不一致，使用numpy计算是最快的，考虑原因可能是torch未封装int类型的不同shape数据的计算方式，如果shape相同可以使用pairwise的方法 形状相同（用于标签预测） \u003e\u003e\u003e b = torch.tensor([[0, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e c = torch.tensor([[1, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e metric = BinaryHammingDistance(multidim_average='samplewise') \u003e\u003e\u003e print(metric(b, c)) tensor([0.3333, 0.0000]) 形状不同（用于检索） \u003e\u003e\u003e from sklearn.neighbors import DistanceMetric \u003e\u003e\u003e import time \u003e\u003e\u003e a = np.array([[0, 0, 2],]) \u003e\u003e\u003e b = np.array([[0, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e s1 = time.time() \u003e\u003e\u003e print(DistanceMetric.get_metric(\"hamming\").pairwise(a,b)) \u003e\u003e\u003e print(f\"耗时{time.time()-s1}\") [[0.33333333 0.66666667]] 耗时0.0003008842468261719 \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float) \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float) \u003e\u003e\u003e s2 = time.time() \u003e\u003e\u003e print(torch.cdist(a_tensor, b_tensor, p=0)) \u003e\u003e\u003e print(f\"耗时{time.time()-s2}\") tensor([[1., 2.]]) 耗时0.0003383159637451172 \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float).cuda() \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float).cuda() \u003e\u003e\u003e s3 = time.time() \u003e\u003e\u003e print(torch.cdist(a_tensor, b_tensor, p=0)) \u003e\u003e\u003e print(f\"耗时{time.time()-s3}\") tensor([[1., 2.]], device='cuda:0') 耗时0.0015494823455810547 ","date":"2022-04-29","objectID":"/loss_function/:1:1","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#f-范数frobenius-范数"},{"categories":["machine_learning"],"content":" 距离度量 F 范数（Frobenius 范数）矩阵所有元素的平方和再开方，他是是向量二范式的拓展类比。 $$ ||A||_F = \\sqrt{\\sum a_{ij}^2} $$ 如何作为距离度量呢？ 可以利用如下方式，如果标签矩阵为$B$（通常为一个batch里的多个样本标签向量组成，如标签向量为4维，batch_size 为128）则$B$的形状为（128，4）），距离度量公式为 $$ \\mathcal L_F = ||A-B|| $$ 核范数（nuclear norm）作为距离度量方式不详 $$ \\begin{aligned} ||A||_*=\\operatorname{tr}\\left(\\sqrt{A^{T} A}\\right) \u0026=\\operatorname{tr}\\left(\\sqrt{\\left(U \\Sigma V^{T}\\right)^{T} U \\Sigma V^{T}}\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V \\Sigma^{T} U^{T} U \\Sigma V^{T}}\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V \\Sigma^{2} V^{T}}\\right)\\left(\\Sigma^{T}=\\Sigma\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V^{T} V \\Sigma^{2}}\\right) \\ \u0026=\\operatorname{tr}(\\Sigma) \\end{aligned} $$ 参考代码 import torch label = np.array([[1, 2, 3, 4], [1, 2, 3, 4]]) pred = np.array([[2, 3, 4, 5], [2, 3, 4, 5]]) print(\"F范数矩阵\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"fro\", dim=-1)) print(\"2范数矩阵\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=2, dim=-1)) print(\"核范数矩阵\") # 核范数作为Loss使用方法不详 print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"nuc\")) print(\"平均损失计算：每个样本的损失相加，再除以总样本数\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"fro\", dim=-1).sum() / label.shape[0]) print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=2, dim=-1).sum() / label.shape[0]) 输出： F范数矩阵 tensor([2., 2.], device='cuda:0') 2范数矩阵 tensor([2., 2.], device='cuda:0') 核范数矩阵 tensor(2.8284, device='cuda:0') 平均损失计算：每个样本的损失相加，再除以总样本数 tensor(2., device='cuda:0') tensor(2., device='cuda:0') Process finished with exit code 0 \u003e\u003e\u003e # 不同shape的欧距计算 作者：https://blog.csdn.net/sinat_24899403/article/details/119249822 \u003e\u003e\u003e import torch \u003e\u003e\u003e from torch import nn \u003e\u003e\u003e a=torch.tensor([[1,1,1], [2,2,2]]) \u003e\u003e\u003e b=torch.tensor([[2,2,2], [1,1,1], [2,2,2], [1,1,1], [2,2,2]]) \u003e\u003e\u003e def pdist(a: torch.Tensor, b: torch.Tensor, p: int = 2) -\u003e torch.Tensor: return (a-b).abs().pow(p).sum(-1).pow(1/p) \u003e\u003e\u003e a_=a.unsqueeze(1) \u003e\u003e\u003e b_=b.unsqueeze(0) \u003e\u003e\u003e print(pdist(a_,b_)) tensor([[1.7321, 0.0000, 1.7321, 0.0000, 1.7321], [0.0000, 1.7321, 0.0000, 1.7321, 0.0000]]) 汉明距离 如果是多对多，且shape不一致，使用numpy计算是最快的，考虑原因可能是torch未封装int类型的不同shape数据的计算方式，如果shape相同可以使用pairwise的方法 形状相同（用于标签预测） \u003e\u003e\u003e b = torch.tensor([[0, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e c = torch.tensor([[1, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e metric = BinaryHammingDistance(multidim_average='samplewise') \u003e\u003e\u003e print(metric(b, c)) tensor([0.3333, 0.0000]) 形状不同（用于检索） \u003e\u003e\u003e from sklearn.neighbors import DistanceMetric \u003e\u003e\u003e import time \u003e\u003e\u003e a = np.array([[0, 0, 2],]) \u003e\u003e\u003e b = np.array([[0, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e s1 = time.time() \u003e\u003e\u003e print(DistanceMetric.get_metric(\"hamming\").pairwise(a,b)) \u003e\u003e\u003e print(f\"耗时{time.time()-s1}\") [[0.33333333 0.66666667]] 耗时0.0003008842468261719 \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float) \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float) \u003e\u003e\u003e s2 = time.time() \u003e\u003e\u003e print(torch.cdist(a_tensor, b_tensor, p=0)) \u003e\u003e\u003e print(f\"耗时{time.time()-s2}\") tensor([[1., 2.]]) 耗时0.0003383159637451172 \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float).cuda() \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float).cuda() \u003e\u003e\u003e s3 = time.time() \u003e\u003e\u003e print(torch.cdist(a_tensor, b_tensor, p=0)) \u003e\u003e\u003e print(f\"耗时{time.time()-s3}\") tensor([[1., 2.]], device='cuda:0') 耗时0.0015494823455810547 ","date":"2022-04-29","objectID":"/loss_function/:1:1","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#核范数nuclear-norm"},{"categories":["machine_learning"],"content":" 距离度量 F 范数（Frobenius 范数）矩阵所有元素的平方和再开方，他是是向量二范式的拓展类比。 $$ ||A||_F = \\sqrt{\\sum a_{ij}^2} $$ 如何作为距离度量呢？ 可以利用如下方式，如果标签矩阵为$B$（通常为一个batch里的多个样本标签向量组成，如标签向量为4维，batch_size 为128）则$B$的形状为（128，4）），距离度量公式为 $$ \\mathcal L_F = ||A-B|| $$ 核范数（nuclear norm）作为距离度量方式不详 $$ \\begin{aligned} ||A||_*=\\operatorname{tr}\\left(\\sqrt{A^{T} A}\\right) \u0026=\\operatorname{tr}\\left(\\sqrt{\\left(U \\Sigma V^{T}\\right)^{T} U \\Sigma V^{T}}\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V \\Sigma^{T} U^{T} U \\Sigma V^{T}}\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V \\Sigma^{2} V^{T}}\\right)\\left(\\Sigma^{T}=\\Sigma\\right) \\ \u0026=\\operatorname{tr}\\left(\\sqrt{V^{T} V \\Sigma^{2}}\\right) \\ \u0026=\\operatorname{tr}(\\Sigma) \\end{aligned} $$ 参考代码 import torch label = np.array([[1, 2, 3, 4], [1, 2, 3, 4]]) pred = np.array([[2, 3, 4, 5], [2, 3, 4, 5]]) print(\"F范数矩阵\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"fro\", dim=-1)) print(\"2范数矩阵\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=2, dim=-1)) print(\"核范数矩阵\") # 核范数作为Loss使用方法不详 print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"nuc\")) print(\"平均损失计算：每个样本的损失相加，再除以总样本数\") print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=\"fro\", dim=-1).sum() / label.shape[0]) print(torch.norm(torch.from_numpy(pred - label).type(torch.cuda.FloatTensor), p=2, dim=-1).sum() / label.shape[0]) 输出： F范数矩阵 tensor([2., 2.], device='cuda:0') 2范数矩阵 tensor([2., 2.], device='cuda:0') 核范数矩阵 tensor(2.8284, device='cuda:0') 平均损失计算：每个样本的损失相加，再除以总样本数 tensor(2., device='cuda:0') tensor(2., device='cuda:0') Process finished with exit code 0 \u003e\u003e\u003e # 不同shape的欧距计算 作者：https://blog.csdn.net/sinat_24899403/article/details/119249822 \u003e\u003e\u003e import torch \u003e\u003e\u003e from torch import nn \u003e\u003e\u003e a=torch.tensor([[1,1,1], [2,2,2]]) \u003e\u003e\u003e b=torch.tensor([[2,2,2], [1,1,1], [2,2,2], [1,1,1], [2,2,2]]) \u003e\u003e\u003e def pdist(a: torch.Tensor, b: torch.Tensor, p: int = 2) -\u003e torch.Tensor: return (a-b).abs().pow(p).sum(-1).pow(1/p) \u003e\u003e\u003e a_=a.unsqueeze(1) \u003e\u003e\u003e b_=b.unsqueeze(0) \u003e\u003e\u003e print(pdist(a_,b_)) tensor([[1.7321, 0.0000, 1.7321, 0.0000, 1.7321], [0.0000, 1.7321, 0.0000, 1.7321, 0.0000]]) 汉明距离 如果是多对多，且shape不一致，使用numpy计算是最快的，考虑原因可能是torch未封装int类型的不同shape数据的计算方式，如果shape相同可以使用pairwise的方法 形状相同（用于标签预测） \u003e\u003e\u003e b = torch.tensor([[0, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e c = torch.tensor([[1, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e metric = BinaryHammingDistance(multidim_average='samplewise') \u003e\u003e\u003e print(metric(b, c)) tensor([0.3333, 0.0000]) 形状不同（用于检索） \u003e\u003e\u003e from sklearn.neighbors import DistanceMetric \u003e\u003e\u003e import time \u003e\u003e\u003e a = np.array([[0, 0, 2],]) \u003e\u003e\u003e b = np.array([[0, 0, 1], [0, 1, 0] ]) \u003e\u003e\u003e s1 = time.time() \u003e\u003e\u003e print(DistanceMetric.get_metric(\"hamming\").pairwise(a,b)) \u003e\u003e\u003e print(f\"耗时{time.time()-s1}\") [[0.33333333 0.66666667]] 耗时0.0003008842468261719 \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float) \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float) \u003e\u003e\u003e s2 = time.time() \u003e\u003e\u003e print(torch.cdist(a_tensor, b_tensor, p=0)) \u003e\u003e\u003e print(f\"耗时{time.time()-s2}\") tensor([[1., 2.]]) 耗时0.0003383159637451172 \u003e\u003e\u003e a_tensor = torch.as_tensor(a, dtype=torch.float).cuda() \u003e\u003e\u003e b_tensor = torch.as_tensor(b, dtype=torch.float).cuda() \u003e\u003e\u003e s3 = time.time() \u003e\u003e\u003e print(torch.cdist(a_tensor, b_tensor, p=0)) \u003e\u003e\u003e print(f\"耗时{time.time()-s3}\") tensor([[1., 2.]], device='cuda:0') 耗时0.0015494823455810547 ","date":"2022-04-29","objectID":"/loss_function/:1:1","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#汉明距离"},{"categories":["machine_learning"],"content":" 相对熵（也称KL散度）维基百科：KL散度（Kullback-Leibler divergence，简称KLD），在讯息系统中称为相对熵（relative entropy），在连续时间序列中称为随机性（randomness），在统计模型推断中称为讯息增益（information gain）。也称讯息散度（information divergence）。它是两个几率分布$P$和$Q$差别的非对称性的度量。 KL散度是用来度量使用基于$Q$的分布来编码服从$P$的分布的样本所需的额外的平均比特数，注意$P$,$Q$先后顺序。典型情况下，P表示数据的真实分布，Q表示数据的理论分布、估计的模型分布、或P的近似分布。 $$ \\begin{equation} \\begin{aligned} D_{KL}(P||Q) \u0026= -\\sum_iP(i)ln\\frac{Q(i)}{P(i)} \\\\ \u0026= \\sum_iP(i)ln\\frac{P(i)}{Q(i)} \\end{aligned} \\end{equation} $$ 相对熵的值为非负数： $$ D_{KL}(P||Q)\\geq 0 $$ 另，概率都为零时取零。 ","date":"2022-04-29","objectID":"/loss_function/:1:2","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#相对熵也称kl散度"},{"categories":["machine_learning"],"content":" 交叉熵——Cross Entropy维基百科：在信息论中，基于相同事件测度的两个概率分布$p$和$q$的交叉熵是指，当基于一个“非自然”（相对于“真实”分布$p$而言）的概率分布$q$进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。 给定两个概率分布$p$和$q$，$p$相对于$q$的交叉熵定义为： $$ H(p,q) = E_p[-\\log q] = H(p) + D_{KL}(p||q), $$ 其中，$H(p)$是$p$的熵，$D_{KL}(p||q)$是从$p$与$q$的KL散度（也被称为$p$，相对于$q$的相对熵）。 对于离散分布$p$和$q$，交叉熵可以定义为： $$ H(p,q)=-\\sum_xp(x)\\log q(x). $$ 其中求和是指在样本空间进行计算求和。记住这个计算方法，因为后续介绍分类任务的交叉熵损失是基于此的。 二分类交叉熵假设在二分类任务时，真实标签$y$和预测标签$\\hat{y}$取值空间为${0,1}$。根据交叉熵定义，可以将交叉熵损失函数定义如下。 $$ J=-\\frac{1}{N}\\sum_{i=1}^{N}[y\\log\\hat{y}+(1-y)\\log(1-\\hat{y})] $$ 假设真实分布为$p(i)$(真实标签$y$的分布)，模型预测的分布为$q(i)$(预测标签$\\hat{y}$的分布，推导如下， $$ \\begin{equation} \\begin{aligned} H(p,q) \u0026=-\\sum_x p(x)\\cdot \\log({q(x)}) \\\\ \u0026=\\sum_x p(x)\\cdot \\log(\\frac{1}{q(x)})\\\\ \u0026=\\sum_x p_{(y=0|x)} \\cdot \\log(\\frac{1}{q_{(y=0|x)} }) + p_{(y=1|x)} \\cdot \\log(\\frac{1}{q_{(y=1|x)}})\\\\ \u0026=\\sum_x y\\log(\\frac{1}{\\hat{y}}) + (1-y)\\log(\\frac{1}{1-\\hat{y}})\\\\ \u0026=-\\sum_x [y\\log \\hat{y} + (1-y)\\log(1-\\hat{y})] \\end{aligned} \\end{equation} $$ 最后乘上$\\frac{1}{N}$进行平均操作。 ","date":"2022-04-29","objectID":"/loss_function/:1:3","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#交叉熵cross-entropy"},{"categories":["machine_learning"],"content":" 交叉熵——Cross Entropy维基百科：在信息论中，基于相同事件测度的两个概率分布$p$和$q$的交叉熵是指，当基于一个“非自然”（相对于“真实”分布$p$而言）的概率分布$q$进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。 给定两个概率分布$p$和$q$，$p$相对于$q$的交叉熵定义为： $$ H(p,q) = E_p[-\\log q] = H(p) + D_{KL}(p||q), $$ 其中，$H(p)$是$p$的熵，$D_{KL}(p||q)$是从$p$与$q$的KL散度（也被称为$p$，相对于$q$的相对熵）。 对于离散分布$p$和$q$，交叉熵可以定义为： $$ H(p,q)=-\\sum_xp(x)\\log q(x). $$ 其中求和是指在样本空间进行计算求和。记住这个计算方法，因为后续介绍分类任务的交叉熵损失是基于此的。 二分类交叉熵假设在二分类任务时，真实标签$y$和预测标签$\\hat{y}$取值空间为${0,1}$。根据交叉熵定义，可以将交叉熵损失函数定义如下。 $$ J=-\\frac{1}{N}\\sum_{i=1}^{N}[y\\log\\hat{y}+(1-y)\\log(1-\\hat{y})] $$ 假设真实分布为$p(i)$(真实标签$y$的分布)，模型预测的分布为$q(i)$(预测标签$\\hat{y}$的分布，推导如下， $$ \\begin{equation} \\begin{aligned} H(p,q) \u0026=-\\sum_x p(x)\\cdot \\log({q(x)}) \\\\ \u0026=\\sum_x p(x)\\cdot \\log(\\frac{1}{q(x)})\\\\ \u0026=\\sum_x p_{(y=0|x)} \\cdot \\log(\\frac{1}{q_{(y=0|x)} }) + p_{(y=1|x)} \\cdot \\log(\\frac{1}{q_{(y=1|x)}})\\\\ \u0026=\\sum_x y\\log(\\frac{1}{\\hat{y}}) + (1-y)\\log(\\frac{1}{1-\\hat{y}})\\\\ \u0026=-\\sum_x [y\\log \\hat{y} + (1-y)\\log(1-\\hat{y})] \\end{aligned} \\end{equation} $$ 最后乘上$\\frac{1}{N}$进行平均操作。 ","date":"2022-04-29","objectID":"/loss_function/:1:3","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#二分类交叉熵"},{"categories":["machine_learning"],"content":" 多分类交叉熵与二分类交叉熵类似，多分类交叉熵同样是计算标签分布的熵值，基于此，我们需要把多分类考虑在内，也就是多一步求和，即每个类别上的交叉熵求和并在样本空间上进行求和，基于二分类交叉熵的参数定义，定义分类的标签种类为$n$，则多分类交叉熵损失可以表示如下 $$ \\begin{equation} \\mathcal L = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{j=1}^ny_j^{(i)}\\cdot \\log\\hat{y}_j^{(i)} \\end{equation} $$ 其中,$n$为分类个数，即标签向量维度。$N$为样本个数。 ","date":"2022-04-29","objectID":"/loss_function/:1:4","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#多分类交叉熵"},{"categories":["machine_learning"],"content":" 对比损失 三元损失 给定anchor $p$，以欧距为例，衡量相似度，其正样本为$q$，负样本为$r$。 $$ \\begin{equation} \\begin{aligned} \\mathcal L = max(||\\boldsymbol p-\\boldsymbol q||^2-||\\boldsymbol p-\\boldsymbol r||^2+\\epsilon,0) \\end{aligned} \\end{equation} $$ 代码实现 \"\"\" loss class \"\"\" class triplet_loss(nn.Module): def __init__(self): super(triplet_loss, self).__init__() self.margin = 0.2 def forward(self, anchor, positive, negative): pos_dist = (anchor - positive).pow(2).sum(1) neg_dist = (anchor - negative).pow(2).sum(1) loss = F.relu(pos_dist - neg_dist + self.margin) return loss.mean()#we can also use #torch.nn.functional.pairwise_distance(anchor,positive, keep_dims=True), which #computes the euclidean distance. \"\"\" training part \"\"\" loss_fun = triplet_loss() optimizer = Adam(custom_model.parameters(), lr = 0.001) for epoch in range(30): total_loss = 0 for i, (anchor, positive, negative) in enumerate(custom_loader): anchor = anchor['image'].to(device) positive = positive['image'].to(device) negative = negative['image'].to(device) anchor_feature = custom_model(anchor) positive_feature = custom_model(positive) negative_feature = custom_model(negative) optimizer.zero_grad() loss = loss_fun(anchor_feature, positive_feature, negative_feature) loss.backward() optimizer.step() 参考链接https://zhuanlan.zhihu.com/p/414327252?ivk_sa=1024320u 更多对比损失 https://lilianweng.github.io/posts/2021-05-31-contrastive/ ","date":"2022-04-29","objectID":"/loss_function/:1:5","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#对比损失"},{"categories":["machine_learning"],"content":" 对比损失 三元损失 给定anchor $p$，以欧距为例，衡量相似度，其正样本为$q$，负样本为$r$。 $$ \\begin{equation} \\begin{aligned} \\mathcal L = max(||\\boldsymbol p-\\boldsymbol q||^2-||\\boldsymbol p-\\boldsymbol r||^2+\\epsilon,0) \\end{aligned} \\end{equation} $$ 代码实现 \"\"\" loss class \"\"\" class triplet_loss(nn.Module): def __init__(self): super(triplet_loss, self).__init__() self.margin = 0.2 def forward(self, anchor, positive, negative): pos_dist = (anchor - positive).pow(2).sum(1) neg_dist = (anchor - negative).pow(2).sum(1) loss = F.relu(pos_dist - neg_dist + self.margin) return loss.mean()#we can also use #torch.nn.functional.pairwise_distance(anchor,positive, keep_dims=True), which #computes the euclidean distance. \"\"\" training part \"\"\" loss_fun = triplet_loss() optimizer = Adam(custom_model.parameters(), lr = 0.001) for epoch in range(30): total_loss = 0 for i, (anchor, positive, negative) in enumerate(custom_loader): anchor = anchor['image'].to(device) positive = positive['image'].to(device) negative = negative['image'].to(device) anchor_feature = custom_model(anchor) positive_feature = custom_model(positive) negative_feature = custom_model(negative) optimizer.zero_grad() loss = loss_fun(anchor_feature, positive_feature, negative_feature) loss.backward() optimizer.step() 参考链接https://zhuanlan.zhihu.com/p/414327252?ivk_sa=1024320u 更多对比损失 https://lilianweng.github.io/posts/2021-05-31-contrastive/ ","date":"2022-04-29","objectID":"/loss_function/:1:5","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#三元损失"},{"categories":["machine_learning"],"content":" 对比损失 三元损失 给定anchor $p$，以欧距为例，衡量相似度，其正样本为$q$，负样本为$r$。 $$ \\begin{equation} \\begin{aligned} \\mathcal L = max(||\\boldsymbol p-\\boldsymbol q||^2-||\\boldsymbol p-\\boldsymbol r||^2+\\epsilon,0) \\end{aligned} \\end{equation} $$ 代码实现 \"\"\" loss class \"\"\" class triplet_loss(nn.Module): def __init__(self): super(triplet_loss, self).__init__() self.margin = 0.2 def forward(self, anchor, positive, negative): pos_dist = (anchor - positive).pow(2).sum(1) neg_dist = (anchor - negative).pow(2).sum(1) loss = F.relu(pos_dist - neg_dist + self.margin) return loss.mean()#we can also use #torch.nn.functional.pairwise_distance(anchor,positive, keep_dims=True), which #computes the euclidean distance. \"\"\" training part \"\"\" loss_fun = triplet_loss() optimizer = Adam(custom_model.parameters(), lr = 0.001) for epoch in range(30): total_loss = 0 for i, (anchor, positive, negative) in enumerate(custom_loader): anchor = anchor['image'].to(device) positive = positive['image'].to(device) negative = negative['image'].to(device) anchor_feature = custom_model(anchor) positive_feature = custom_model(positive) negative_feature = custom_model(negative) optimizer.zero_grad() loss = loss_fun(anchor_feature, positive_feature, negative_feature) loss.backward() optimizer.step() 参考链接https://zhuanlan.zhihu.com/p/414327252?ivk_sa=1024320u 更多对比损失 https://lilianweng.github.io/posts/2021-05-31-contrastive/ ","date":"2022-04-29","objectID":"/loss_function/:1:5","series":[],"tags":["math"],"title":"损失函数","uri":"/loss_function/#参考链接"},{"categories":[],"content":" 排序","date":"2022-04-26","objectID":"/basic_algorithm/:1:0","series":[],"tags":["py"],"title":"好用的Python基础算法应用技巧","uri":"/basic_algorithm/#排序"},{"categories":[],"content":" 列表 内置排序方法 sort() lis = [4,2,3,1] lis.sort() print(lis) [1, 2, 3, 4] sorted() lis = [4,2,3,1] sorted(lis) print(lis) lis = sorted(lis) print(lis) lis.reverse() print(lis) [4, 2, 3, 1] [1, 2, 3, 4] [4, 3, 2, 1] 带原索引一起排序这种方法的好处是，如果你有两个列表，一一对应，你想在变化其中一个的时候希望另一个跟着变动相同的索引，你便需要用到。 lis_a = [4,2,3,1] lis_b = [3,4,1,2] new_lis_a = [] new_lis_b = [] c = sorted(enumerate(lis_a),key = lambda x:x[1]) # x[1] 根据元组的第二个元素排序 for i in range(len(c)): new_lis_a.append(c[i][1]) new_lis_b.append(lis_b[c[i][0]]) print(new_lis_a) print(new_lis_b) [1, 2, 3, 4] [2, 4, 1, 3] 发现了前面两个列表对应的位置都是一样的元素了么？easy ","date":"2022-04-26","objectID":"/basic_algorithm/:1:1","series":[],"tags":["py"],"title":"好用的Python基础算法应用技巧","uri":"/basic_algorithm/#列表"},{"categories":[],"content":" 列表 内置排序方法 sort() lis = [4,2,3,1] lis.sort() print(lis) [1, 2, 3, 4] sorted() lis = [4,2,3,1] sorted(lis) print(lis) lis = sorted(lis) print(lis) lis.reverse() print(lis) [4, 2, 3, 1] [1, 2, 3, 4] [4, 3, 2, 1] 带原索引一起排序这种方法的好处是，如果你有两个列表，一一对应，你想在变化其中一个的时候希望另一个跟着变动相同的索引，你便需要用到。 lis_a = [4,2,3,1] lis_b = [3,4,1,2] new_lis_a = [] new_lis_b = [] c = sorted(enumerate(lis_a),key = lambda x:x[1]) # x[1] 根据元组的第二个元素排序 for i in range(len(c)): new_lis_a.append(c[i][1]) new_lis_b.append(lis_b[c[i][0]]) print(new_lis_a) print(new_lis_b) [1, 2, 3, 4] [2, 4, 1, 3] 发现了前面两个列表对应的位置都是一样的元素了么？easy ","date":"2022-04-26","objectID":"/basic_algorithm/:1:1","series":[],"tags":["py"],"title":"好用的Python基础算法应用技巧","uri":"/basic_algorithm/#内置排序方法"},{"categories":[],"content":" 列表 内置排序方法 sort() lis = [4,2,3,1] lis.sort() print(lis) [1, 2, 3, 4] sorted() lis = [4,2,3,1] sorted(lis) print(lis) lis = sorted(lis) print(lis) lis.reverse() print(lis) [4, 2, 3, 1] [1, 2, 3, 4] [4, 3, 2, 1] 带原索引一起排序这种方法的好处是，如果你有两个列表，一一对应，你想在变化其中一个的时候希望另一个跟着变动相同的索引，你便需要用到。 lis_a = [4,2,3,1] lis_b = [3,4,1,2] new_lis_a = [] new_lis_b = [] c = sorted(enumerate(lis_a),key = lambda x:x[1]) # x[1] 根据元组的第二个元素排序 for i in range(len(c)): new_lis_a.append(c[i][1]) new_lis_b.append(lis_b[c[i][0]]) print(new_lis_a) print(new_lis_b) [1, 2, 3, 4] [2, 4, 1, 3] 发现了前面两个列表对应的位置都是一样的元素了么？easy ","date":"2022-04-26","objectID":"/basic_algorithm/:1:1","series":[],"tags":["py"],"title":"好用的Python基础算法应用技巧","uri":"/basic_algorithm/#带原索引一起排序"},{"categories":["tutorial"],"content":"室友帮忙弄的clash又不行了，今天打算自己修修（重装）这个梯子软件，记录一下。 现在的病情就是谷歌上不去，如下图所示。 clash的网页配置端口也上不去。 ","date":"2022-04-24","objectID":"/clash/:0:0","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#"},{"categories":["tutorial"],"content":" 准备工具首先我们要使用一台能够访问外网的机器来下载必要的文件，经典悖论，没错，上外网下clash用来上外网，不过github还是有几率能上去的。 到clash开源工具(go 写的)的repo地址下载最新的release，比如我本次下载的是版本。传送门 所使用的Linux系统是 lsb_release -a ","date":"2022-04-24","objectID":"/clash/:1:0","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#准备工具"},{"categories":["tutorial"],"content":" 开始安装首先解压下载的release文件在你想要的目录。比如我的在家目录的clash文件夹下。 gunzip clash-linux-amd64-v1.10.0.gz 然后进行第一次运行clash chmod +x clash-linux-amd64-v1.10.0(换成自己的clash版本) ./clash-linux-amd64-v1.10.0(换成自己的clash版本) 端口被占用了…… 查看是哪个进程占用了端口并kill sudo lsof -i tcp:7890 kill 11556(换成你自己找到的PID) 再试一次，ok启动了…… 这个时候，我们可以在用户家目录的.config/文件夹下找到自动生成的clash文件夹 ","date":"2022-04-24","objectID":"/clash/:2:0","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#开始安装"},{"categories":["tutorial"],"content":" 后续更新URL从这里开始然后我们使用运营商给的订阅链接输入浏览器网址，把内容全部复制到config.yaml文件里 vi ~/.config/clash/config.yaml（不推荐直接复制粘贴网页内容，建议往后看） 这种方式非常慢，由于页面文本较长，长时间在粘贴…… 果断关掉重来，我们将刚刚链接打开的网页右键另存为一个文件 然后重定向写入config.yaml文件，一秒钟搞定 cat ~/sub.txt \u003e ~/.config/clash/config.yaml ","date":"2022-04-24","objectID":"/clash/:3:0","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#后续更新url从这里开始"},{"categories":["tutorial"],"content":" 改完config之后，后续更新URL的操作至此完成写好了config文件之后，我们启动试试。顺便先在对应目录下给clash的可执行文件改个名 mv clash-linux-amd64-v1.10.0 clash ./clash 使用谷歌浏览器的端口代理插件设置网页代理端口为自己主机的7890如下 访问Google OK正常 如果你想使用本机端口服务做代理然后在命令行进行访问，可以使用 export http_proxy=http://127.0.0.1:7890 export https_proxy=http://127.0.0.1:7890 # 随便使用命令行git上下载一个包试试 wget https://github.com/pyenv/pyenv/archive/refs/tags/v2.2.5.zip 好使 ","date":"2022-04-24","objectID":"/clash/:3:1","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#改完config之后后续更新url的操作至此完成"},{"categories":["tutorial"],"content":" 基本功能完成，后续设置开机启动。将功能设置为一个service 增加system配置 sudo vi /etc/systemd/system/clash.service 写入以下内容 [Unit] Description=Clash service After=network.target [Service] Type=simple User=oliver #换成你自己用户名 WorkingDirectory=**** # 你希望这个服务在哪个文件夹下运行 ExecStart=/home/oliver/clash/clash -d /home/oliver/.config/clash/ #换成你自己clash目录，注意，前半部分一定要连接到文件！不能只到目录 Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target sudo systemctl daemon-reload sudo systemctl enable clash sudo systemctl start clash sudo systemctl status clash OK，成功了，图中端口9090是UI的端口，意味着访问http://127.0.0.1:9090就能出现如下界面 ","date":"2022-04-24","objectID":"/clash/:4:0","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#基本功能完成后续设置开机启动"},{"categories":["tutorial"],"content":" 配置定时更新订阅创建config.yaml更新脚本 vi /home/oliver/.config/clash/get_clash_config.py import requests # 订阅链接 url = 'https://xxxx.com' head = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A\"} config_yaml_content = requests.get(url, headers=head).text # 写入文件 config_yaml_path = '/home/oliver/.config/clash/config.yaml' f = open(config_yaml_path, 'w', encoding='utf-8') f.write(config_yaml_content) f.close() 每天更新 sudo vi /etc/cron.daily/clash.sh 向其中写入如下脚本 #!/bin/bash systemctl stop clash python3 /home/oliver/.config/clash/get_clash_config.py systemctl start clash sudo chmod +x /etc/cron.daily/clash.sh ","date":"2022-04-24","objectID":"/clash/:4:1","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#配置定时更新订阅"},{"categories":["tutorial"],"content":" 配置定时更新订阅创建config.yaml更新脚本 vi /home/oliver/.config/clash/get_clash_config.py import requests # 订阅链接 url = 'https://xxxx.com' head = {'User-Agent': \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3) AppleWebKit/537.75.14 (KHTML, like Gecko) Version/7.0.3 Safari/7046A194A\"} config_yaml_content = requests.get(url, headers=head).text # 写入文件 config_yaml_path = '/home/oliver/.config/clash/config.yaml' f = open(config_yaml_path, 'w', encoding='utf-8') f.write(config_yaml_content) f.close() 每天更新 sudo vi /etc/cron.daily/clash.sh 向其中写入如下脚本 #!/bin/bash systemctl stop clash python3 /home/oliver/.config/clash/get_clash_config.py systemctl start clash sudo chmod +x /etc/cron.daily/clash.sh ","date":"2022-04-24","objectID":"/clash/:4:1","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#每天更新"},{"categories":["tutorial"],"content":" 配置clash for windows 的linux版本，和我一样怕麻烦的跳到这里（2022年10月4日更新）这标题有点绕，说白了就是你会得到clash的windows下的相同界面。 首先，你需要在clash for windows的项目地址下载安装包，地址是https://github.com/Fndroid/clash_for_windows_pkg/releases 根据cpu架构2选1 然后，解压后进入该文件夹，运行如下命令 ./cfw 就能打开界面了 最后导入yaml文件即可 较为简易 至此结束~ echo \"至此结束~\" ","date":"2022-04-24","objectID":"/clash/:5:0","series":[],"tags":["linux","tools"],"title":"记一次Clash for Linux 的配置","uri":"/clash/#配置clash-for-windows-的linux版本和我一样怕麻烦的跳到这里2022年10月4日更新"},{"categories":[],"content":" 模型权重保存重载示例 # 自己定义的网络 ## 方法一：只保存权重 encoder = BertModel.from_pretrained(bert_model_path) model = EffiGlobalPointer(encoder, ENT_CLS_NUM, args.pointer_num).to(device) torch.save(best_model.state_dict(), \"data/model_data/best.pth\") prev_state_dict = torch.load('data/model_data/best.pth') model.load_state_dict(prev_state_dict) ## 方法二：保存所有的 torch.save(best_model, \"data/model_data/best.pt\") model = torch.load('data/model_data/best_state_dict.pt') # BertModel内置方法 # 注意特定的模型种类需要使用不同的from_pretrained方法，否则某些权重可能不加载,具体查看模型类 model = BertModel() model.save_pretrained('./output/bert_saved') model = BertModel.from_pretrained() 查询方式 权重文件内部是这样的，熟悉的W和b。 ","date":"2022-04-18","objectID":"/lm_pretraining/:0:0","series":[],"tags":["nlp"],"title":"语言模型预训练方法","uri":"/lm_pretraining/#模型权重保存重载示例"},{"categories":[],"content":" BERT","date":"2022-04-18","objectID":"/lm_pretraining/:0:0","series":[],"tags":["nlp"],"title":"语言模型预训练方法","uri":"/lm_pretraining/#bert"},{"categories":[],"content":" huggingface官方示例可以到https://github.com/huggingface/transformers/下的例子文件中寻找demo，由于官方更新速度快，具体路径请自行定位至examples文件夹~ 以pytorch版本的masked language model为例 只需要运行以下脚本即可 python run_mlm.py \\ --model_name_or_path roberta-base \\ --dataset_name wikitext \\ --dataset_config_name wikitext-2-raw-v1 \\ --per_device_train_batch_size 8 \\ --per_device_eval_batch_size 8 \\ --do_train \\ --do_eval \\ --output_dir /tmp/test-mlm 其中 dataset_name为语料文件，可以是txt格式的,每行一个文本序列 ","date":"2022-04-18","objectID":"/lm_pretraining/:1:0","series":[],"tags":["nlp"],"title":"语言模型预训练方法","uri":"/lm_pretraining/#huggingface官方示例"},{"categories":[],"content":" nezha","date":"2022-04-18","objectID":"/lm_pretraining/:0:0","series":[],"tags":["nlp"],"title":"语言模型预训练方法","uri":"/lm_pretraining/#nezha"},{"categories":[],"content":" 来源网络 import sys import os import csv from transformers import BertTokenizer, WEIGHTS_NAME,TrainingArguments,BertForMaskedLM,BertConfig import tokenizers import torch # from configuration_nezha import NeZhaConfig from model.modeling_nezha import NeZhaForMaskedLM from model.configuration_nezha import NeZhaConfig os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' from transformers import ( CONFIG_MAPPING, MODEL_FOR_MASKED_LM_MAPPING, AutoConfig, AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling, HfArgumentParser, Trainer, TrainingArguments, set_seed, LineByLineTextDataset, BertTokenizerFast ) model_path = '../nezha-base-www' tokenizer = BertTokenizerFast.from_pretrained(model_path, do_lower_case=True) config = NeZhaConfig.from_pretrained(model_path, num_labels=52) model = NeZhaForMaskedLM.from_pretrained(model_path, config=config) train_dataset=LineByLineTextDataset(tokenizer=tokenizer,file_path='../pretrain_unlabel_dataset/train_data/unlabeled_train_data.txt',block_size=128) data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15) pretrain_batch_size=128 num_train_epochs=30 training_args = TrainingArguments( output_dir='./pretrained_nezha', overwrite_output_dir=True, num_train_epochs=num_train_epochs, learning_rate=6e-5, per_device_train_batch_size=pretrain_batch_size, save_total_limit=10, logging_dir='./pretrained_nezha_log', logging_steps=10000, no_cuda=False)# save_steps=10000 trainer = Trainer( model=model, args=training_args, data_collator=data_collator, train_dataset=train_dataset) trainer.train() trainer.save_model('./pretrained_nezha') for i in range(100): torch.cuda.empty_cache() ","date":"2022-04-18","objectID":"/lm_pretraining/:1:0","series":[],"tags":["nlp"],"title":"语言模型预训练方法","uri":"/lm_pretraining/#来源网络"},{"categories":["tutorial"],"content":"Scrapy信息检索大作业驱动，爬虫应该是python使用者比较重要的一门技能，学会这个也能进厂了，本项目较为详细，实现不了你打我。","date":"2022-04-09","objectID":"/scrapy/","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/"},{"categories":["tutorial"],"content":" Scrapy框架概览Scrapy使用了Twisted异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活完成各种需求。 ","date":"2022-04-09","objectID":"/scrapy/:1:0","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#scrapy框架概览"},{"categories":["tutorial"],"content":" Scrapy结构组成 Components Introduction Scrapy Engine Scrapy引擎负责控制各组件之间的所有数据流，并在特定动作发生时触发事件。 Scheduler 调度器接收来自引擎的请求并将它们按一定要求排入队列以便后续响应引擎。（爬虫执行过程需要时间，不能同时完成所有任务，因而需要任务队列。） Downloader 下载器负责抓取网页并将它们喂给引擎，引擎转而喂给Spider爬虫程序。 Spider 爬虫是由Scrapy用户自行编写的客制化类，用来解析响应以及提取网页数据项或其他请求。 Item Pipline 负责处理由爬虫提取的items。典型任务包括清理数据，验证数据以及持续化存储在数据库中等。 Downloader middlewares 下载中间件是位于引擎和下载器之间的特定钩子程序（可以理解为连接点），用来处理由引擎发往下载器的请求已经来自下载器发回引擎的响应。使用下载中间件的情景如下：- 在请求发送给下载器之前进行处理（比如就在Scrapy向网页发送请求时）- 在响应发往爬虫程序时，改写得到的响应。- 发送一个新的请求而不是将得到的响应发给爬虫程序。- 在不抓取网页的情况下向爬虫传递一个响应。- 静默丢弃一些请求。 Spider middlewares 爬虫程序中间件是位于引擎和爬虫程序之间的特定的钩子程序，它能够处理爬虫程序的输入（发往爬虫程序的响应）以及爬虫程序得到的输出（数据项和请求）。使用爬虫程序中间件的情景如下：- 后处理爬虫程序回调的输出 - 更改/添加/删除请求或item；- 后处理启动请求；- 处理爬虫程序异常；- 对于一些基于响应内容的请求，调用errback而不是callback。 ","date":"2022-04-09","objectID":"/scrapy/:1:1","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#scrapy结构组成"},{"categories":["tutorial"],"content":" Scrapy工作数据流 简略框架 详细框架 Scrapy 的数据流由执行引擎进行控制，进行过程如下（步骤序号对应上图）： 流程开始，引擎从爬虫程序得到初始的爬取请求。 引擎将Requests放入调度器并请求进行下一个爬取的Requests。 调度器返回下一个Requests给引擎。 引擎将得到的Requests发送给下载器，途径下载器中间件。 在网页完成下载的同时，生成页面的响应并将它发送给引擎，途径下载器中间件。 引擎收到来自下载器的响应并将其发送给爬虫程序处理，途径爬虫程序中间件。 爬虫程序处理得到的响应并将抓取的items和新的Requests返回给引擎，途径爬虫中间件。 引擎发送处理好的items发送给Item Piplines，并将发送处理好的Requests给调度器并请求下一个爬取的Requests。 重复步骤3直到调度器里没有更多的Requests，流程结束。 ","date":"2022-04-09","objectID":"/scrapy/:1:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#scrapy工作数据流"},{"categories":["tutorial"],"content":" Scrapy 安装","date":"2022-04-09","objectID":"/scrapy/:2:0","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#scrapy-安装"},{"categories":["tutorial"],"content":" 文档官方文档：https://docs.scrapy.org/en/latest 中文维护：http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html ","date":"2022-04-09","objectID":"/scrapy/:2:1","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#文档"},{"categories":["tutorial"],"content":" 环境介绍Ubuntu 21.10, Python3.10.3，pyenv ","date":"2022-04-09","objectID":"/scrapy/:2:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#环境介绍"},{"categories":["tutorial"],"content":" 安装流程pip 更新 python -m pip install --upgrade pip 安装scrapy pip install scrapy ","date":"2022-04-09","objectID":"/scrapy/:2:3","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#安装流程"},{"categories":["tutorial"],"content":" Scrapy基本使用本次实践的是房天下网站，仅用于学习交流，不作任何商业用途，比如就以二手房为例。 st=\u003estart: 开始 op1=\u003eoperation: 进入城市列表页，获取每个城市房源的进入链接 op2=\u003eoperation: 进入每个城市对应的二手房源列表页面。 op3=\u003eoperation: 进入列表页每个二手房的详情页。 e=\u003eend: 结束 st-\u003eop1-\u003eop2-\u003eop3-\u003ee ","date":"2022-04-09","objectID":"/scrapy/:3:0","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#scrapy基本使用"},{"categories":["tutorial"],"content":" 创建scrapy项目 cd ~ scrapy startproject scrapy_fang ","date":"2022-04-09","objectID":"/scrapy/:3:1","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#创建scrapy项目"},{"categories":["tutorial"],"content":" 创建爬虫，注意爬虫名字不要和项目域名相同 cd scrapy_fang scrapy genspider esf esf.fang.com ","date":"2022-04-09","objectID":"/scrapy/:3:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#创建爬虫注意爬虫名字不要和项目域名相同"},{"categories":["tutorial"],"content":" 基本使用命令行输入scrapy进行查看 scrapy Scrapy 2.6.1 - project: scrapy_fang Usage: scrapy [options] [args] Available commands: bench Run quick benchmark test check Check spider contracts commands crawl Run a spider edit Edit spider fetch Fetch a URL using the Scrapy downloader genspider Generate new spider using pre-defined templates list List available spiders parse Parse URL (using its spider) and print the results runspider Run a self-contained spider (without creating a project) settings Get settings values shell Interactive scraping console startproject Create new project version Print Scrapy version view Open URL in browser, as seen by Scrapy Use “scrapy -h” to see more info about a command ","date":"2022-04-09","objectID":"/scrapy/:3:3","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#基本使用"},{"categories":["tutorial"],"content":" 模拟浏览器访问设置请求头(模拟浏览器访问)，在settings.py下添加，可以更换，随便打开一个网页，F12查看network，然后找一下User-Agent内容。 USER_AGENT = 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.60 Safari/537.36' 或者在下载器中间件写入如下类，随机请求头 class UserAgentDownloadMiddleware(object): def __int__(self): self.User_Agents = [ 'Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0', 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.13; ko; rv:1.9.1b2) Gecko/2008' '1201 Firefox/60.0', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)' ' Chrome/70.0.3538.77 Safari/537.36', 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome' '/44.0.2403.155 Safari/537.36', ] def process_request(self, request, spider): USER_AGENT = random.choice(self.User_Agents) request.headers['User-Agent'] = USER_AGENT **==不要忘记==**在settings.py里头指定该类为下载器中间件，否则无效，数字543等指定优先级，越小优先级越高。 DOWNLOADER_MIDDLEWARES = { 'scrapy_fang.middlewares.UserAgentDownloadMiddleware': 543, } ","date":"2022-04-09","objectID":"/scrapy/:3:4","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#模拟浏览器访问"},{"categories":["tutorial"],"content":" 请求和下载设定在settings.py中写入如下 # 限制并发Requests数量，默认是16 CONCURRENT_REQUESTS = 32 # 延时最低为2s DOWNLOAD_DELAY = 1 # 启动[自动限速] AUTOTHROTTLE_ENABLED = True # 开启[自动限速]的debug AUTOTHROTTLE_DEBUG = True # 设置最大下载延时 AUTOTHROTTLE_MAX_DELAY = 10 # 设置下载超时 DOWNLOAD_TIMEOUT = 15 # 限制对该网站的并发请求数 CONCURRENT_REQUESTS_PER_DOMAIN = 4 ","date":"2022-04-09","objectID":"/scrapy/:3:5","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#请求和下载设定"},{"categories":["tutorial"],"content":" 二手房链接获取爬虫","date":"2022-04-09","objectID":"/scrapy/:4:0","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#二手房链接获取爬虫"},{"categories":["tutorial"],"content":" 进入链接入口页面https://www.fang.com/SoufunFamily.htm ，测试插件是xpath helper 正确定位到各城市入口页面 //*[@id='senfe']//tr 测试程序里的xpath路径是否能抓到东西。 scrapy shell http://www.fang.com/SoufunFamily.htm [s] Available Scrapy objects: [s] scrapy scrapy module (contains scrapy.Request, scrapy.Selector, etc) [s] crawler \u003cscrapy.crawler.Crawler object at 0x7f34bb89eaa0\u003e [s] item {} [s] request \u003cGET http://www.fang.com/SoufunFamily.htm\u003e [s] response \u003c200 https://www.fang.com/SoufunFamily.htm\u003e [s] settings \u003cscrapy.settings.Settings object at 0x7f34bb89f3a0\u003e [s] spider \u003cDefaultSpider ‘default’ at 0x7f34bb2bc760\u003e [s] Useful shortcuts: [s] fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed) [s] fetch(req) Fetch a scrapy.Request and update local objects [s] shelp() Shell help (print this help) [s] view(response) View response in a browser \u003e\u003e\u003eresponse.xpath(\"//*[@id='senfe']//tr\") [, , , …… 抓取成功，想要抓取html里的其他内容也可以进行类似的操作，此处不一一列举。 注意xpath返回的是xpath对象组成的列表，而不是单纯的网页源码文本，要想获得内容需要.extrract() \u003e\u003e\u003eresponse.xpath(\"//*[@id='senfe']//tr\").extract() [’\\r\\n\\t\\t\\t\\t\\xa0\\r\\n\\t\\t\\t\\t直辖市\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t\\t\\t北京 \\r\\n\\t\\t\\t\\t\\t上海 \\r\\n\\t\\t\\t天津 \\r\\n\\t\\t\\t\\t\\t重庆\\r\\n\\t\\t\\t\\t\\r\\n\\t\\t\\t’,…… 这样就得到了抓取的网页html源码，每个tr标签是一个列表项~ ","date":"2022-04-09","objectID":"/scrapy/:4:1","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#进入链接入口页面"},{"categories":["tutorial"],"content":" 编写items.py 抓取城市入口链接数据类现在，明确我们要从这个页面获得什么数据，显然，我们需要获取，省份，城市，以及该城市二手房页面进入的链接，所以我们需要编写item类，用来暂时存放我们每个元数据的数据字典，scrapy给我们定义好了一个Item类，可以用来创建类字典的数据类型，可以当字典用，后续看。 class UrlItem(scrapy.Item): # 省份 province = scrapy.Field() # 城市 city = scrapy.Field() # 二手房链接 esfhouse_url = scrapy.Field() ","date":"2022-04-09","objectID":"/scrapy/:4:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#编写itemspy"},{"categories":["tutorial"],"content":" 编写items.py 抓取城市入口链接数据类现在，明确我们要从这个页面获得什么数据，显然，我们需要获取，省份，城市，以及该城市二手房页面进入的链接，所以我们需要编写item类，用来暂时存放我们每个元数据的数据字典，scrapy给我们定义好了一个Item类，可以用来创建类字典的数据类型，可以当字典用，后续看。 class UrlItem(scrapy.Item): # 省份 province = scrapy.Field() # 城市 city = scrapy.Field() # 二手房链接 esfhouse_url = scrapy.Field() ","date":"2022-04-09","objectID":"/scrapy/:4:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#抓取城市入口链接数据类"},{"categories":["tutorial"],"content":" 建立并编写spiders目录下的esf.py主要属性和方法 name 定义sipder名字的字符串，必须。 例如，如果spider爬取qq.com，该spider的name通常定义为qq allowed_domains 包含了spider允许爬取的域名列表，可选。 start_urls 初始URL列表。当没有制定特定的URL时，spider将从该列表中进行爬取，必选。 start_requests(self) 该方法必须返回一个可迭代（iterable）对象。该对象包含了spider用于爬取（默认实现是使用start_urls的url）的第一个Request。 当spider启动爬取并且未指定start_urls，该方法将被调用。 parse(self, response) 当请求url返回网页没有指定回调函数时，默认的Requests对象回调函数是parse。用来处理网页返回的response，以及生成item和Requests对象。 log(self, message[, level, component]) 使用scrapy.log.msg()方法记录(log)message。 class EsfSpider(scrapy.Spider): # 别名，后续用来其启动爬虫 name = 'esf' # 允许爬取的页面 allowed_domains = ['fang.com'] # 启动爬取的页面 start_urls = ['http://www.fang.com/SoufunFamily.htm'] # Scrapy框架默认调用这个方法。response.body是html源码 def parse(self, response): item = UrlItem() # 获取数据的每一行tr，可能一行爲一個省份，也可能多行爲一個省份 trs = response.xpath(\"//*[@id='senfe']//tr\") # print(trs) city_lis = [] for tr in trs: # 获取省份的td tds = tr.xpath(\".//td[not(@class)]\") # \"not(@class)\" ：没有为class赋值的被选取 province_td = tds[0] # #获取省份的值 province_text = province_td.xpath(\".//text()\").get() # #给省份去掉 多余空格 province_text = re.sub(r'\\s', '', province_text) # 判断省份是否为空的 if province_text: # 将不为空的省份赋值给province province = province_text # 除去国外的链接 if province == '其它': break # 除去直辖市中的重庆和天津，因为在文中有写到 if province == '重庆' or province == '天津': continue item['province'] = province # 获取城市的td city_td = tds[1] # 获取到城市的a元素 city_links = city_td.xpath(\".//a\") for city_link in city_links: # 城市 city = city_link.xpath(\".//text()\").get() # get方法相当于取列表第0个元素.extract() item['city'] = city # 城市的链接 city_url = city_link.xpath(\".//@href\").get() # 以点分割城市的链接 city_url_list = city_url.split('.') # 拼接城市二手房链接 esfhouse_url = city_url_list[0] + '.esf.' + city_url_list[1] + '.' + city_url_list[2] item['esfhouse_url'] = esfhouse_url # 这里dict比较关键，因为我们只初始化了一次item = UrlItem()，对python底层实现不是很清楚，但如果不转化会重复append相同item对象。 city_lis.append(dict(item)) return city_lis 所以这里return给谁了呢？与Scrapy框架图对应起来，return给了引擎，对应第7步，尝试在此直接输出到文件而**==不经过pipline==**。 ","date":"2022-04-09","objectID":"/scrapy/:4:3","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#建立并编写spiders目录下的esfpy"},{"categories":["tutorial"],"content":" 简单爬虫的持久化存储 scrapy crawl esf -o esf_url.json 打开esf_url.json查看如下，json后缀默认中文为Unicode编码格式，csv和xml默认为utf-8编码格式 [ {\"province\": \"\\u5b89\\u5fbd\", \"city\": \"\\u5408\\u80a5\", \"esfhouse_url\": \"http://hf.esf.fang.com/\"}, {\"province\": \"\\u5b89\\u5fbd\", \"city\": \"\\u829c\\u6e56\", \"esfhouse_url\": \"http://wuhu.esf.fang.com/\"}, {\"province\": \"\\u5b89\\u5fbd\", \"city\": \"\\u6dee\\u5357\", \"esfhouse_url\": \"http://huainan.esf.fang.com/\"}, {\"province\": \"\\u5b89\\u5fbd\", \"city\": \"\\u868c\\u57e0\", \"esfhouse_url\": \"http://bengbu.esf.fang.com/\"}, {\"province\": \"\\u5b89\\u5fbd\", \"city\": \"\\u961c\\u9633\", \"esfhouse_url\": \"http://fuyang.esf.fang.com/\"}, …… ] scrapy crawl esf -o esf_url.csv city,esfhouse_url,province 北京,http://bj.esf.fang.com/,直辖市 上海,http://sh.esf.fang.com/,直辖市 …… scrapy crawl esf -o esf_url.xml \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003citems\u003e \u003citem\u003e\u003cprovince\u003e直辖市\u003c/province\u003e\u003ccity\u003e北京\u003c/city\u003e\u003cesfhouse_url\u003ehttp://bj.esf.fang.com/\u003c/esfhouse_url\u003e\u003c/item\u003e \u003citem\u003e\u003cprovince\u003e直辖市\u003c/province\u003e\u003ccity\u003e上海\u003c/city\u003e\u003cesfhouse_url\u003ehttp://sh.esf.fang.com/\u003c/esfhouse_url\u003e\u003c/item\u003e …… 老老实实存下来了，在此其实一个简单的爬虫就写好了，如果你只是要这个页面上的链接的话，已经完成了，然而这种写法难以进行写入数据库等存储操作，后续就要用到Pipline机制了。 ","date":"2022-04-09","objectID":"/scrapy/:4:4","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#简单爬虫的持久化存储"},{"categories":["tutorial"],"content":" 使用Scrapy的Pipline机制进行持久化存储。我们这里尝试使用yield方法返回item，改写爬虫类中的parse方法如下。（yield方法的使用与return显式区别是yield不会终止本程序运行，如果函数里有yield，且它在循环里，下次调用函数的时候，会直接从循环的下一次开始，而不是像return一样从函数从头到尾执行。更为详细的区别参考CSDN上的这篇博客） def parse(self, response): item = UrlItem() # 获取数据的每一行tr，可能一行爲一個省份，也可能多行爲一個省份 trs = response.xpath(\"//*[@id='senfe']//tr\") # print(trs) for tr in trs: # 获取省份的td tds = tr.xpath(\".//td[not(@class)]\") # \"not(@class)\" ：没有为class赋值的被选取 province_td = tds[0] # #获取省份的值 province_text = province_td.xpath(\".//text()\").get() # #给省份去掉 多余空格 province_text = re.sub(r'\\s', '', province_text) # 判断省份是否为空的 if province_text: # 将不为空的省份赋值给province province = province_text # 除去国外的链接 if province == '其它': break # 除去直辖市中的重庆和天津，因为在文中有写到 if province == '重庆' or province == '天津': continue item['province'] = province # 获取城市的td city_td = tds[1] # 获取到城市的a元素 city_links = city_td.xpath(\".//a\") for city_link in city_links: # 城市 city = city_link.xpath(\".//text()\").get() # get方法相当于取列表第0个元素.extract() item['city'] = city # 城市的链接 city_url = city_link.xpath(\".//@href\").get() # 以点分割城市的链接 city_url_list = city_url.split('.') # 拼接城市二手房链接 esfhouse_url = city_url_list[0] + '.esf.' + city_url_list[1] + '.' + city_url_list[2] item['esfhouse_url'] = esfhouse_url # 返回提取到的每个item给pipline，处理完成之后继续执行后面代码（下一个循环） yield item 写pipline类，向piplines.py里写入如下内容 from scrapy.exporters import CsvItemExporter class ScrapyFangPipeline(object): url_fp = open('esf_url.csv', 'wb') url_exporter = CsvItemExporter(url_fp) def process_item(self, item, spider): self.url_exporter.export_item(item) # 返回item对象是必须的，告诉引擎item已经处理完成了，可以继续执行后面代码。 return item def close_spider(self, spider): self.url_fp.close() 启用pipline，在settings.py中写入如下，300为优先级，多个管道程序(执行类中的process_item方法)按照优先级依次进行 ITEM_PIPELINES = { 'scrapy_fang.pipelines.ScrapyFangPipeline': 300, } scrapy crawl esf 查看生成的esf_url.csv如下 city,esfhouse_url,province 北京,http://bj.esf.fang.com/,直辖市 上海,http://sh.esf.fang.com/,直辖市 天津,http://tj.esf.fang.com/,直辖市 重庆,http://cq.esf.fang.com/,直辖市 ","date":"2022-04-09","objectID":"/scrapy/:4:5","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#使用scrapy的pipline机制进行持久化存储"},{"categories":["tutorial"],"content":" 至此完成基本的Scrapy项目基本Scrapy框架实现完成了单个页面上的数据爬取，省份，城市，链接…… ","date":"2022-04-09","objectID":"/scrapy/:4:6","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#至此完成基本的scrapy项目"},{"categories":["tutorial"],"content":" 特定城市二手房概要信息爬虫我们在本次尝试直接拼接页码得到url，观察得到，北京二手房首页URL为http://bj.esf.fang.com/，也可以在页码中写为https://bj.esf.fang.com/house/i31/，第二页就是https://bj.esf.fang.com/house/i32/，依次类推…… 在shell里面尝试能否抓到数据 scrapy shell https://bj.esf.fang.com/house/i31/ \u003e\u003e\u003eresponse.xpath(\"//dl[@dataflag='bg']\") [] 抓取失败（返回空），后面分析是URL重定向的原因，网站做了防护，让网页重定向去往了另一个URL。同时，测试发现http://bj.esf.fang.com/不会进行重定向操作，而想要访问迭代的网址https://bj.esf.fang.com/house/i31/，32，33等却会进行重定向。 ","date":"2022-04-09","objectID":"/scrapy/:5:0","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#特定城市二手房概要信息爬虫"},{"categories":["tutorial"],"content":" 矛与盾——重定向分析返回的response \u003ca class=\"btn-redir\" style=\"font-size: 14pt;\" href=\"https://bj.esf.fang.com/house/i31/?rfss=1-a9078821a1b90b2a70-34\"\u003e\\xe7\\x82\\xb9\\xe5\\x87\\xbb\\xe8\\xb7\\xb3\\xe8\\xbd\\xac\u003c/a\u003e\\r\\n\u003c/div\u003e\\r\\n\u003c/div\u003e\\r\\n\u003cscript\u003e\\r\\n(function(){\\r\\n var secs=100,si=setInterval(function(){\\r\\n if(!--secs){\\r\\n //location.href=\"https://bj.esf.fang.com/house/i31/?rfss=1-a9078821a1b90b2a70-34\";\\r\\n clearInterval(si);\\r\\n }else{\\r\\n jQuery(\\'.second\\').text(secs);\\r\\n }\\r\\n }, 100)})();\\r\\n\u003c/script\u003e\\r\\n\u003c/div\u003e\\r\\n\u003c/div\u003e\\r\\n\\r\\n\u003cscript\u003e\\r\\n/* $(\".guan-intro\").mouseover(function(){\\r\\n\\t$(this).parents(\".guan-wrap\").siblings().show();\\r\\n}).mouseleave(function(){\\r\\n\\t$(this).parents(\".guan-wrap\").siblings().hide();\\r\\n}); */\\r\\n\\r\\nfunction showDiv(size,obj){\\r\\n\\tif(size\u003e32){\\r\\n\\t\\t$(obj).parents(\".guan-wrap\").siblings().show();\\r\\n\\t}\\r\\n}\\r\\nfunction hideDiv(size,obj){\\r\\n\\tif(size\u003e32){\\r\\n\\t\\t$(obj).parents(\".guan-wrap\").siblings().hide();\\r\\n\\t}\\r\\n}\\r\\n\u003c/script\u003e\\r\\n\u003c/body\u003e\\r\\n\u003c/html\u003e' 可以看到，访问https://bj.esf.fang.com/house/i31/时网页重定向去了\"https://bj.esf.fang.com/house/i31/?rfss=1-a9078821a1b90b2a70-34\"这个网址，我们尝试获取这个网址的response。 scrapy shell https://bj.esf.fang.com/house/i31/?rfss=1-a9078821a1b90b2a70-34 \u003e\u003e\u003eresponse.xpath(\"//dl[@dataflag='bg']\") [\u003cSelector xpath=\"//dl[@dataflag='bg']\" data='\u003cdl class=\"clearfix\" dataflag=\"bg\" da...'\u003e, \u003cSelector xpath=\"//dl[@dataflag='bg']\" data='\u003cdl class=\"clearfix\" dataflag=\"bg\" da...'\u003e, \u003cSelector xpath=\"//dl[@dataflag='bg']\" data='\u003cdl class=\"clearfix\" dataflag=\"bg\" da...'\u003e, \u003cSelector xpath=\"//dl[@dataflag='bg']\" data='\u003cdl class=\"clearfix\" dataflag=\"bg\" da...'\u003e, \u003cSelector ……] 抓取成功了，后续将想法在中间件中实现。 ","date":"2022-04-09","objectID":"/scrapy/:5:1","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#矛与盾重定向"},{"categories":["tutorial"],"content":" Item.py #二手房页面的Item class EsfItem(scrapy.Item): # 省份 province = scrapy.Field() # 城市 city = scrapy.Field() #二手房的标题 title = scrapy.Field() # 二手房详细信息的链接 esfhousenews_url = scrapy.Field() # 户型 room_type = scrapy.Field() # 面积 area = scrapy.Field() # 楼层 floor = scrapy.Field() # 朝向 orientation = scrapy.Field() # 建筑时间 build_year = scrapy.Field() # 小区名字 village_name = scrapy.Field() # 地址 address = scrapy.Field() # 总价 total = scrapy.Field() #每平米的价格 unit = scrapy.Field() ","date":"2022-04-09","objectID":"/scrapy/:5:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#itempy"},{"categories":["tutorial"],"content":" esf.py import scrapy import re from scrapy_fang.items import UrlItem, EsfItem class EsfSpider(scrapy.Spider): # 别名，后续用来其启动爬虫 name = 'esf' # 允许爬取的页面 allowed_domains = ['fang.com'] # 启动爬取的页面 base_url = 'https://bj.esf.fang.com/house/i3%d/' offset = 1 start_urls = ['https://bj.esf.fang.com'] # 总页数，解析赋值 page_num = 1 get_page_num_flag = True # Scrapy框架默认调用这个方法。response.body是html源码 def parse(self, response): # 初次执行会进入，后续不进入 if self.get_page_num_flag: # 获取总页码 page_num_text = response.xpath(\"//div[@class='page_al']/span/text()\")[-1].get() # 将不是数字的替换为空 page_num = re.sub('\\D', '', page_num_text) page_num = int(page_num) self.page_num = page_num self.get_page_num_flag = False print( \"-------------------------------------------------对于二手房页面的解析--------------------------------------------------\") dls = response.xpath(\"//dl[@class='clearfix']\") for dl in dls: item = EsfItem() item['province'] = '直辖市' item['city'] = '北京' item['build_year'] = '' # 二手房标题 title = dl.xpath(\".//h4[@class='clearfix']/a/@title\").get() item['title'] = title # 二手房详情链接 esfhousenews_url_text = dl.xpath(\".//h4[@class='clearfix']/a/@href\").get() esfhousenews_url = response.urljoin(esfhousenews_url_text) # print(esfhousenews_url) item['esfhousenews_url'] = esfhousenews_url # 二手房信息 lists = \"\".join(dl.xpath(\".//p[@class='tel_shop']/text()\").getall()).split() for list in lists: if '室' in list: # 户型 item['room_type'] = list elif '㎡' in list: # 面积 item['area'] = list elif '层' in list: # 楼层 item['floor'] = list elif '向' in list: # 朝向 item['orientation'] = list elif '年' in list: # 建筑时间 item['build_year'] = list # 小区名字 village_name = dl.xpath(\".//p[@class='add_shop']/a/@title\").get() item['village_name'] = village_name # 地址 address = dl.xpath(\".//p[@class='add_shop']/span/text()\").get() item['address'] = address # 总价 total_price = dl.xpath(\".//dd[@class='price_right']/span[@class='red']/b/text()\").get() item['total'] = total_price unit_price = dl.xpath(\".//dd[@class='price_right']/span[not(@class)]/text()\").get() # 每平方米的价格 item['unit'] = unit_price yield item # 下一页的链接 if self.offset \u003c self.page_num: self.offset += 1 next_link = self.base_url % self.offset # 跳到下一页 yield scrapy.Request(url=next_link, callback=self.parse) ","date":"2022-04-09","objectID":"/scrapy/:5:3","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#esfpy"},{"categories":["tutorial"],"content":" middlewares.py import requests import random from scrapy.http import HtmlResponse # 模拟浏览器请求头 class UserAgentDownloadMiddleware(object): User_Agents = [ 'Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0', 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.13; ko; rv:1.9.1b2) Gecko/2008' '1201 Firefox/60.0', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)' ' Chrome/70.0.3538.77 Safari/537.36', 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome' '/44.0.2403.155 Safari/537.36', ] # 随机请求头 def process_request(self, request, spider): User_Agent = random.choice(self.User_Agents) request.headers['User-Agent'] = User_Agent return None # 处理URL重定向问题 class ProcessRedirectionMiddleware(object): User_Agents = [ 'Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0', 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.13; ko; rv:1.9.1b2) Gecko/2008' '1201 Firefox/60.0', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)' ' Chrome/70.0.3538.77 Safari/537.36', 'Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.36 (KHTML like Gecko) Chrome' '/44.0.2403.155 Safari/537.36', ] def process_response(self, request, response, spider): User_Agent = random.choice(self.User_Agents) # 从response的body中提取重定向的网址 redirected_url = response.selector.xpath(\"//a[@class='btn-redir']//@href\").get() # 如果抓取成功，则说明进行了重定向，返回目标网址的response if redirected_url: response = requests.get(redirected_url, headers={'User_Agent': User_Agent}) # 构造scrapy的response对象。 return HtmlResponse(url=redirected_url, body=response.content) # 否则返回原response return response ","date":"2022-04-09","objectID":"/scrapy/:5:4","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#middlewarespy"},{"categories":["tutorial"],"content":" settings.py # 下载器中间件，添加请求头，处理重定向 DOWNLOADER_MIDDLEWARES = { 'scrapy_fang.middlewares.UserAgentDownloadMiddleware': 543, 'scrapy_fang.middlewares.ProcessRedirectionMiddleware': 553, } # 指定Pipline，本流程用于保存yield的item数据 ITEM_PIPELINES = { 'scrapy_fang.pipelines.ScrapyFangPipeline': 300, } # 允许爬虫缓存（不会重复请求目标页面，一次请求成功后，会存在缓存中，减小网站压力。） HTTPCACHE_ENABLED = True HTTPCACHE_EXPIRATION_SECS = 0 HTTPCACHE_DIR = 'httpcache' HTTPCACHE_IGNORE_HTTP_CODES = [] HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage' # 是否遵循robots协议 ROBOTSTXT_OBEY = False # Configure maximum concurrent requests performed by Scrapy (default: 16) CONCURRENT_REQUESTS = 32 # 延时最低为0 DOWNLOAD_DELAY = 0 ","date":"2022-04-09","objectID":"/scrapy/:5:5","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#settingspy"},{"categories":["tutorial"],"content":" pipelines.py from scrapy.exporters import CsvItemExporter class ScrapyFangPipeline(object): esf_info_fp = open('esf_info.csv', 'wb') esf_info_exporter = CsvItemExporter(esf_info_fp) def process_item(self, item, spider): self.esf_info_exporter.export_item(item) # 返回是必须的，告诉引擎item已经处理完成了，可以继续执行后面代码。 return item def close_spider(self, spider): self.esf_info_fp.close() ","date":"2022-04-09","objectID":"/scrapy/:5:6","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#pipelinespy"},{"categories":["tutorial"],"content":" 启动爬虫 scrapy crawl esf ","date":"2022-04-09","objectID":"/scrapy/:5:7","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#启动爬虫"},{"categories":["tutorial"],"content":" 获得爬得数据 ","date":"2022-04-09","objectID":"/scrapy/:5:8","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#获得爬得数据"},{"categories":["tutorial"],"content":" 多城市二手房详情页爬虫","date":"2022-04-09","objectID":"/scrapy/:6:0","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#多城市二手房详情页爬虫"},{"categories":["tutorial"],"content":" 完整爬取流程从城市页拿到各城市二手房链接页的基础URL 进入列表页拿到基本信息以及指向详情页的URL 从详情页获取有关二手房的详情信息 ","date":"2022-04-09","objectID":"/scrapy/:6:1","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#完整爬取流程"},{"categories":["tutorial"],"content":" 矛与盾由于网站的防护，房天下网站有一系列反爬机制，有些隐藏的很深，也是通过实践才发现的。本节主要介绍作者对于反爬验证的主要解决方法，robot协议，延时爬取这种就不在这里展开详细阐述了。 IP代理IP代理是爬虫的一个关键技术，通常情况下，网站会通过同一个IP地址的并发请求量来判断这是否为一个爬虫，因此，IP代理便可以使用其他代理IP进行请求的转发，来隐藏发出请求的原始IP，以达到持续爬取而不被检测到的目的。接下来介绍尝试过的两种IP代理方法。 IP代理池方法 简述：通过向代理商获取一定数量的IP作为代理池，每次请求使用代理池其中的一个IP进行转发。在请求时进行代理，创建IP代理中间件方法如下 class ProxyDownloaderMiddleware: def process_request(self, request, spider): proxy = pro.get_next_proxy() request.meta['proxy'] = \"http://%(proxy)s\" % {'proxy': proxy} print(\"*****************使用代理*****************\") print(\"***\", proxy, \"***\") print(\"*****************************************\") # 用户名密码认证(私密代理/独享代理) request.headers['Proxy-Authorization'] = basic_auth_header('a15179737600', 'p8x7284f') # 白名单认证可注释此行 return None 获取IP代理池程序（每过60s获取一次），从服务商获取到可用的IP列表之后，我们也可以自行进行可用性检测，以免所使用的代理IP无效。本人采用的方法是使用代理IP访问http://icanhazip.com，此网站会返回当前的IP地址，如果状态码为200表明成功访问，于是成功加入代理池。 # -- coding: utf-8 -- import time import threading import requests from scrapy import signals # 提取代理IP的api api_url = 'https://dps.kdlapi.com/api/getdps/?orderid=965208072230596\u0026num=10\u0026signature=bqcsohuyc0qcmz1446bn3jt234sbcezq\u0026pt=1\u0026format=json\u0026sep=1' foo = True class Proxy: def __init__(self, ): self.api_url = api_url self.username = 'a15179737600' self.password = 'p8x7284f' self._proxy_list = self.get_proxy_lis() def get_proxy_lis(self): print( \"-------------------------------------------------从代理商获取IP代理池--------------------------------------------------\") proxy_lis = requests.get(self.api_url).json().get('data').get('proxy_list') print( \"-------------------------------------------------获取成功，开始测试IP代理池--------------------------------------------------\") # 测试连通性 proxy_lis = [proxy for proxy in proxy_lis if self.verification(proxy)] # 可用代理列表长度为零则重新获取 while not len(proxy_lis): print( \"-------------------------------------------------可用IP代理获取失败，重新获取中------------\\ --------------------------------------\") proxy_lis = requests.get(self.api_url).json().get('data').get('proxy_list') print( \"-------------------------------------------------开始测试新获IP代理池--------------------------------------------------\") # 测试连通性 proxy_lis = [proxy for proxy in proxy_lis if self.verification(proxy)] print( \"-------------------------------------------------成功获得可用IP代理池（大小为%d）如下--------------------------------------------------\" % len(proxy_lis)) print(proxy_lis) self.index_ = 1 return proxy_lis def get_next_proxy(self): self.index_ += 1 proxy = self._proxy_list[self.index_ % len(self._proxy_list)] return proxy @property def proxy_list(self): return self._proxy_list @proxy_list.setter def proxy_list(self, list): self._proxy_list = list # 测试IP代理是否正常响应 def verification(self, proxy): head = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36', 'Connection': 'keep-alive'} '''http://icanhazip.com会返回当前的IP地址''' proxies = { \"http\": \"http://%(user)s:%(pwd)s@%(proxy)s/\" % {\"user\": self.username, \"pwd\": self.password, \"proxy\": proxy}, \"https\": \"http://%(user)s:%(pwd)s@%(proxy)s/\" % {\"user\": self.username, \"pwd\": self.password, \"proxy\": proxy} } try: p = requests.get('http://icanhazip.com', headers=head, proxies=proxies, timeout=2) if p.status_code == 200: return True return False except: return False pro = Proxy() print(pro.proxy_list) class MyExtend: def __init__(self, crawler): self.crawler = crawler # 将自定义方法绑定到scrapy信号上,使程序与spider引擎同步启动与关闭 # scrapy信号文档: https://www.osgeo.cn/scrapy/topics/signals.html # scrapy自定义拓展文档: https://www.osgeo.cn/scrapy/topics/extensions.html crawler.signals.connect(self.start, signals.engine_started) crawler.signals.connect(self.close, signals.spider_closed) @classmethod def from_crawler(cls, crawler): return cls(crawler) def start(self): t = threading.Thread(target=self.extract_proxy) t.start() def extract_proxy(self): while foo: # 设置每60秒提取一次ip time.sleep(60) pro.proxy_list = pro.get_proxy_lis() def close(self): global foo foo = False TUN隧道IP代理方法 隧道代理是通过代理商的链接直接获取一个代理IP，可以选择每次访问切换以及每几分钟切换IP等，使用较为便捷，以下是中间件的代理方法实现","date":"2022-04-09","objectID":"/scrapy/:6:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#矛与盾"},{"categories":["tutorial"],"content":" 矛与盾由于网站的防护，房天下网站有一系列反爬机制，有些隐藏的很深，也是通过实践才发现的。本节主要介绍作者对于反爬验证的主要解决方法，robot协议，延时爬取这种就不在这里展开详细阐述了。 IP代理IP代理是爬虫的一个关键技术，通常情况下，网站会通过同一个IP地址的并发请求量来判断这是否为一个爬虫，因此，IP代理便可以使用其他代理IP进行请求的转发，来隐藏发出请求的原始IP，以达到持续爬取而不被检测到的目的。接下来介绍尝试过的两种IP代理方法。 IP代理池方法 简述：通过向代理商获取一定数量的IP作为代理池，每次请求使用代理池其中的一个IP进行转发。在请求时进行代理，创建IP代理中间件方法如下 class ProxyDownloaderMiddleware: def process_request(self, request, spider): proxy = pro.get_next_proxy() request.meta['proxy'] = \"http://%(proxy)s\" % {'proxy': proxy} print(\"*****************使用代理*****************\") print(\"***\", proxy, \"***\") print(\"*****************************************\") # 用户名密码认证(私密代理/独享代理) request.headers['Proxy-Authorization'] = basic_auth_header('a15179737600', 'p8x7284f') # 白名单认证可注释此行 return None 获取IP代理池程序（每过60s获取一次），从服务商获取到可用的IP列表之后，我们也可以自行进行可用性检测，以免所使用的代理IP无效。本人采用的方法是使用代理IP访问http://icanhazip.com，此网站会返回当前的IP地址，如果状态码为200表明成功访问，于是成功加入代理池。 # -- coding: utf-8 -- import time import threading import requests from scrapy import signals # 提取代理IP的api api_url = 'https://dps.kdlapi.com/api/getdps/?orderid=965208072230596\u0026num=10\u0026signature=bqcsohuyc0qcmz1446bn3jt234sbcezq\u0026pt=1\u0026format=json\u0026sep=1' foo = True class Proxy: def __init__(self, ): self.api_url = api_url self.username = 'a15179737600' self.password = 'p8x7284f' self._proxy_list = self.get_proxy_lis() def get_proxy_lis(self): print( \"-------------------------------------------------从代理商获取IP代理池--------------------------------------------------\") proxy_lis = requests.get(self.api_url).json().get('data').get('proxy_list') print( \"-------------------------------------------------获取成功，开始测试IP代理池--------------------------------------------------\") # 测试连通性 proxy_lis = [proxy for proxy in proxy_lis if self.verification(proxy)] # 可用代理列表长度为零则重新获取 while not len(proxy_lis): print( \"-------------------------------------------------可用IP代理获取失败，重新获取中------------\\ --------------------------------------\") proxy_lis = requests.get(self.api_url).json().get('data').get('proxy_list') print( \"-------------------------------------------------开始测试新获IP代理池--------------------------------------------------\") # 测试连通性 proxy_lis = [proxy for proxy in proxy_lis if self.verification(proxy)] print( \"-------------------------------------------------成功获得可用IP代理池（大小为%d）如下--------------------------------------------------\" % len(proxy_lis)) print(proxy_lis) self.index_ = 1 return proxy_lis def get_next_proxy(self): self.index_ += 1 proxy = self._proxy_list[self.index_ % len(self._proxy_list)] return proxy @property def proxy_list(self): return self._proxy_list @proxy_list.setter def proxy_list(self, list): self._proxy_list = list # 测试IP代理是否正常响应 def verification(self, proxy): head = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36', 'Connection': 'keep-alive'} '''http://icanhazip.com会返回当前的IP地址''' proxies = { \"http\": \"http://%(user)s:%(pwd)s@%(proxy)s/\" % {\"user\": self.username, \"pwd\": self.password, \"proxy\": proxy}, \"https\": \"http://%(user)s:%(pwd)s@%(proxy)s/\" % {\"user\": self.username, \"pwd\": self.password, \"proxy\": proxy} } try: p = requests.get('http://icanhazip.com', headers=head, proxies=proxies, timeout=2) if p.status_code == 200: return True return False except: return False pro = Proxy() print(pro.proxy_list) class MyExtend: def __init__(self, crawler): self.crawler = crawler # 将自定义方法绑定到scrapy信号上,使程序与spider引擎同步启动与关闭 # scrapy信号文档: https://www.osgeo.cn/scrapy/topics/signals.html # scrapy自定义拓展文档: https://www.osgeo.cn/scrapy/topics/extensions.html crawler.signals.connect(self.start, signals.engine_started) crawler.signals.connect(self.close, signals.spider_closed) @classmethod def from_crawler(cls, crawler): return cls(crawler) def start(self): t = threading.Thread(target=self.extract_proxy) t.start() def extract_proxy(self): while foo: # 设置每60秒提取一次ip time.sleep(60) pro.proxy_list = pro.get_proxy_lis() def close(self): global foo foo = False TUN隧道IP代理方法 隧道代理是通过代理商的链接直接获取一个代理IP，可以选择每次访问切换以及每几分钟切换IP等，使用较为便捷，以下是中间件的代理方法实现","date":"2022-04-09","objectID":"/scrapy/:6:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#ip代理"},{"categories":["tutorial"],"content":" 矛与盾由于网站的防护，房天下网站有一系列反爬机制，有些隐藏的很深，也是通过实践才发现的。本节主要介绍作者对于反爬验证的主要解决方法，robot协议，延时爬取这种就不在这里展开详细阐述了。 IP代理IP代理是爬虫的一个关键技术，通常情况下，网站会通过同一个IP地址的并发请求量来判断这是否为一个爬虫，因此，IP代理便可以使用其他代理IP进行请求的转发，来隐藏发出请求的原始IP，以达到持续爬取而不被检测到的目的。接下来介绍尝试过的两种IP代理方法。 IP代理池方法 简述：通过向代理商获取一定数量的IP作为代理池，每次请求使用代理池其中的一个IP进行转发。在请求时进行代理，创建IP代理中间件方法如下 class ProxyDownloaderMiddleware: def process_request(self, request, spider): proxy = pro.get_next_proxy() request.meta['proxy'] = \"http://%(proxy)s\" % {'proxy': proxy} print(\"*****************使用代理*****************\") print(\"***\", proxy, \"***\") print(\"*****************************************\") # 用户名密码认证(私密代理/独享代理) request.headers['Proxy-Authorization'] = basic_auth_header('a15179737600', 'p8x7284f') # 白名单认证可注释此行 return None 获取IP代理池程序（每过60s获取一次），从服务商获取到可用的IP列表之后，我们也可以自行进行可用性检测，以免所使用的代理IP无效。本人采用的方法是使用代理IP访问http://icanhazip.com，此网站会返回当前的IP地址，如果状态码为200表明成功访问，于是成功加入代理池。 # -- coding: utf-8 -- import time import threading import requests from scrapy import signals # 提取代理IP的api api_url = 'https://dps.kdlapi.com/api/getdps/?orderid=965208072230596\u0026num=10\u0026signature=bqcsohuyc0qcmz1446bn3jt234sbcezq\u0026pt=1\u0026format=json\u0026sep=1' foo = True class Proxy: def __init__(self, ): self.api_url = api_url self.username = 'a15179737600' self.password = 'p8x7284f' self._proxy_list = self.get_proxy_lis() def get_proxy_lis(self): print( \"-------------------------------------------------从代理商获取IP代理池--------------------------------------------------\") proxy_lis = requests.get(self.api_url).json().get('data').get('proxy_list') print( \"-------------------------------------------------获取成功，开始测试IP代理池--------------------------------------------------\") # 测试连通性 proxy_lis = [proxy for proxy in proxy_lis if self.verification(proxy)] # 可用代理列表长度为零则重新获取 while not len(proxy_lis): print( \"-------------------------------------------------可用IP代理获取失败，重新获取中------------\\ --------------------------------------\") proxy_lis = requests.get(self.api_url).json().get('data').get('proxy_list') print( \"-------------------------------------------------开始测试新获IP代理池--------------------------------------------------\") # 测试连通性 proxy_lis = [proxy for proxy in proxy_lis if self.verification(proxy)] print( \"-------------------------------------------------成功获得可用IP代理池（大小为%d）如下--------------------------------------------------\" % len(proxy_lis)) print(proxy_lis) self.index_ = 1 return proxy_lis def get_next_proxy(self): self.index_ += 1 proxy = self._proxy_list[self.index_ % len(self._proxy_list)] return proxy @property def proxy_list(self): return self._proxy_list @proxy_list.setter def proxy_list(self, list): self._proxy_list = list # 测试IP代理是否正常响应 def verification(self, proxy): head = { 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36', 'Connection': 'keep-alive'} '''http://icanhazip.com会返回当前的IP地址''' proxies = { \"http\": \"http://%(user)s:%(pwd)s@%(proxy)s/\" % {\"user\": self.username, \"pwd\": self.password, \"proxy\": proxy}, \"https\": \"http://%(user)s:%(pwd)s@%(proxy)s/\" % {\"user\": self.username, \"pwd\": self.password, \"proxy\": proxy} } try: p = requests.get('http://icanhazip.com', headers=head, proxies=proxies, timeout=2) if p.status_code == 200: return True return False except: return False pro = Proxy() print(pro.proxy_list) class MyExtend: def __init__(self, crawler): self.crawler = crawler # 将自定义方法绑定到scrapy信号上,使程序与spider引擎同步启动与关闭 # scrapy信号文档: https://www.osgeo.cn/scrapy/topics/signals.html # scrapy自定义拓展文档: https://www.osgeo.cn/scrapy/topics/extensions.html crawler.signals.connect(self.start, signals.engine_started) crawler.signals.connect(self.close, signals.spider_closed) @classmethod def from_crawler(cls, crawler): return cls(crawler) def start(self): t = threading.Thread(target=self.extract_proxy) t.start() def extract_proxy(self): while foo: # 设置每60秒提取一次ip time.sleep(60) pro.proxy_list = pro.get_proxy_lis() def close(self): global foo foo = False TUN隧道IP代理方法 隧道代理是通过代理商的链接直接获取一个代理IP，可以选择每次访问切换以及每几分钟切换IP等，使用较为便捷，以下是中间件的代理方法实现","date":"2022-04-09","objectID":"/scrapy/:6:2","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#selenium模拟拖动绕过验证码"},{"categories":["tutorial"],"content":" 主要类定义 爬虫文件 class EsfSpider(scrapy.Spider): # 别名，后续用来其启动爬虫 name = 'esf' # 允许爬取的页面 allowed_domains = ['fang.com'] # 启动爬取的页面 start_urls = ['https://www.fang.com/SoufunFamily.htm'] # 获取随机请求头 get_random_UserAgent = GetRandomUserAgent() # 重定向链接获取方法 def get_redirected_url(self, url): head = self.get_random_UserAgent.agent_head() # 构造scrapy的response对象。 req_response = requests.get(url, headers=head) response = HtmlResponse(url=url, body=req_response.content) # 处理重定向 redirected_url = response.xpath(\"//a[@class='btn-redir']//@href\").get() # 如果抓取成功，则说明进行了重定向，发送一个新的请求而不是将得到的响应发给爬虫程序 if redirected_url: print(\"重定向\", redirected_url, \"*\" * 200) return redirected_url return url # Scrapy框架默认调用这个方法。response.body是html源码 def parse(self, response): df = pd.read_csv('esf_info.csv') esfhousenews_urls = df[['esfhousenews_url']].values provinces = df[['province']].values cities = df[['city']].values k = 1 for u, p, c in zip(esfhousenews_urls, provinces, cities): if k \u003e 5: break esfhousenews_url = u[0] province = p[0] city = c[0] try: directed_url = self.get_redirected_url(esfhousenews_url) yield scrapy.Request(url=directed_url, callback=self.parse_esfhouseInfo, meta={\"info\": (province, city, directed_url)}, dont_filter=True) except Exception as ex: print(\"捕获异常%s\" % ex) # 对于二手房详情页面的解析 def parse_esfhouseInfo(self, response): province, city, esf_link = response.meta.get(\"info\") print( \"-------------------------------------------------对于%s二手房 %s 详情页的解析--------------------------------------------------\" % ( province + ' ' + city, esf_link)) # 验证码 response_url = str(response.url) print(response) house_info_divs = response.xpath( \"//div[contains(@class, 'tr-line clearfix')]//div[contains(@class,'trl-item1')]\") if 'captcha' in response_url and not house_info_divs: with open('captcha.txt', 'a', encoding='utf-8') as f: f.write(response_url + '\\n') print('处理验证码', '*' * 200) deal_captcha = DealCaptcha(parse_target='EsfInfoItem', url=response_url) item = deal_captcha.run() if item: item['province'] = province item['city'] = city item['esf_link'] = esf_link return item elif house_info_divs: print('链接解析成功了', '*' * 200) item = EsfInfoItem() item['province'] = province item['city'] = city item['esf_link'] = esf_link title = response.xpath(\"//span[@class='tit_text']//text()\").get() if title: item['title'] = title.strip() if len(house_info_divs) == 6: # 户型 room_type = house_info_divs[0].xpath(\".//div[@class='tt']//text()\").get() if room_type: item['room_type'] = room_type.strip() # 面积 area = house_info_divs[1].xpath(\".//div[@class='tt']//text()\").get() if area: item['area'] = area.strip() # 单价 unit_price = house_info_divs[2].xpath(\".//div[@class='tt']//text()\").get() if unit_price: item['unit_price'] = unit_price.strip() # 朝向 orientation = house_info_divs[3].xpath(\".//div[@class='tt']//text()\").get() if orientation: item['orientation'] = orientation.strip() # 楼层 floor_info = house_info_divs[4].xpath(\".//div[@class='tt']//text()\").get() floor_info_a = house_info_divs[4].xpath(\".//div[@class='tt']//a//text()\").get() floor_type = house_info_divs[4].xpath(\".//div[@class='font14']//text()\").get() if floor_info_a and floor_type: item['floor'] = floor_info_a.strip() + ',' + floor_type.strip() elif floor_info and floor_type: item['floor'] = floor_info.strip() + ',' + floor_type.strip() # 装修 decoration = house_info_divs[5].xpath(\".//div[@class='tt']//text()\").get() decoration_a = house_info_divs[5].xpath(\".//div[@class='tt']/a//text()\").get() if decoration_a: item['decoration'] = decoration_a.strip() elif decoration: item['decoration'] = decoration.strip() # 总价 total_price = response.xpath(\"//div[contains(@class, 'price_esf')]/i//text()\").get() if total_price: item['total_price'] = total_price.strip() + '万元' # 房源信息 xpath_house_info_lis = response.xpath( \"//div[contains(@class, 'content-item fydes-item')]//div[contains(@class, 'cont clearfix')]//div\") for div in xpath_house_info_lis: if len(div.xpath(\".//span//text()\")) == 2: # 建筑年代 if div.xpath(\".//span//text()\")[0].get().strip()","date":"2022-04-09","objectID":"/scrapy/:6:3","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#主要类定义"},{"categories":["tutorial"],"content":" 主要类定义 爬虫文件 class EsfSpider(scrapy.Spider): # 别名，后续用来其启动爬虫 name = 'esf' # 允许爬取的页面 allowed_domains = ['fang.com'] # 启动爬取的页面 start_urls = ['https://www.fang.com/SoufunFamily.htm'] # 获取随机请求头 get_random_UserAgent = GetRandomUserAgent() # 重定向链接获取方法 def get_redirected_url(self, url): head = self.get_random_UserAgent.agent_head() # 构造scrapy的response对象。 req_response = requests.get(url, headers=head) response = HtmlResponse(url=url, body=req_response.content) # 处理重定向 redirected_url = response.xpath(\"//a[@class='btn-redir']//@href\").get() # 如果抓取成功，则说明进行了重定向，发送一个新的请求而不是将得到的响应发给爬虫程序 if redirected_url: print(\"重定向\", redirected_url, \"*\" * 200) return redirected_url return url # Scrapy框架默认调用这个方法。response.body是html源码 def parse(self, response): df = pd.read_csv('esf_info.csv') esfhousenews_urls = df[['esfhousenews_url']].values provinces = df[['province']].values cities = df[['city']].values k = 1 for u, p, c in zip(esfhousenews_urls, provinces, cities): if k \u003e 5: break esfhousenews_url = u[0] province = p[0] city = c[0] try: directed_url = self.get_redirected_url(esfhousenews_url) yield scrapy.Request(url=directed_url, callback=self.parse_esfhouseInfo, meta={\"info\": (province, city, directed_url)}, dont_filter=True) except Exception as ex: print(\"捕获异常%s\" % ex) # 对于二手房详情页面的解析 def parse_esfhouseInfo(self, response): province, city, esf_link = response.meta.get(\"info\") print( \"-------------------------------------------------对于%s二手房 %s 详情页的解析--------------------------------------------------\" % ( province + ' ' + city, esf_link)) # 验证码 response_url = str(response.url) print(response) house_info_divs = response.xpath( \"//div[contains(@class, 'tr-line clearfix')]//div[contains(@class,'trl-item1')]\") if 'captcha' in response_url and not house_info_divs: with open('captcha.txt', 'a', encoding='utf-8') as f: f.write(response_url + '\\n') print('处理验证码', '*' * 200) deal_captcha = DealCaptcha(parse_target='EsfInfoItem', url=response_url) item = deal_captcha.run() if item: item['province'] = province item['city'] = city item['esf_link'] = esf_link return item elif house_info_divs: print('链接解析成功了', '*' * 200) item = EsfInfoItem() item['province'] = province item['city'] = city item['esf_link'] = esf_link title = response.xpath(\"//span[@class='tit_text']//text()\").get() if title: item['title'] = title.strip() if len(house_info_divs) == 6: # 户型 room_type = house_info_divs[0].xpath(\".//div[@class='tt']//text()\").get() if room_type: item['room_type'] = room_type.strip() # 面积 area = house_info_divs[1].xpath(\".//div[@class='tt']//text()\").get() if area: item['area'] = area.strip() # 单价 unit_price = house_info_divs[2].xpath(\".//div[@class='tt']//text()\").get() if unit_price: item['unit_price'] = unit_price.strip() # 朝向 orientation = house_info_divs[3].xpath(\".//div[@class='tt']//text()\").get() if orientation: item['orientation'] = orientation.strip() # 楼层 floor_info = house_info_divs[4].xpath(\".//div[@class='tt']//text()\").get() floor_info_a = house_info_divs[4].xpath(\".//div[@class='tt']//a//text()\").get() floor_type = house_info_divs[4].xpath(\".//div[@class='font14']//text()\").get() if floor_info_a and floor_type: item['floor'] = floor_info_a.strip() + ',' + floor_type.strip() elif floor_info and floor_type: item['floor'] = floor_info.strip() + ',' + floor_type.strip() # 装修 decoration = house_info_divs[5].xpath(\".//div[@class='tt']//text()\").get() decoration_a = house_info_divs[5].xpath(\".//div[@class='tt']/a//text()\").get() if decoration_a: item['decoration'] = decoration_a.strip() elif decoration: item['decoration'] = decoration.strip() # 总价 total_price = response.xpath(\"//div[contains(@class, 'price_esf')]/i//text()\").get() if total_price: item['total_price'] = total_price.strip() + '万元' # 房源信息 xpath_house_info_lis = response.xpath( \"//div[contains(@class, 'content-item fydes-item')]//div[contains(@class, 'cont clearfix')]//div\") for div in xpath_house_info_lis: if len(div.xpath(\".//span//text()\")) == 2: # 建筑年代 if div.xpath(\".//span//text()\")[0].get().strip()","date":"2022-04-09","objectID":"/scrapy/:6:3","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#爬虫文件"},{"categories":["tutorial"],"content":" 主要类定义 爬虫文件 class EsfSpider(scrapy.Spider): # 别名，后续用来其启动爬虫 name = 'esf' # 允许爬取的页面 allowed_domains = ['fang.com'] # 启动爬取的页面 start_urls = ['https://www.fang.com/SoufunFamily.htm'] # 获取随机请求头 get_random_UserAgent = GetRandomUserAgent() # 重定向链接获取方法 def get_redirected_url(self, url): head = self.get_random_UserAgent.agent_head() # 构造scrapy的response对象。 req_response = requests.get(url, headers=head) response = HtmlResponse(url=url, body=req_response.content) # 处理重定向 redirected_url = response.xpath(\"//a[@class='btn-redir']//@href\").get() # 如果抓取成功，则说明进行了重定向，发送一个新的请求而不是将得到的响应发给爬虫程序 if redirected_url: print(\"重定向\", redirected_url, \"*\" * 200) return redirected_url return url # Scrapy框架默认调用这个方法。response.body是html源码 def parse(self, response): df = pd.read_csv('esf_info.csv') esfhousenews_urls = df[['esfhousenews_url']].values provinces = df[['province']].values cities = df[['city']].values k = 1 for u, p, c in zip(esfhousenews_urls, provinces, cities): if k \u003e 5: break esfhousenews_url = u[0] province = p[0] city = c[0] try: directed_url = self.get_redirected_url(esfhousenews_url) yield scrapy.Request(url=directed_url, callback=self.parse_esfhouseInfo, meta={\"info\": (province, city, directed_url)}, dont_filter=True) except Exception as ex: print(\"捕获异常%s\" % ex) # 对于二手房详情页面的解析 def parse_esfhouseInfo(self, response): province, city, esf_link = response.meta.get(\"info\") print( \"-------------------------------------------------对于%s二手房 %s 详情页的解析--------------------------------------------------\" % ( province + ' ' + city, esf_link)) # 验证码 response_url = str(response.url) print(response) house_info_divs = response.xpath( \"//div[contains(@class, 'tr-line clearfix')]//div[contains(@class,'trl-item1')]\") if 'captcha' in response_url and not house_info_divs: with open('captcha.txt', 'a', encoding='utf-8') as f: f.write(response_url + '\\n') print('处理验证码', '*' * 200) deal_captcha = DealCaptcha(parse_target='EsfInfoItem', url=response_url) item = deal_captcha.run() if item: item['province'] = province item['city'] = city item['esf_link'] = esf_link return item elif house_info_divs: print('链接解析成功了', '*' * 200) item = EsfInfoItem() item['province'] = province item['city'] = city item['esf_link'] = esf_link title = response.xpath(\"//span[@class='tit_text']//text()\").get() if title: item['title'] = title.strip() if len(house_info_divs) == 6: # 户型 room_type = house_info_divs[0].xpath(\".//div[@class='tt']//text()\").get() if room_type: item['room_type'] = room_type.strip() # 面积 area = house_info_divs[1].xpath(\".//div[@class='tt']//text()\").get() if area: item['area'] = area.strip() # 单价 unit_price = house_info_divs[2].xpath(\".//div[@class='tt']//text()\").get() if unit_price: item['unit_price'] = unit_price.strip() # 朝向 orientation = house_info_divs[3].xpath(\".//div[@class='tt']//text()\").get() if orientation: item['orientation'] = orientation.strip() # 楼层 floor_info = house_info_divs[4].xpath(\".//div[@class='tt']//text()\").get() floor_info_a = house_info_divs[4].xpath(\".//div[@class='tt']//a//text()\").get() floor_type = house_info_divs[4].xpath(\".//div[@class='font14']//text()\").get() if floor_info_a and floor_type: item['floor'] = floor_info_a.strip() + ',' + floor_type.strip() elif floor_info and floor_type: item['floor'] = floor_info.strip() + ',' + floor_type.strip() # 装修 decoration = house_info_divs[5].xpath(\".//div[@class='tt']//text()\").get() decoration_a = house_info_divs[5].xpath(\".//div[@class='tt']/a//text()\").get() if decoration_a: item['decoration'] = decoration_a.strip() elif decoration: item['decoration'] = decoration.strip() # 总价 total_price = response.xpath(\"//div[contains(@class, 'price_esf')]/i//text()\").get() if total_price: item['total_price'] = total_price.strip() + '万元' # 房源信息 xpath_house_info_lis = response.xpath( \"//div[contains(@class, 'content-item fydes-item')]//div[contains(@class, 'cont clearfix')]//div\") for div in xpath_house_info_lis: if len(div.xpath(\".//span//text()\")) == 2: # 建筑年代 if div.xpath(\".//span//text()\")[0].get().strip()","date":"2022-04-09","objectID":"/scrapy/:6:3","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#管道文件"},{"categories":["tutorial"],"content":" 主要类定义 爬虫文件 class EsfSpider(scrapy.Spider): # 别名，后续用来其启动爬虫 name = 'esf' # 允许爬取的页面 allowed_domains = ['fang.com'] # 启动爬取的页面 start_urls = ['https://www.fang.com/SoufunFamily.htm'] # 获取随机请求头 get_random_UserAgent = GetRandomUserAgent() # 重定向链接获取方法 def get_redirected_url(self, url): head = self.get_random_UserAgent.agent_head() # 构造scrapy的response对象。 req_response = requests.get(url, headers=head) response = HtmlResponse(url=url, body=req_response.content) # 处理重定向 redirected_url = response.xpath(\"//a[@class='btn-redir']//@href\").get() # 如果抓取成功，则说明进行了重定向，发送一个新的请求而不是将得到的响应发给爬虫程序 if redirected_url: print(\"重定向\", redirected_url, \"*\" * 200) return redirected_url return url # Scrapy框架默认调用这个方法。response.body是html源码 def parse(self, response): df = pd.read_csv('esf_info.csv') esfhousenews_urls = df[['esfhousenews_url']].values provinces = df[['province']].values cities = df[['city']].values k = 1 for u, p, c in zip(esfhousenews_urls, provinces, cities): if k \u003e 5: break esfhousenews_url = u[0] province = p[0] city = c[0] try: directed_url = self.get_redirected_url(esfhousenews_url) yield scrapy.Request(url=directed_url, callback=self.parse_esfhouseInfo, meta={\"info\": (province, city, directed_url)}, dont_filter=True) except Exception as ex: print(\"捕获异常%s\" % ex) # 对于二手房详情页面的解析 def parse_esfhouseInfo(self, response): province, city, esf_link = response.meta.get(\"info\") print( \"-------------------------------------------------对于%s二手房 %s 详情页的解析--------------------------------------------------\" % ( province + ' ' + city, esf_link)) # 验证码 response_url = str(response.url) print(response) house_info_divs = response.xpath( \"//div[contains(@class, 'tr-line clearfix')]//div[contains(@class,'trl-item1')]\") if 'captcha' in response_url and not house_info_divs: with open('captcha.txt', 'a', encoding='utf-8') as f: f.write(response_url + '\\n') print('处理验证码', '*' * 200) deal_captcha = DealCaptcha(parse_target='EsfInfoItem', url=response_url) item = deal_captcha.run() if item: item['province'] = province item['city'] = city item['esf_link'] = esf_link return item elif house_info_divs: print('链接解析成功了', '*' * 200) item = EsfInfoItem() item['province'] = province item['city'] = city item['esf_link'] = esf_link title = response.xpath(\"//span[@class='tit_text']//text()\").get() if title: item['title'] = title.strip() if len(house_info_divs) == 6: # 户型 room_type = house_info_divs[0].xpath(\".//div[@class='tt']//text()\").get() if room_type: item['room_type'] = room_type.strip() # 面积 area = house_info_divs[1].xpath(\".//div[@class='tt']//text()\").get() if area: item['area'] = area.strip() # 单价 unit_price = house_info_divs[2].xpath(\".//div[@class='tt']//text()\").get() if unit_price: item['unit_price'] = unit_price.strip() # 朝向 orientation = house_info_divs[3].xpath(\".//div[@class='tt']//text()\").get() if orientation: item['orientation'] = orientation.strip() # 楼层 floor_info = house_info_divs[4].xpath(\".//div[@class='tt']//text()\").get() floor_info_a = house_info_divs[4].xpath(\".//div[@class='tt']//a//text()\").get() floor_type = house_info_divs[4].xpath(\".//div[@class='font14']//text()\").get() if floor_info_a and floor_type: item['floor'] = floor_info_a.strip() + ',' + floor_type.strip() elif floor_info and floor_type: item['floor'] = floor_info.strip() + ',' + floor_type.strip() # 装修 decoration = house_info_divs[5].xpath(\".//div[@class='tt']//text()\").get() decoration_a = house_info_divs[5].xpath(\".//div[@class='tt']/a//text()\").get() if decoration_a: item['decoration'] = decoration_a.strip() elif decoration: item['decoration'] = decoration.strip() # 总价 total_price = response.xpath(\"//div[contains(@class, 'price_esf')]/i//text()\").get() if total_price: item['total_price'] = total_price.strip() + '万元' # 房源信息 xpath_house_info_lis = response.xpath( \"//div[contains(@class, 'content-item fydes-item')]//div[contains(@class, 'cont clearfix')]//div\") for div in xpath_house_info_lis: if len(div.xpath(\".//span//text()\")) == 2: # 建筑年代 if div.xpath(\".//span//text()\")[0].get().strip()","date":"2022-04-09","objectID":"/scrapy/:6:3","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#item文件"},{"categories":["tutorial"],"content":" 爬虫程序运行结果示例 解析示例 入口页解析示例 列表页解析示例 详情页解析示例 验证码通过示例 OpenCV目标图像示例 ","date":"2022-04-09","objectID":"/scrapy/:6:4","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#爬虫程序运行结果示例"},{"categories":["tutorial"],"content":" 爬虫程序运行结果示例 解析示例 入口页解析示例 列表页解析示例 详情页解析示例 验证码通过示例 OpenCV目标图像示例 ","date":"2022-04-09","objectID":"/scrapy/:6:4","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#解析示例"},{"categories":["tutorial"],"content":" 参考资料 https://www.bilibili.com/video/BV1jx411b7E3?p=1 ","date":"2022-04-09","objectID":"/scrapy/:0:0","series":null,"tags":["Scrapy2.6"],"title":"Scrapy实战案例——房天下","uri":"/scrapy/#参考资料"},{"categories":[],"content":" 诗集 ","date":"2022-04-04","objectID":"/collection/:0:0","series":[],"tags":[],"title":"收藏","uri":"/collection/#诗集"},{"categories":[],"content":" 外","date":"2022-04-04","objectID":"/collection/:1:0","series":[],"tags":[],"title":"收藏","uri":"/collection/#外"},{"categories":[],"content":" 近 As I Began to Love Myself“As I began to love myself I found that anguish and emotional suffering are only warning signs that I was living against my own truth. Today, I know, this is “AUTHENTICITY”. As I began to love myself I understood how much it can offend somebody if I try to force my desires on this person, even though I knew the time was not right and the person was not ready for it, and even though this person was me. Today I call it “RESPECT”. As I began to love myself I stopped craving for a different life, and I could see that everything that surrounded me was inviting me to grow. Today I call it “MATURITY”. As I began to love myself I understood that at any circumstance, I am in the right place at the right time, and everything happens at the exactly right moment. So I could be calm. Today I call it “SELF-CONFIDENCE”. As I began to love myself I quit stealing my own time, and I stopped designing huge projects for the future. Today, I only do what brings me joy and happiness, things I love to do and that make my heart cheer, and I do them in my own way and in my own rhythm. Today I call it “SIMPLICITY”. As I began to love myself I freed myself of anything that is no good for my health – food, people, things, situations, and everything that drew me down and away from myself. At first I called this attitude a healthy egoism. Today I know it is “LOVE OF ONESELF”. As I began to love myself I quit trying to always be right, and ever since I was wrong less of the time. Today I discovered that is “MODESTY”. As I began to love myself I refused to go on living in the past and worrying about the future. Now, I only live for the moment, where everything is happening. Today I live each day, day by day, and I call it “FULFILLMENT”. As I began to love myself I recognized that my mind can disturb me and it can make me sick. But as I connected it to my heart, my mind became a valuable ally. Today I call this connection “WISDOM OF THE HEART”. We no longer need to fear arguments, confrontations or any kind of problems with ourselves or others. Even stars collide, and out of their crashing new worlds are born. Today I know “THAT IS LIFE”!” ― Charlie Chaplin 当我开始爱自己当我开始爱自己，我发现痛苦和情感的折磨仅仅是提醒的信号，提醒我，自己正在背离着真实的自己而活。如今，我懂得了，这叫做，真实。 当我开始爱自己，我懂得如果我试着将我的意志强加于人，这将是何等的冒犯，即便在这时我知道，时机尚未成熟，而他，也尚未做好准备，即便这个人，就是我。今天，我把它叫做，尊重。 当我开始爱自己，我不再渴求一个不一样的人生，并且我能够明白，所有发生在我身边的事，都是对我成长的邀约。如今，我把它叫做，成熟。 当我开始爱自己，我明白无论在何时，我都处于正确的时间在正确的地点，所有一切发生的时机都恰到好处。如今，我称之为，自信。 当我开始爱自己，我不再浪费自己的时间，并停止对未来远大烙饼的设计。如今，我仅仅做能够为我带来愉悦和幸福的事情，我乐意去做，并能够使我的心为之鼓舞的事情，用我自己的方式，保持我自己的节奏。今天，我称之为，简单。 当我开始爱自己，我使自己远离所有有害健康的东西——食物，人，事情，场景，以及一切拖累我，背离真我的东西。起初，我把它叫做健康的利己主义。如今我懂得，这是 自爱。 当我开始爱自己，我停止尝试永不失败的行为，而自此我反而更少犯错了。如今，我称之为，谦逊。 当我开始爱自己，我拒绝活在过去或对未来满心焦虑。现在，我只活在当下，一切正在发生的地方。今天，我过好每一天，日复一日，而我称之为，富足。 当我开始爱自己，我意识到，我的思想可以扰乱我，并能使我失去活力。但当我将它与我的心灵相连， 我的思绪摇身一变，成为了我宝贵的盟友。今天我称这种连结为，心灵的智慧。 我们再也无需害怕争执，对抗，或者其他何种与我们自己或他人之间的问题。即便是行星碰撞，也会在他们的撞击过后，催生出一个全新的世界。今天我明白了，这就是生活！ ——查理·卓别林 (dp0d译) ","date":"2022-04-04","objectID":"/collection/:1:1","series":[],"tags":[],"title":"收藏","uri":"/collection/#近"},{"categories":[],"content":" 近 As I Began to Love Myself“As I began to love myself I found that anguish and emotional suffering are only warning signs that I was living against my own truth. Today, I know, this is “AUTHENTICITY”. As I began to love myself I understood how much it can offend somebody if I try to force my desires on this person, even though I knew the time was not right and the person was not ready for it, and even though this person was me. Today I call it “RESPECT”. As I began to love myself I stopped craving for a different life, and I could see that everything that surrounded me was inviting me to grow. Today I call it “MATURITY”. As I began to love myself I understood that at any circumstance, I am in the right place at the right time, and everything happens at the exactly right moment. So I could be calm. Today I call it “SELF-CONFIDENCE”. As I began to love myself I quit stealing my own time, and I stopped designing huge projects for the future. Today, I only do what brings me joy and happiness, things I love to do and that make my heart cheer, and I do them in my own way and in my own rhythm. Today I call it “SIMPLICITY”. As I began to love myself I freed myself of anything that is no good for my health – food, people, things, situations, and everything that drew me down and away from myself. At first I called this attitude a healthy egoism. Today I know it is “LOVE OF ONESELF”. As I began to love myself I quit trying to always be right, and ever since I was wrong less of the time. Today I discovered that is “MODESTY”. As I began to love myself I refused to go on living in the past and worrying about the future. Now, I only live for the moment, where everything is happening. Today I live each day, day by day, and I call it “FULFILLMENT”. As I began to love myself I recognized that my mind can disturb me and it can make me sick. But as I connected it to my heart, my mind became a valuable ally. Today I call this connection “WISDOM OF THE HEART”. We no longer need to fear arguments, confrontations or any kind of problems with ourselves or others. Even stars collide, and out of their crashing new worlds are born. Today I know “THAT IS LIFE”!” ― Charlie Chaplin 当我开始爱自己当我开始爱自己，我发现痛苦和情感的折磨仅仅是提醒的信号，提醒我，自己正在背离着真实的自己而活。如今，我懂得了，这叫做，真实。 当我开始爱自己，我懂得如果我试着将我的意志强加于人，这将是何等的冒犯，即便在这时我知道，时机尚未成熟，而他，也尚未做好准备，即便这个人，就是我。今天，我把它叫做，尊重。 当我开始爱自己，我不再渴求一个不一样的人生，并且我能够明白，所有发生在我身边的事，都是对我成长的邀约。如今，我把它叫做，成熟。 当我开始爱自己，我明白无论在何时，我都处于正确的时间在正确的地点，所有一切发生的时机都恰到好处。如今，我称之为，自信。 当我开始爱自己，我不再浪费自己的时间，并停止对未来远大烙饼的设计。如今，我仅仅做能够为我带来愉悦和幸福的事情，我乐意去做，并能够使我的心为之鼓舞的事情，用我自己的方式，保持我自己的节奏。今天，我称之为，简单。 当我开始爱自己，我使自己远离所有有害健康的东西——食物，人，事情，场景，以及一切拖累我，背离真我的东西。起初，我把它叫做健康的利己主义。如今我懂得，这是 自爱。 当我开始爱自己，我停止尝试永不失败的行为，而自此我反而更少犯错了。如今，我称之为，谦逊。 当我开始爱自己，我拒绝活在过去或对未来满心焦虑。现在，我只活在当下，一切正在发生的地方。今天，我过好每一天，日复一日，而我称之为，富足。 当我开始爱自己，我意识到，我的思想可以扰乱我，并能使我失去活力。但当我将它与我的心灵相连， 我的思绪摇身一变，成为了我宝贵的盟友。今天我称这种连结为，心灵的智慧。 我们再也无需害怕争执，对抗，或者其他何种与我们自己或他人之间的问题。即便是行星碰撞，也会在他们的撞击过后，催生出一个全新的世界。今天我明白了，这就是生活！ ——查理·卓别林 (dp0d译) ","date":"2022-04-04","objectID":"/collection/:1:1","series":[],"tags":[],"title":"收藏","uri":"/collection/#as-i-began-to-love-myself"},{"categories":[],"content":" 近 As I Began to Love Myself“As I began to love myself I found that anguish and emotional suffering are only warning signs that I was living against my own truth. Today, I know, this is “AUTHENTICITY”. As I began to love myself I understood how much it can offend somebody if I try to force my desires on this person, even though I knew the time was not right and the person was not ready for it, and even though this person was me. Today I call it “RESPECT”. As I began to love myself I stopped craving for a different life, and I could see that everything that surrounded me was inviting me to grow. Today I call it “MATURITY”. As I began to love myself I understood that at any circumstance, I am in the right place at the right time, and everything happens at the exactly right moment. So I could be calm. Today I call it “SELF-CONFIDENCE”. As I began to love myself I quit stealing my own time, and I stopped designing huge projects for the future. Today, I only do what brings me joy and happiness, things I love to do and that make my heart cheer, and I do them in my own way and in my own rhythm. Today I call it “SIMPLICITY”. As I began to love myself I freed myself of anything that is no good for my health – food, people, things, situations, and everything that drew me down and away from myself. At first I called this attitude a healthy egoism. Today I know it is “LOVE OF ONESELF”. As I began to love myself I quit trying to always be right, and ever since I was wrong less of the time. Today I discovered that is “MODESTY”. As I began to love myself I refused to go on living in the past and worrying about the future. Now, I only live for the moment, where everything is happening. Today I live each day, day by day, and I call it “FULFILLMENT”. As I began to love myself I recognized that my mind can disturb me and it can make me sick. But as I connected it to my heart, my mind became a valuable ally. Today I call this connection “WISDOM OF THE HEART”. We no longer need to fear arguments, confrontations or any kind of problems with ourselves or others. Even stars collide, and out of their crashing new worlds are born. Today I know “THAT IS LIFE”!” ― Charlie Chaplin 当我开始爱自己当我开始爱自己，我发现痛苦和情感的折磨仅仅是提醒的信号，提醒我，自己正在背离着真实的自己而活。如今，我懂得了，这叫做，真实。 当我开始爱自己，我懂得如果我试着将我的意志强加于人，这将是何等的冒犯，即便在这时我知道，时机尚未成熟，而他，也尚未做好准备，即便这个人，就是我。今天，我把它叫做，尊重。 当我开始爱自己，我不再渴求一个不一样的人生，并且我能够明白，所有发生在我身边的事，都是对我成长的邀约。如今，我把它叫做，成熟。 当我开始爱自己，我明白无论在何时，我都处于正确的时间在正确的地点，所有一切发生的时机都恰到好处。如今，我称之为，自信。 当我开始爱自己，我不再浪费自己的时间，并停止对未来远大烙饼的设计。如今，我仅仅做能够为我带来愉悦和幸福的事情，我乐意去做，并能够使我的心为之鼓舞的事情，用我自己的方式，保持我自己的节奏。今天，我称之为，简单。 当我开始爱自己，我使自己远离所有有害健康的东西——食物，人，事情，场景，以及一切拖累我，背离真我的东西。起初，我把它叫做健康的利己主义。如今我懂得，这是 自爱。 当我开始爱自己，我停止尝试永不失败的行为，而自此我反而更少犯错了。如今，我称之为，谦逊。 当我开始爱自己，我拒绝活在过去或对未来满心焦虑。现在，我只活在当下，一切正在发生的地方。今天，我过好每一天，日复一日，而我称之为，富足。 当我开始爱自己，我意识到，我的思想可以扰乱我，并能使我失去活力。但当我将它与我的心灵相连， 我的思绪摇身一变，成为了我宝贵的盟友。今天我称这种连结为，心灵的智慧。 我们再也无需害怕争执，对抗，或者其他何种与我们自己或他人之间的问题。即便是行星碰撞，也会在他们的撞击过后，催生出一个全新的世界。今天我明白了，这就是生活！ ——查理·卓别林 (dp0d译) ","date":"2022-04-04","objectID":"/collection/:1:1","series":[],"tags":[],"title":"收藏","uri":"/collection/#当我开始爱自己"},{"categories":null,"content":" 本站版权本站内容欢迎大家转载和分享，与此同时，请在内容上下文注明作者，并将链接指向本站，谢谢。 ","date":"2022-04-03","objectID":"/copyright/:1:0","series":null,"tags":null,"title":"关于版权","uri":"/copyright/#本站版权"},{"categories":null,"content":" 侵权问题如内容对您存在侵权，博主在此表示歉意，请附相关证明文件联系到我，本人会第一时间进行删帖。 邮箱 draftpaperofgod@gmail.com If the content is infringing to you, sincerely apologizes. Please do contact me with the relevant supporting documents, and I will delete the post as soon as possible. email draftpaperofgod@gmail.com ","date":"2022-04-03","objectID":"/copyright/:2:0","series":null,"tags":null,"title":"关于版权","uri":"/copyright/#侵权问题"},{"categories":[],"content":"要访问内网资源，内网资源缺少公网ip被墙，现在我们假设需要在内网主机上运行python，我们需要在公网访问到它，并考虑安全性。 ","date":"2022-03-27","objectID":"/frp_by_docker/:0:0","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#"},{"categories":[],"content":" 原理图 ","date":"2022-03-27","objectID":"/frp_by_docker/:0:0","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#原理图"},{"categories":[],"content":" 准备工作 一台公网服务器，以及本地内网需要穿透到的主机 一个域名，本次配置中的web服务需要注册域名 下载好了最新版本的frp 发布，如：博主使用的是v0.41.0👇(现阶段github被墙，科学上网或使用其他方式下载) wget https://github.com/fatedier/frp/releases/download/v0.41.0/frp_0.41.0_linux_amd64.tar.gz ​ 目录树👇 保证在公网服务器上经过设置的所有端口不被防火墙限制。 ","date":"2022-03-27","objectID":"/frp_by_docker/:0:0","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#准备工作"},{"categories":[],"content":" 搭建流程","date":"2022-03-27","objectID":"/frp_by_docker/:0:0","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#搭建流程"},{"categories":[],"content":" 1.web服务基础版 如果需要搭建ssh服务等操作类似，Demo详情见官方文档，此处博主仅进行web服务搭建。 ","date":"2022-03-27","objectID":"/frp_by_docker/:1:0","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#1web服务基础版"},{"categories":[],"content":" 含有公网ip的服务器端将刚刚下载的文件解压到~/frps下👇 mkdir ~/frp tar zxvf frp_0.41.0_linux_amd64.tar.gz -C ~/frp cd ~/frp/frp_0.41.0_linux_amd64 然后编辑文件frps.ini，写入以下内容👇 [common] ;公网服务器与内网主机通信的端口 bind_port = 1234 ;访问公网服务器端口 vhost_http_port = 4231 启动frps服务👇 ./frps -c ./frps.ini 如显示frps started successfully即为成功，端口也会写明 ","date":"2022-03-27","objectID":"/frp_by_docker/:1:1","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#含有公网ip的服务器端"},{"categories":[],"content":" 内网主机端将刚刚下载的文件解压到~/frps下👇 mkdir ~/frp tar zxvf frp_0.41.0_linux_amd64.tar.gz -C ~/frp cd ~/frp/frp_0.41.0_linux_amd64 然后编辑文件frpc.ini，写入以下内容👇 [common] ;你的服务器ip server_addr = xx.xx.xx.xx ;公网服务器与主机通信的端口(和服务器端的vhost_http_port一致) server_port = 1234 [web] type = http ;你想要映射到的内网主机端口，常用的有22（ssh端口）、443等 local_port = 8888 ;你的服务器域名 custom_domains = xxxx.com 启动frpc服务👇 ./frpc -c ./frpc.ini 服务器端反应👇 浏览器访问公网域名http://xxxx.com:vhost_http_port端口号👇 ","date":"2022-03-27","objectID":"/frp_by_docker/:1:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#内网主机端"},{"categories":[],"content":" 2.web服务Docker版（Jupyter lab款） 动机：使用docker提供服务，外网访问的服务在内网主机的docker内运行，相当于做了一层内网隔离，较为安全。 配置条件同基础版 一台公网服务器，以及本地内网需要穿透到的主机 一个域名，本次配置中的web服务需要注册域名 下载好了最新版本的frp 发布，如：博主使用的是v0.41.0👇(现阶段github被墙，科学上网或使用其他方式下载) wget https://github.com/fatedier/frp/releases/download/v0.41.0/frp_0.41.0_linux_amd64.tar.gz 保证在公网服务器上经过设置的所有端口不被防火墙限制。 另：需要在内网主机安装docker环境 官网安装地址 安装需要用来使用https利用仓库的包 sudo apt-get update sudo apt-get install \\ ca-certificates \\ curl \\ gnupg \\ lsb-release 设置稳定存储库 echo \\ \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list \u003e /dev/null 安装docker engine sudo apt-get install docker ","date":"2022-03-27","objectID":"/frp_by_docker/:2:0","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#2web服务docker版jupyter-lab款"},{"categories":[],"content":" 含有公网ip的服务器端(与基础版一致)将刚刚下载的文件解压到~/frps下👇 mkdir ~/frp tar zxvf frp_0.41.0_linux_amd64.tar.gz -C ~/frp cd ~/frp/frp_0.41.0_linux_amd64 然后编辑文件frps.ini，写入以下内容👇 [common] ;公网服务器与内网主机通信的端口 bind_port = 1234 ;访问公网服务器端口 vhost_http_port = 4231 启动frps服务👇 ./frps -c ./frps.ini 如显示frps started successfully即为成功，端口也会写明 ","date":"2022-03-27","objectID":"/frp_by_docker/:2:1","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#含有公网ip的服务器端与基础版一致"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#内网主机端-1"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#到dockerhub寻找合适的仓库dockerhubhttpshubdockercom"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#进入容器内"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#源更新"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#安装基础工具"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#安装python依赖"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#在容器内安装pyenv"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#重启shell"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#配置pyenv环境"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#使用pyenv安装python3410并建立jupyterlab虚拟环境这是一个很推荐使用的python环境管理软件之前的博客中有介绍使用传送门httpsdp0dcnenv_managemente9a39fe794a8e696b9e6b395"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#然后退出但不关闭这个容器使用快捷键ctrlqp"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#继续配置我们的frpc"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#frpcini里的配置和基础版的一样"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#附加篇按需取用"},{"categories":[],"content":" 内网主机端 到dockerhub寻找合适的仓库dockerhub这里选择unbuntu作为我们的基础镜像 sudo docker pull python sudo docker pull ubuntu 本文中docker 使用的一些命令 拉取镜像命令 sudo docker pull 镜像名 通过Dockerfile构建镜像命令 sudo docker build -t 查看镜像命令 目标镜像名 . sudo docker images 删除镜像命令 sudo docker image rm 镜像名 进入运行中的docker 容器，退出时不关闭容器 sudo docker exec -it 容器名 /bin/sh 其中 bin/sh是指令运行器在镜像中的位置，可以使用以下命令查看 sudo docker inspect 容器名 或者 镜像名 删除所有容器 sudo docker container prune sudo docker system prune –volumes sudo docker system prune –all 停用所有并删除所有，上面那条命令删除不了在运行的容器(需要多重复运行回车几次就干净了) sudo docker stop $(sudo docker ps -q) \u0026 sudo docker rm $(sudo docker ps -aq) 使用该镜像创建容器，起名为jupyterlab，并将8888端口映射到内网主机 sudo docker run -p 8888:8888 --name jupyterlab -itd python 注:-d可以省略来调试无法启动的信息，正式使用在本次配置中需要加上。 查看是否在运行 sudo docker ps -a Up表示在正常运行 进入容器内 sudo docker exec -it jupyterlab /bin/bash 源更新 apt-get update 安装基础工具 apt install git vim curl nodejs 安装python依赖 由于选择的是python仓库，不需要考虑python的依赖问题 在容器内安装pyenv curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash 重启shell exec $SHELL 配置pyenv环境 echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv init -)\"' \u003e\u003e ~/.bashrc echo 'eval \"$(pyenv virtualenv-init -)\"' \u003e\u003e ~/.bashrc source ~/.bashrc 使用pyenv安装python3.4.10并建立jupyterlab虚拟环境，这是一个很推荐使用的python环境管理软件，之前的博客中有介绍使用。(传送门) 查看可安装的python版本命令 pyenv install -l pyenv install 3.10.4 pyenv virtualenv 3.10.4 jupyterlab pyenv global jupyterlab pip install update pip pip install jupyterlab 生成jupyter lab的登陆口令 jupyter lab password 输入你想要的口令，如：2933194thg309rgbn13495y1tb1 启动jupyter lab, 让它在后台运行 ～ nohup jupyter lab --allow-root --no-browser --ip '*' --port '8888' \u003e ~/.jupyter/jupyterlab.log 2\u003e\u00261 \u0026 查看它的运行状态 ps -a 乖乖在后台呆着 更好一点，设置jupyter服务并让它开机自启动 增加system配置，创建并编辑名为jupyterlab的服务 sudo vi /etc/systemd/system/jupyterlab.service 注意ExecStart一定要精确到jupyterlab的执行文件，后续可以自行添加指定端口等命令-p等，指定启动路径直接跟在后面就好了。 [Unit] Description=jupyterlab service After=network.target [Service] Type=simple User=oliver ExecStart=/home/oliver/.pyenv/versions/jupyterlab/bin/jupyter-lab p 9001 /home/ Restart=on-failure RestartPreventExitStatus=23 [Install] WantedBy=multi-user.target 然后保存退出，继续执行如下命令 sudo systemctl daemon-reload sudo systemctl enable jupyterlab sudo systemctl start jupyterlab sudo systemctl status jupyterlab ok，服务设置完成 然后退出但不关闭这个容器，使用快捷键Ctrl+Q+P 浏览器访问127.0.0.1:8888 输入刚刚的密码2933194thg309rgbn13495y1tb1 进来了，证明这时我们的jupyter还在容器后台乖乖呆着。，因为端口映射出来了，访问在主机8888端口相当于访问docker容器的8888端口。 继续配置我们的frpc frpc.ini里的配置和基础版的一样 cd ~/frp/frp_0.41.0_linux_amd64/ ./frpc -c ./frpc.ini ok,服务起来了 在网址中输入我们的http://域名xxx.com:4321 ok,访问到了 然后输入我们的jupyter口令2933194thg309rgbn13495y1tb1 OK，通了 大功告成～ 附加篇，按需取用https://blog.csdn.net/weixin_43975924/article/details/104046790 SSH映射情景1：用户通过主机A访问C 待续…… ","date":"2022-03-27","objectID":"/frp_by_docker/:2:2","series":[],"tags":["linux"],"title":"Linux下配置frp内网穿透","uri":"/frp_by_docker/#ssh映射"},{"categories":["tutorial"],"content":"说明 本文根据常见的读写进行方法阐述，本文实验环境基于python3.8，并需要提前告知各类读写方式的文件操作效果，如下列出： r : 读取文件，文件需要手动创建 w: 写入文件，若文件不存在则会自动创建再写入，会覆盖原文件 a : 写入文件，若文件不存在则会自动创建再写入，但不会覆盖原文件，而是追加在文件末尾 rb,wb： 分别于r, w类似，但是用于读写二进制文件，此类型可用于保存变量。 r+ : 可读、可写，文件需要手动创建，写操作时会覆盖原文件。 w+ : 可读，可写，文件不存在自动创建，会覆盖原文件。 a+ : 可读、可写，文件不存在自动创建，不会覆盖原文件，追加在末尾。 ","date":"2022-03-26","objectID":"/file_read_py/:0:0","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#"},{"categories":["tutorial"],"content":" csv格式文件","date":"2022-03-26","objectID":"/file_read_py/:1:0","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#csv格式文件"},{"categories":["tutorial"],"content":" csv库 列表读写 # 将以下两个列表作为两列写入CSV，用 ' ' 空格隔开： import csv list1=[1,2] list2=['1','2'] with open('filename.csv', \"w\", newline='', encoding='utf-8') as f1: writer = csv.writer(f1, delimiter=',') writer.writerows(zip(list1,list2)) print('文件写入完成……') # 作为两行呢 lis = [[1,2],['1','2']] with open('filename.csv', \"w\", newline='', encoding='utf-8') as f1: writer = csv.writer(f1, delimiter=',') for data in lis: writer.writerow(data) # 从上述写成的CSV中读出两个列表, 同样用 ' ' 空格隔开读取： import csv with open('filename.csv','r',encoding='utf-8') as f2: list1 = [] list2 = [] # 这里注意，之前int类型的列表写入和读取之后都会变为str类型的列表 reader = csv.reader(f2, delimiter = ',') for i in reader: list1.append(i[0]) list2.append(i[1]) print('文件读取完成……') # 其他模式 list = [[1,2,3],[4,5,6]] with open('filename.csv', \"w\", newline='', encoding='utf-8') as f1: writer = csv.writer(f1, delimiter=',') writer.writerows(list) print('文件写入完成……') ","date":"2022-03-26","objectID":"/file_read_py/:1:1","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#csv库"},{"categories":["tutorial"],"content":" csv库 列表读写 # 将以下两个列表作为两列写入CSV，用 ' ' 空格隔开： import csv list1=[1,2] list2=['1','2'] with open('filename.csv', \"w\", newline='', encoding='utf-8') as f1: writer = csv.writer(f1, delimiter=',') writer.writerows(zip(list1,list2)) print('文件写入完成……') # 作为两行呢 lis = [[1,2],['1','2']] with open('filename.csv', \"w\", newline='', encoding='utf-8') as f1: writer = csv.writer(f1, delimiter=',') for data in lis: writer.writerow(data) # 从上述写成的CSV中读出两个列表, 同样用 ' ' 空格隔开读取： import csv with open('filename.csv','r',encoding='utf-8') as f2: list1 = [] list2 = [] # 这里注意，之前int类型的列表写入和读取之后都会变为str类型的列表 reader = csv.reader(f2, delimiter = ',') for i in reader: list1.append(i[0]) list2.append(i[1]) print('文件读取完成……') # 其他模式 list = [[1,2,3],[4,5,6]] with open('filename.csv', \"w\", newline='', encoding='utf-8') as f1: writer = csv.writer(f1, delimiter=',') writer.writerows(list) print('文件写入完成……') ","date":"2022-03-26","objectID":"/file_read_py/:1:1","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#列表读写"},{"categories":["tutorial"],"content":" pandas库 读出多个文件并按列连接写入一个文件 import pandas as pd inputfile_csv_1='head-dep_context_1.csv' inputfile_csv_2='head-dep_context_2.csv' inputfile_csv_3='head-dep_context_3.csv' inputfile_csv_4='head-dep_context_4.csv' outputfile='4.csv' csv_1 = pd.read_csv(inputfile_csv_1) csv_2 = pd.read_csv(inputfile_csv_2) csv_3 = pd.read_csv(inputfile_csv_3) csv_4 = pd.read_csv(inputfile_csv_4) out_csv=pd.concat([csv_1,csv_2,csv_3,csv_4],axis =1) out_csv.to_csv(outputfile, index=False) 按列读入四个文件并按列合并： ","date":"2022-03-26","objectID":"/file_read_py/:1:2","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#pandas库"},{"categories":["tutorial"],"content":" pandas库 读出多个文件并按列连接写入一个文件 import pandas as pd inputfile_csv_1='head-dep_context_1.csv' inputfile_csv_2='head-dep_context_2.csv' inputfile_csv_3='head-dep_context_3.csv' inputfile_csv_4='head-dep_context_4.csv' outputfile='4.csv' csv_1 = pd.read_csv(inputfile_csv_1) csv_2 = pd.read_csv(inputfile_csv_2) csv_3 = pd.read_csv(inputfile_csv_3) csv_4 = pd.read_csv(inputfile_csv_4) out_csv=pd.concat([csv_1,csv_2,csv_3,csv_4],axis =1) out_csv.to_csv(outputfile, index=False) 按列读入四个文件并按列合并： ","date":"2022-03-26","objectID":"/file_read_py/:1:2","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#读出多个文件并按列连接写入一个文件"},{"categories":["tutorial"],"content":" txt格式文件注：txt格式文件可以用于存放字典。 # 向文件写入list1的字典vocab list1 = ['3','1','1','2','2'] vocab = sorted(set(list)) # 创建一个list1的字典，由于set每次顺序会改变，为了保证程序能够复现，这里可以排序一下。 with open('filename.txt', 'w',encoding = 'utf-8') as f1: for i in vocab: f1.write(i + '\\n') # 这里向文件每行写入vocab的一个词作为字典 print('字典写入完成……') # 从文件读取list1的字典vocab，逐行读取，一行一个词 vocab = [] with open('filename.txt', 'r', encoding = 'utf-8') as f2: lines = f2.readlines() for i in lines: vocab.append(i.strip('\\n')) # 去除每一行的换行符 print('字典读取完成……') ","date":"2022-03-26","objectID":"/file_read_py/:2:0","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#txt格式文件"},{"categories":["tutorial"],"content":" pickle格式文件注：pickle可以将很多格式的数据保存到一个文件中以二进制保存。重新加载之后还是之前的格式。此种方式通常用于保存生成花销较大的中间变量，以提升工作效率。 # 将var变量以二进制的形式写入文件,可以保存多种类型的变量： import pickle var = \"I am your friend.\" with open('filename.pickle', 'wb') as f1: pickle.dump(var, f1) print('变量写入完成……') # 从上述写成的pickle文件中以二进制读出var变量，变量类型读完后与保存时一致： import pickle with open('filename.pickle', 'rb') as f2: var = pickle.load(f2) print('变量读取完成……') ","date":"2022-03-26","objectID":"/file_read_py/:3:0","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#pickle格式文件"},{"categories":["tutorial"],"content":" json格式文件 # 写json文件，可以保存字典等类型的变量 import json var_dic = {'1st':'你好','2nd':'世界'} with open('filename.json', 'w') as f1: json.dump(var_dic,f1) # 如果保存的是中文，默认会转为ascii码存储，如果想以中文存储，则需要指定 with open('filename_ch.json', 'w', encoding='utf-8') as f: json.dump(var_dic,f,ensure_ascii=False,indent = 4) #读出json文件： with open('filename.json', 'r') as f2: var_dic = json.dump(f2) ","date":"2022-03-26","objectID":"/file_read_py/:4:0","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#json格式文件"},{"categories":["tutorial"],"content":" mat格式文件此种格式文件起初为matlab软件保存得到的，在python中也逐渐有了相关函数进行处理，其特点是可以压缩保存大矩阵文件，减小磁盘空间占用。 以下数据存取需要配套使用 大矩阵处理方式一 import scipy.io as scio import hdf5storage # 存储你的数据字典， def save(out_file_path: str,data: dict, do_compression: bool, large_array: bool): \"\"\" :param out_file_path: 需要保存到的数据路径 :param data: 你的数据字典 :param do_compression: 是否压缩存储 :return: 无 \"\"\" if not large_array: scio.savemat(out_file_path, data, do_compression=do_compression) else: hdf5storage.savemat(out_file_path, data, do_compression=do_compression) # 加载该数据 def save(file_path: str, large_array: bool): if not large_array: scio.loadmat(file_path) else: hdf5storage.loadmat(file_pathn) 以下数据存取需要配套使用 大矩阵处理方式二 import scipy.io as scio from scipy.sparse import csc_matrix # 存储你的数据字典， def save(out_file_path: str,data: dict, do_compression: bool, large_array: bool): \"\"\" :param out_file_path: 需要保存到的数据路径 :param data: 你的数据字典 :param do_compression: 是否压缩存储 :return: 无 \"\"\" if not large_array: scio.savemat(out_file_path, data, do_compression=do_compression) else: data['large_matrix_key'] = csc_matrix(data['large_matrix_key']) scio.savemat(out_file_path, data, do_compression=do_compression) # 加载该数据 def save(file_path: str, large_array: bool): if not large_array: scio.loadmat(file_path) else: scio.loadmat(file_path) data['large_matrix_key'] = data['large_matrix_key'].toarray() ","date":"2022-03-26","objectID":"/file_read_py/:5:0","series":[],"tags":["py"],"title":"Python中常见的文件读写","uri":"/file_read_py/#mat格式文件"},{"categories":null,"content":" 前言在Python工程中，特别是对于要复现一些项目时、需要测试特定环境下程序表现时、同一个服务器上多人进行跑程序时需要自己的独立环境时等，我们可能需要用到特定版本的库，而这时，我们可以使用虚拟环境安装依赖。这样依赖，我们就不会影响到主环境，诶这样就 不用重复卸载重装主环境， 也不会使得主环境日渐臃肿； 还可以安装多版本python（如经典的pyenv 包管理软件便可以实现）； 如有需要直接删掉虚拟环境，然后建新的即可。 由于系统配置的不确定性，建议友友们在linux下食用。 ","date":"2022-03-22","objectID":"/env_management/:0:0","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#前言"},{"categories":null,"content":" 环境管理途径介绍环境需要用到的包可以写入requirements.txt文件里，如果是从另一个工程环境切换过来，可以在那个环境中运行 pip freeze \u003e requirements.txt 然后在需要配置的环境中运行如下命令进行安装大部分需要的包 cat requirements.txt | xargs -n 1 pip install 此方法会忽略报错进行pip包的安装，会出现包的缺损，后续可以自行补上。 镜像源 pip -i https://pypi.tuna.tsinghua.edu.cn/simple 豆瓣：-i http://pypi.douban.com/simple/ --trusted-host pypi.douban.com 阿里云：-i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com # 设置全局 # 清华源 pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple ","date":"2022-03-22","objectID":"/env_management/:0:0","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#环境管理途径介绍"},{"categories":null,"content":" Conda 查看我们当前conda里有的环境 ，三种方式都可👇🏻 conda info --envs conda info -e conda env list 输出如下👇🏻 可以看到，我们目前只有一个base的基础环境。接下来我们试着查看一下该环境下的包列表👇🏻 ","date":"2022-03-22","objectID":"/env_management/:1:0","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#conda"},{"categories":null,"content":" Conda创建虚拟环境创建时指定 conda下的虚拟环境名字为env_name，python版本为3.6👇 conda create -n env_name python=3.6 或者，你也可以通过yml文件来构建自己想要的环境，比如你想要如下配置的环境（name指的是你的虚拟环境叫啥，与上条命令中的env_name效果一样）👇 那么，你则需要在当前目录创建一个yml后缀的文件，然后写入上图类似的你需要的环境内容，并运行如下命令👇 conda env create -f conda_env.yml 再次查看conda下的环境列表 可以看到创建了一个新的env_name环境。 ","date":"2022-03-22","objectID":"/env_management/:1:1","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#conda创建虚拟环境"},{"categories":null,"content":" Conda激活虚拟环境输入activate指令👇 conda activate env_name 可以看到 命令行前的括号内容已经改变(base)👉(env_name)，此时已经是基于该虚拟环境了。 然后呢，我们就可以进入这个环境乱装一通了，此处省略一万行操作…… 好了，这个环境的使命已经结束了，我们再把它删掉👇 conda deactivate conda remove -n env_name --all ","date":"2022-03-22","objectID":"/env_management/:1:2","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#conda激活虚拟环境"},{"categories":null,"content":" Python Virtualenv这是一款python自带的虚拟环境管理方案，那么我们为啥不用conda呢？conda多好，自带包又多，多省事。问题就是，有些情况下我们不需要任何一个多余的包，在工程上即便是多余一个包也是对存储空间的浪费，conda内部我不是很清楚还封装了什么，但总是给人一种很健壮的感觉，怕麻烦的话推荐conda，但是有洁癖的人推荐自己逐步搭建相关依赖。现在介绍的这款就是官方给的，诶，干净。需要注意的是，如果你需要在多个python版本上建立虚拟环境，那么比较麻烦，下节会说，建议直接移步下一节的pyenv。（Python Virtualenv官方文档#） 此方案中虚拟环境的创建方式和conda差不多，这里介绍指定环境路径的方法，其中env_path为你要保存该虚拟环境的路径 python -m venv env_path 然后可以激活此环境 source env_path/bin/activate 关闭环境 deactivate ","date":"2022-03-22","objectID":"/env_management/:2:0","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#python-virtualenv"},{"categories":null,"content":" Pyenv 推荐食用 Pyenv是一个开源的python环境管理软件（源码链接），它十分方便可以实现我所说的虚上加虚。诶？啥意思呢？如果我们把创建虚拟环境的环境称为底座，那么很显然，本地环境通常只能满足一两个python版本，那么我们在上一节的Python Virtualenv环境管理方案就只有一两个底座，它就是本机已经安装的python版本，当然，你也可以将多个不同版本的python装到不同的文件，然后加不同的启动命令比如python2.x和python3.x来使用，但这并不好管理，且较为麻烦。而且，就像之前说的，会使得对于有洁癖的友友很不好受，不能一揽全局。那么我们就可以使用Pyenv，创建虚拟环境，因为它可以用来安装不同版本的python，conda，miniconda等，写到这里博主忽然觉得Pyenv好像就是在套壳上述两种方法，但是便捷性提升很多，故我们就相当于创建了不同的底座，使用Pyenv来统一管理👇 ","date":"2022-03-22","objectID":"/env_management/:3:0","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#pyenv"},{"categories":null,"content":" 安装 Linux👇 curl -L https://github.com/pyenv/pyenv-installer/raw/master/bin/pyenv-installer | bash echo 'export PATH=\"$HOME/.pyenv/bin:$PATH\"' \u003e\u003e ~/.bashrc 离线部署 先使用能够科学上网的机器下载release https://github.com/pyenv/pyenv 然后放到需要部署的机器上上 tar -zxvf pyenv-2.3.2.tar.gz mv pyenv-2.3.2 ~/.pyenv mkdir ~/.pyenv/cache cat \u003e\u003e ~/.bashrc\u003c\u003c'EOF' ### pyenv ### export PYENV_ROOT=\"$HOME/.pyenv\" export PATH=\"$PYENV_ROOT/bin:$PATH\" export PATH=\"$PYENV_ROOT/shims:$PATH\" eval \"$(pyenv init -)\" EOF source ~/.bashrc pyenv --version pyenv 2.2.0 MacOS brew update brew install pyenv Windows pyenv官方没有支持windows，推荐使用pyenv-win（github地址） windows下推荐使用pip安装pyenv-win pip install pyenv-win --target %USERPROFILE%\\.pyenv 或者下载zip文件然后解压到%USERPROFILE%\\.pyenv\\pyenv-win下 ","date":"2022-03-22","objectID":"/env_management/:3:1","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#安装"},{"categories":null,"content":" 食用方法 查看可以安装的相应版本的环境（一般只有数字的选项默认指python环境） pyenv install -l 安装所需要的,如选择python3.6.15环境 pyenv install 3.6.15 借助镜像源安装 v=3.6.15; curl -L https://npm.taobao.org/mirrors/python/$v/Python-$v.tar.xz -o ~/.pyenv/cache/Python-$v.tar.xz; pyenv install $v 激活所需要的（此处可以直接激活前文中二级底座的虚拟环境） pyenv activate 3.6.15 暂时将某个环境应用到所有目录的命令行下 pyenv global 3.6.15 进入某个目录时，命令行自动切换到对应环境的方法 在这个目录新建一个名为.python-version的文件 在其中写入你希望激活的环境名称 退出环境 pyenv source deactivate 删除环境（一级底座） pyenv uninstall 3.6.15 基于某个环境建立虚拟环境并取虚拟环境名 可以先激活这个环境，然后在这个环境下创建，激活，退出，删除虚拟环境 也可以不激活，直接使用pyenv命令 在一级底座上创建虚拟环境 pyenv virtualenv 3.6.15 虚拟环境名 然后激活虚拟环境 pyenv activate 虚拟环境名 退出虚拟环境 pyenv deactivate 虚拟环境名 删除虚拟环境（二级底座） pyenv virtualenv-delete 虚拟环境名 ","date":"2022-03-22","objectID":"/env_management/:3:2","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#食用方法"},{"categories":null,"content":" 食用方法 查看可以安装的相应版本的环境（一般只有数字的选项默认指python环境） pyenv install -l 安装所需要的,如选择python3.6.15环境 pyenv install 3.6.15 借助镜像源安装 v=3.6.15; curl -L https://npm.taobao.org/mirrors/python/$v/Python-$v.tar.xz -o ~/.pyenv/cache/Python-$v.tar.xz; pyenv install $v 激活所需要的（此处可以直接激活前文中二级底座的虚拟环境） pyenv activate 3.6.15 暂时将某个环境应用到所有目录的命令行下 pyenv global 3.6.15 进入某个目录时，命令行自动切换到对应环境的方法 在这个目录新建一个名为.python-version的文件 在其中写入你希望激活的环境名称 退出环境 pyenv source deactivate 删除环境（一级底座） pyenv uninstall 3.6.15 基于某个环境建立虚拟环境并取虚拟环境名 可以先激活这个环境，然后在这个环境下创建，激活，退出，删除虚拟环境 也可以不激活，直接使用pyenv命令 在一级底座上创建虚拟环境 pyenv virtualenv 3.6.15 虚拟环境名 然后激活虚拟环境 pyenv activate 虚拟环境名 退出虚拟环境 pyenv deactivate 虚拟环境名 删除虚拟环境（二级底座） pyenv virtualenv-delete 虚拟环境名 ","date":"2022-03-22","objectID":"/env_management/:3:2","series":null,"tags":null,"title":"Python 工程环境管理","uri":"/env_management/#基于某个环境建立虚拟环境并取虚拟环境名"},{"categories":null,"content":" 反向传播算法作为一位机器学习领域的博主，第一篇博客肯定要从反向传播算法开始啦~ ","date":"2022-03-18","objectID":"/backpropagation/:0:0","series":null,"tags":null,"title":"反向传播算法","uri":"/backpropagation/#反向传播算法"},{"categories":null,"content":" 简介​ 来自维基百科 ​ 首先是反向传播算法的历史沿用👇 ​ 弗拉基米尔·瓦普尼克引用（Bryson, A.E.; W.F. Denham; S.E. Dreyfus. Optimal programming problems with inequality constraints. I: Necessary conditions for extremal solutions. AIAA J. 1, 11 (1963) 2544-2550）在他的书《支持向量机》中首次发表反向传播算法。在1969年Arthur E. Bryson和何毓琦将其描述为多级动态系统优化方法。直到1974年以后在神经网络的背景下应用，并由Paul Werbos[7]、David E. Rumelhart、杰弗里·辛顿和Ronald J. Williams[1][8]的著作，它才获得认可，并引发了一场人工神经网络的研究领域的“文艺复兴”。在21世纪初人们对其失去兴趣，但在2010年后又拥有了兴趣，如今可以通过GPU等大型现代运算器件用于训练更大的网络。例如在2013年，顶级语音识别器现在使用反向传播算法训练神经网络。 ​ 那么反向传播算法是啥？👇 ​ 反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会回馈给最佳化方法，用来更新权值以最小化损失函数。 ","date":"2022-03-18","objectID":"/backpropagation/:1:0","series":null,"tags":null,"title":"反向传播算法","uri":"/backpropagation/#简介"},{"categories":null,"content":" 前向传播推导","date":"2022-03-18","objectID":"/backpropagation/:2:0","series":null,"tags":null,"title":"反向传播算法","uri":"/backpropagation/#前向传播推导"},{"categories":null,"content":" a. 了解矩阵乘法假设👇 $$ \\textbf{A} = \\begin{bmatrix} 1 \u0026 2 \\\\ 3 \u0026 4 \\\\ \\end{bmatrix} \\quad \\textbf{B}=\\left[ \\begin{matrix} -1 \u0026 -2 \\\\ -3 \u0026 -4 \\\\ \\end{matrix} \\right] $$ 点积（通常省略 · 符号）计算如下👇 $$ \\textbf{AB} = \\left[ \\begin{matrix} 1 \\times(-1) + 2\\times(-3) \u0026 1\\times (-2) +2\\times(-4)\\\\ 3 \\times(-1) + 4\\times(-3) \u0026 3\\times(-2) +4 \\times(-4) \\end{matrix} \\right]= \\left[ \\begin{matrix} -7 \u0026 -10\\\\ -15\u0026 -22 \\end{matrix} \\right] $$ 逐元素乘法如下👇 $$ \\textbf{A}\\odot\\textbf{B} = \\left[ \\begin{matrix} 1 \\times(-1)\u0026 2\\times(-2)\\\\ 3 \\times(-3)\u0026 4 \\times(-4) \\end{matrix} \\right]= \\left[ \\begin{matrix} -1 \u0026 -4\\\\ -9\u0026 -16 \\end{matrix} \\right] $$ ","date":"2022-03-18","objectID":"/backpropagation/:2:1","series":null,"tags":null,"title":"反向传播算法","uri":"/backpropagation/#a-了解矩阵乘法"},{"categories":null,"content":" b. 了解连式法则如果将复合函数f(g(x))对x求导，需要使用链式法则， $$ \\frac{\\partial f(g(x))}{\\partial x} = \\frac{\\partial f(g(x))}{\\partial g(x)} \\frac{\\partial g(x)}{\\partial x} $$ 举个栗子， $$ \\frac{\\partial e^{-x}}{\\partial x} = \\frac{\\partial e^{-x}}{\\partial (-x)} \\frac{\\partial (-x)}{\\partial x}=e^{-x}\\cdot(-1) = -e^{-x} $$ 这也是神经网络根据结果来更新前面n层参数的基础。 ","date":"2022-03-18","objectID":"/backpropagation/:2:2","series":null,"tags":null,"title":"反向传播算法","uri":"/backpropagation/#b-了解连式法则"},{"categories":null,"content":" c.了解神经网络如下图所示，这是一个神经网络👇 可以看到，这个神经网络的隐藏层（除了第一层输入层和最后一层输出层之外都是隐藏层）与所有上一层的神经元都连接了，我们一般称这样的层为全连接层。可以看到，图中的神经网络较为复杂，接下来，我们对网络进行拆解至最小单元。 先看看最基本的神经元是如何进行内部计算的。👇 可以看到，其实不难，每个神经元接受到来自上一层的每一个输入（即上一层需要连接的输出乘以（上一层至当前层的权重矩阵）得到的$w_{j_i}^{[l]}$）值，并将这些值求和，然后加上本神经元的Bias，经过激活函数$\\sigma$得到此神经元的输出 $a_j^{[l]}$。 然后我们让神经元回归到神经网络中，看一下整个网络的前馈计算脉路👇 如上图所示，这就是一个基本的前馈神经网络计算过程。其中，$Z$代表的是本神经元的激活函数所接受的输入值，也就是本神经元接受的来自上一层各神经元的输出（如果不是全连接层，也有可能是上一层部分神经元的输出）乘以权重w，求和后，加上本神经元的Bias得到的值。 如果对这个过程不是特别了解，那我们以矩阵的形式写一次$Z$的计算范例。👇 对部分参数进行如下定义👇 $$ \\begin{array}{cc} w^{[1]}=\\left[\\begin{array}{ll} w_{11}^{[1]} \u0026 w_{12}^{[1]} \\\\ w_{21}^{[1]} \u0026 w_{22}^{[1]} \\\\ w_{31}^{[1]} \u0026 w_{32}^{[1]} \\end{array}\\right] \u0026 w^{[2]}=\\left[\\begin{array}{lll} w_{11}^{[2]} \u0026 w_{12}^{[2]} \u0026 w_{13}^{[2]} \\\\ w_{21}^{[2]} \u0026 w_{22}^{[2]} \u0026 w_{23}^{[2]} \\end{array}\\right] \\\\ b^{[1]}=\\left[\\begin{array}{c} b_{1}^{[1]} \\\\ b_{2}^{[1]} \\\\ b_{3}^{[1]} \\end{array}\\right] \u0026 b^{[2]}=\\left[\\begin{array}{c} b_{1}^{[2]} \\\\ b_{2}^{[2]} \\end{array}\\right] \\end{array} $$ 于是，图中两个隐藏层$Z$的计算如下👇 $$ \\begin{array}{c} z^{[1]}=\\left[\\begin{array}{ll} w_{11}^{[1]} \u0026 w_{12}^{[1]} \\\\ w_{21}^{[1]} \u0026 w_{22}^{[1]} \\\\ w_{31}^{[1]} \u0026 w_{32}^{[1]} \\end{array}\\right] \\cdot\\left[\\begin{array}{c} a_{1}^{[0]} \\\\ a_{2}^{[0]} \\end{array}\\right]+\\left[\\begin{array}{c} b_{1}^{[1]} \\\\ b_{2}^{[1]} \\\\ b_{3}^{[1]} \\end{array}\\right]=\\left[\\begin{array}{c} w_{11}^{[1]} a_{1}^{[0]}+w_{12}^{[1]} a_{2}^{[0]}+b_{1}^{[1]} \\\\ w_{21}^{[1]} a_{1}^{[0]}+w_{22}^{[1]} a_{2}^{[0]}+b_{2}^{[1]} \\\\ w_{31}^{[1]} a_{1}^{[0]}+w_{32}^{[1]} a_{2}^{[0]}+b_{3}^{[1]} \\end{array}\\right] \\\\ z^{[2]}=\\left[\\begin{array}{lll} w_{11}^{[2]} \u0026 w_{12}^{[2]} \u0026 w_{13}^{[2]} \\\\ w_{21}^{[2]} \u0026 w_{22}^{[2]} \u0026 w_{23}^{[2]} \\end{array}\\right] \\cdot\\left[\\begin{array}{c} a_{1}^{[1]} \\\\ a_{2}^{[1]} \\\\ a_{3}^{[1]} \\end{array}\\right]+\\left[\\begin{array}{l} b_{1}^{[2]} \\\\ b_{2}^{[2]} \\end{array}\\right]=\\left[\\begin{array}{l} w_{11}^{[2]} a_{1}^{[1]}+w_{12}^{[2]} a_{2}^{[1]}+w_{13}^{[2]} a_{3}^{[1]}+b_{1}^{[2]} \\\\ w_{21}^{[2]} a_{1}^{[1]}+w_{22}^{[2]} a_{2}^{[1]}+w_{23}^{[2]} a_{3}^{[1]}+b_{2}^{[2]} \\end{array}\\right] \\\\ \\end{array} $$ 可以表述成👇 $$ \\textbf{Z}^{[l]}=w^{[l]}\\cdot \\textbf{A}^{[l-1]} +b^{[l]} $$ 那么，最后一层的输出呢就是我们需要的预测值，对于这一个预测值，我们希望它与真实值不要有过大的偏差，这个时候，我们便需要一个度量方法来衡量预测值与目标值的偏差程度。通常，我们将这个程度量化为损失函数的函数值Loss，那么这个损失函数如何去定义呢？这就牵扯到你所需要的预测是什么样的，它与实际值的关系是怎样的了，以后有机会出一篇文章梳理一些常用的损失函数。现在我们使用一个最简单的度量方法，也成为$L_1$度量，即绝对差值度量法，定义$(\\hat{y},y)$为（网络预测值，真实值），那么函数$L(\\hat{y},y)$可以定义如下👇 $$ L(\\hat{y},y)=|\\hat{y}-y| $$ 那么，他输出的函数值就是我们所需要的偏差度量。我们都知道，神经网络需要进行学习，学习的是什么呢？其实就是一个分布，希望在给出条件$x$下，能够给出相应的映射$y$，即网络能够通过映射计算去拟合数据中的分布$p(x)$。这个拟合过程，也就是缩小偏差度量，那么如何缩小偏差呢？这时便出现了我们的主角，反向传播算法，这是一个通过输出来反馈信息的算法，也是神经网络学习的根本所在，它的全名其实是误差反向传播，即通过误差回传，让模型的参数共同调整去减小这个误差。 那么误差我们熟悉，就是两个值之间的差值，而当我们将这个差值视为函数差值的时候，他就可以与函数求导挂钩，回顾一下求导法则👇 $$ f'(x)=\\lim_{\\Delta x \\rightarrow0} \\frac{\\Delta y}{\\Delta x} $$ 可以看到，通俗的说，有了导数，我们就可以知道要将偏差多少$y$调整回来，需要多少的$x$。 那么问题又来了，对于那么多的网络参数，我们怎么去调整呢？ $$ \\begin{array}{cc} w^{[1]}=\\left[\\begin{array}{ll} w_{11}^{[1]} \u0026 w_{12}^{[1]} \\\\ w_{21}^{[1]} \u0026 w_{22}^{[1]} \\\\ w_{31}^{[1]} \u0026 w_{32}^{[1]} \\end{array}\\right] \\quad w^{[2]}=\\left[\\begin{array}{lll} w_{11}^{[2]} \u0026 w_{12}^{[2]} \u0026 w_{13}^{[2]} \\\\ w_{21}^{[2]} \u0026 w_{22}^{[2]} \u0026 w_{23}^{[2]} \\end{array}\\right] \\\\ b^{[1]}=\\left[\\begin{array}{c} b_{1}^{[1]} \\\\ b_{2}^{[1]} \\\\ b_{3}^{[1]} \\end{array}\\right] \u0026 b^{[2]}=\\left[\\begin{array}{c} b_{1}^{[2]} \\\\ b_{2}^{[2]} \\end{array}\\right] \\end{array} $$ 答案就是，偏导数👇 $$ f_x’(x,y)=\\frac{\\partial f}{\\partial x} $$ 偏导数是啥，偏导数就是有偏见的导数，只对特定的条件$x$感兴趣，也就是偏导的一方，而其他的视为常数，这样我们就能精准地对不同参数进行更新。 举个最简单的例子，设👇 $$ f(x,y)=xy $$ 那么如果对$x$偏导👇 $$ f_x(x,y)=y $$ 反之，若对$y$偏导👇 $$ f_y(x,y)=x $$ 那么，在神经网络中，偏导的概念就视为了梯度$\\nabla$，为啥叫梯度呢？梯度$\\nabla$的本意是一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，那么如果在某一个变量上需要对误差产生最大的影响，则需要沿着梯度$\\nabla$的方向，也就是偏导的方向。 $$ w:=w-a \\frac{\\partial J(w)}{\\partial w} $$ 图中是目标函数对变量$w$的梯度$\\nabla$，可以看到，梯度$\\nabla$在图线上表现为切线，很容易知道在这个方向上能够对目标函数的值带来最大的影响，那么变量$w$在当前时刻沿着这个方向变化便能为缩小误差做出最大贡献。 如果拓展到矩阵则容易得到矩阵梯度$\\nabla$矩阵，也就是每个元素的梯度啦。👇 $$ \\nabla_{\\textbf{A}}f(\\textbf{A})= \\begin","date":"2022-03-18","objectID":"/backpropagation/:2:3","series":null,"tags":null,"title":"反向传播算法","uri":"/backpropagation/#c了解神经网络"},{"categories":null,"content":" 误差反向传播为误差对神经元输入的值（激活前）$z$ 做如下假设👇 那么对于输出层👇 $$ \\delta_{j}^{[L]}=\\frac{\\partial L}{\\partial a_{j}^{[L]}} \\sigma^{\\prime}\\left(z_{j}^{[L]}\\right) \\quad \\delta^{[L]}=\\left[\\begin{array}{c} \\frac{\\partial L}{\\partial a_{1}^{L L}} \\\\ \\frac{\\partial L}{\\partial a_{2}^{[L]}} \\\\ \\vdots \\\\ \\frac{\\partial L}{\\partial a_{j}^{L L}} \\end{array}\\right] \\odot\\left[\\begin{array}{c} \\sigma^{\\prime}\\left(z_{1}^{[L]}\\right) \\\\ \\sigma^{\\prime}\\left(z_{2}^{[L]}\\right) \\\\ \\vdots \\\\ \\sigma^{\\prime}\\left(z_{j}^{[L]}\\right) \\end{array}\\right] \\quad \\delta^{[L]}=\\nabla_{a} L \\odot \\sigma^{\\prime}\\left(z^{[L]}\\right) $$ 对于隐藏层👇 $$ \\delta_{j}^{[l]}=\\sum_{k} w_{k j}^{[l+1]} \\delta_{k}^{[l+1]} \\sigma^{\\prime}\\left(z_{j}^{[l]}\\right) \\\\ \\delta^{[l]}=\\left[\\left[\\begin{array}{cccc} w_{11}^{[l]} \u0026 w_{12}^{[l]} \u0026 \\ldots \u0026 w_{1 k}^{[l]} \\\\ w_{21}^{[l]} \u0026 w_{22}^{[l]} \u0026 \\ldots \u0026 w_{2 k}^{[l]} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ w_{j 1}^{[l]} \u0026 w_{j 2}^{[l]} \u0026 \\ldots \u0026 w_{j k}^{[l]} \\end{array}\\right]\\left[\\begin{array}{c} \\delta_{1}^{[l+1]} \\\\ \\delta_{2}^{[l+1]} \\\\ \\vdots \\\\ \\delta_{k}^{[l+1]} \\end{array}\\right]\\right] \\odot\\left[\\begin{array}{c} \\sigma^{\\prime}\\left(z_{1}^{[l]}\\right) \\\\ \\sigma^{\\prime}\\left(z_{2}^{[l]}\\right) \\\\ \\vdots \\\\ \\sigma^{\\prime}\\left(z_{j}^{[l]}\\right) \\end{array}\\right] \\\\ (\\mathrm{j}, \\mathrm{k}) *(\\mathrm{k}, 1) \\odot(\\mathrm{j}, 1)=(\\mathrm{j}, 1) \\\\ \\delta^{[l]}=\\left[w^{[l+1]^{T}} \\delta^{[l+1]}\\right] \\odot \\sigma^{\\prime}\\left(z^{[l]}\\right) $$ 更新步👇 $$ \\begin{array}{c} \\frac{\\partial L}{\\partial b_{j}^{[l]}}=\\delta_{j}^{[l]} \\\\ \\frac{\\partial L}{\\partial w_{j k}^{[l]}}=a_{k}^{[l-1]} \\delta_{j}^{[l]} \\\\ \\frac{\\partial L}{\\partial b^{[l]}}=\\left[\\begin{array}{c} \\delta_{1}^{[l]} \\\\ \\delta_{2}^{[l]} \\\\ \\vdots \\\\ \\delta_{j}^{[l]} \\end{array}\\right]=\\delta^{[l]} \\\\ \\frac{\\partial L}{\\partial w^{[l]}}=\\left[\\begin{array}{c} \\delta_{1}^{[l]} \\\\ \\delta_{2}^{[l]} \\\\ \\vdots \\\\ \\delta_{j}^{[l]} \\end{array}\\right]\\left[\\begin{array}{lll} a_{1}^{[l]} \u0026 a_{2}^{[l]} \\ldots \u0026 a_{k}^{[l]} \\end{array}\\right]\\\\ (j, 1) *(1, k)=(j, k)\\\\ \\frac{\\partial L}{\\partial b^{[l]}}=\\delta^{[l]}\\\\ \\frac{\\partial L}{\\partial w^{[l]}}=\\delta^{[l]} a^{[l-1] T} \\end{array} $$ $$ \\begin{aligned} b_{j}^{[l]} \u0026 \\leftarrow b_{j}^{[l]}-\\alpha \\frac{\\partial L}{\\partial b_{j}^{l]}} \\\\ w_{j k}^{[l]} \u0026 \\leftarrow w_{j k}^{[l]}-\\alpha \\frac{\\partial L}{\\partial w_{j k}^{[l]}} \\\\ b^{[l]} \u0026 \\leftarrow b^{[l]}-\\alpha \\frac{\\partial L}{\\partial b^{[l]}} \\\\ w^{[l]} \u0026 \\leftarrow w^{[l]}-\\alpha \\frac{\\partial L}{\\partial w^{[l]}} \\end{aligned} $$ 图解反向传播过程，首先是输出层👇 误差梯度传向第一个隐藏层👇 👇 👇 👇 👇 👇 以此类推，反向传播的过程就结束啦~ ","date":"2022-03-18","objectID":"/backpropagation/:3:0","series":null,"tags":null,"title":"反向传播算法","uri":"/backpropagation/#误差反向传播"},{"categories":null,"content":" You are not connected to the Internet, only cached pages will be available. ","date":"0001-01-01","objectID":"/offline/:0:0","series":null,"tags":null,"title":"Offline","uri":"/offline/#"},{"categories":null,"content":" 某985硕士生在读 ","date":"0001-01-01","objectID":"/about/:0:0","series":null,"tags":null,"title":"关于dp0d","uri":"/about/#"},{"categories":null,"content":" 致敬博尔特 ","date":"0001-01-01","objectID":"/charge/:0:0","series":null,"tags":null,"title":"朋友，给自己充个电吧～","uri":"/charge/#致敬博尔特"}]