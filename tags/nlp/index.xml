<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>nlp - 标签 - dp0d&#39;s blog</title>
        <link>https://dp0d.github.io/tags/nlp/</link>
        <description>nlp - 标签 - dp0d&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>draftpaperofgod@gmail.com (dp0d)</managingEditor>
            <webMaster>draftpaperofgod@gmail.com (dp0d)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Mon, 18 Apr 2022 11:04:18 &#43;0800</lastBuildDate><atom:link href="https://dp0d.github.io/tags/nlp/" rel="self" type="application/rss+xml" /><item>
    <title>语言模型预训练方法</title>
    <link>https://dp0d.github.io/lm_pretraining/</link>
    <pubDate>Mon, 18 Apr 2022 11:04:18 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/lm_pretraining/</guid>
    <description><![CDATA[模型权重保存重载示例1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 自己定义的网络 ## 方法一：只保存权重 encoder = BertModel.from_pretrained(bert_model_path) model = EffiGlobalPointer(encoder, ENT_CLS_NUM, args.pointer_num).to(device) torch.save(best_model.state_dict(), &#34;data/model_data/best.pth&#34;) prev_state_dict = torch.load(&#39;data/model_data/best.pth&#39;) model.load_state_dict(prev_state_dict) ## 方法二：保存所有的 torch.save(best_model, &#34;data/model_data/best.pt&#34;)]]></description>
</item></channel>
</rss>
