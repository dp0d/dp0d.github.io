<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - dp0d&#39;s blog</title>
        <link>https://dp0d.github.io/posts/</link>
        <description>All Posts | dp0d&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 18 Mar 2022 20:31:36 &#43;0800</lastBuildDate><atom:link href="https://dp0d.github.io/posts/" rel="self" type="application/rss+xml" /><item>
    <title>New</title>
    <link>https://dp0d.github.io/new/</link>
    <pubDate>Fri, 18 Mar 2022 20:31:36 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/new/</guid>
    <description><![CDATA[hello~]]></description>
</item><item>
    <title>反向传播算法</title>
    <link>https://dp0d.github.io/backpropagation/</link>
    <pubDate>Fri, 18 Mar 2022 11:29:42 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/backpropagation/</guid>
    <description><![CDATA[反向传播算法作为一位机器学习领域的博主，第一篇博客肯定要从反向传播算法开始啦~
简介​	来自维基百科
​	首先是反向传播算法的历史沿用↓
​	弗拉基米尔·瓦普尼克引用（Bryson, A.E.; W.F. Denham; S.E. Dreyfus. Optimal programming problems with inequality constraints. I: Necessary conditions for extremal solutions. AIAA J. 1, 11 (1963) 2544-2550）在他的书《支持向量机》中首次发表反向传播算法。在1969年Arthur E. Bryson和何毓琦将其描述为多级动态系统优化方法。直到1974年以后在神经网络的背景下应用，并由Paul Werbos[7]、David E. Rumelhart、杰弗里·辛顿和Ronald J. Williams[1][8]的著作，它才获得认可，并引发了一场人工神经网络的研究领域的“文艺复兴”。在21世纪初人们对其失去兴趣，但在2010年后又拥有了兴趣，如今可以通过GPU等大型现代运算器件用于训练更大的网络。例如在2013年，顶级语音识别器现在使用反向传播算法训练神经网络。
​	那么反向传播算法是啥？↓
​	反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会回馈给最佳化方法，用来更新权值以最小化损失函数。 $A$
推导a. 了解矩阵乘法假设
$$ \textbf{A} =
\begin{bmatrix} 1 &amp; 2 \ 3 &amp; 4 \ \end{bmatrix}
, \textbf{B}=\left[ \begin{matrix} -1 &amp; -2 \ -3 &amp; -4 \ \end{matrix} \right] $$]]></description>
</item></channel>
</rss>
