<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>机器学习 - 分类 - dp0d&#39;s blog</title>
        <link>https://dp0d.github.io/categories/machine_learning/</link>
        <description>机器学习 - 分类 - dp0d&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><managingEditor>draftpaperofgod@gmail.com (dp0d)</managingEditor>
            <webMaster>draftpaperofgod@gmail.com (dp0d)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 15 Sep 2022 21:29:55 &#43;0800</lastBuildDate><atom:link href="https://dp0d.github.io/categories/machine_learning/" rel="self" type="application/rss+xml" /><item>
    <title>变分推断</title>
    <link>https://dp0d.github.io/variational_inference/</link>
    <pubDate>Thu, 15 Sep 2022 21:29:55 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/variational_inference/</guid>
    <description><![CDATA[1.变分函数微分（非严格）$$ dy = f(x_1+dx)-f(x_1) $$ $dy$是$y$的微分，也就是函数的微分，是指的$x$变化了一个极小值引起的因变量函数的变化。 泛函泛函]]></description>
</item><item>
    <title>预训练模型的对比</title>
    <link>https://dp0d.github.io/pretrain_model/</link>
    <pubDate>Mon, 05 Sep 2022 15:10:01 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/pretrain_model/</guid>
    <description><![CDATA[BERTBERT（Bidirectional Encoder Representations from Transformers）由谷歌在2018年提出[1]。在语言模型中，它的优势是采用了动态]]></description>
</item><item>
    <title>预训练模型的finetune技巧</title>
    <link>https://dp0d.github.io/finetune_trick/</link>
    <pubDate>Sat, 09 Jul 2022 11:13:47 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/finetune_trick/</guid>
    <description><![CDATA[权重冻结在使用预训练语言模型进行下游任务的微调时，有时数据量难以使得其收敛，此时我们可以选择固定住某些层的参数，使其仍然保持在预训练语料上的]]></description>
</item><item>
    <title>Tensor的简单使用</title>
    <link>https://dp0d.github.io/tensor/</link>
    <pubDate>Fri, 08 Jul 2022 11:33:08 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/tensor/</guid>
    <description><![CDATA[Tensor是什么？Tensor又叫张量，与标量，向量等的区别如下： 标量其实就是一个独立存在的数，比如在线性代数中一个实数 5 就可以被看作一个]]></description>
</item><item>
    <title>炫酷的processbar</title>
    <link>https://dp0d.github.io/bravo_processbar/</link>
    <pubDate>Sun, 03 Jul 2022 11:11:29 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/bravo_processbar/</guid>
    <description><![CDATA[远古的我(远古时期没有截图)：[00:00:00 &lt; 9999:9999:9999] 之前的我现在的我Processbar实现1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24]]></description>
</item><item>
    <title>K折交叉验证</title>
    <link>https://dp0d.github.io/kfold_cross_validation/</link>
    <pubDate>Wed, 29 Jun 2022 15:04:39 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/kfold_cross_validation/</guid>
    <description><![CDATA[什么是K折交叉验证？如果你有五个土豆，你想要教会一个小盆友，这些是土豆，好了，现在你拿出四个土豆让他逐一认识，最后拿出第五个土豆来测试他是否]]></description>
</item><item>
    <title>深度学习优化策略</title>
    <link>https://dp0d.github.io/deep_learning_optimize/</link>
    <pubDate>Wed, 18 May 2022 15:20:58 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/deep_learning_optimize/</guid>
    <description><![CDATA[训练优化策略梯度裁剪梯度裁剪策略是解决模型训练过程中梯度爆炸策略的一个有效方法，为什么会导致梯度爆炸呢？我们知道，如果模型如下 $$ f(x)=wx+b $$ 省略b的]]></description>
</item><item>
    <title>损失函数</title>
    <link>https://dp0d.github.io/loss_function/</link>
    <pubDate>Fri, 29 Apr 2022 10:34:04 &#43;0800</pubDate><author>
        <name>dp0d</name>
    </author><guid>https://dp0d.github.io/loss_function/</guid>
    <description><![CDATA[经典损失函数距离度量F 范数（Frobenius 范数）矩阵所有元素的平方和再开方，他是是向量二范式的拓展类比。 $$ ||A||_F = \sqrt{\sum a_{ij}^2} $$ 如何作为距离度量呢？]]></description>
</item></channel>
</rss>
